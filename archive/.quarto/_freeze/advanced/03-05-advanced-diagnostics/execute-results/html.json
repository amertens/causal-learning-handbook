{
  "hash": "244b9656af660bb30c44ac7b1a81e509",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 3.5: Advanced Diagnostics and Sensitivity Analyses\"\nformat: html\n---\n\n\n\n\n# Chapter 3.5  \n## Advanced Diagnostics and Sensitivity Analyses  \n*Ensuring credible causal conclusions in real-world longitudinal data*\n\nCausal inference is never just about estimating an effect — it is about **credibly defending** that effect.  \nDiagnostics and sensitivity analyses are essential components of the causal roadmap because:\n\n- Identifying assumptions are never fully testable  \n- Real-world data contain missingness, selection, unmeasured confounding, and misclassification  \n- Positivity and model misspecification can quietly undermine estimation  \n- Regulatory-grade RWE requires transparency and robustness checks  \n\nThis chapter walks through:\n\n1. Diagnostics for identification assumptions  \n2. Positivity and overlap assessment  \n3. Diagnostics for nuisance models (Q and g)  \n4. Weight diagnostics  \n5. Sensitivity analyses for unmeasured confounding  \n6. Negative controls  \n7. TMLE-specific diagnostics  \n8. Longitudinal diagnostics (LMTP / longitudinal TMLE)  \n\n--- \n\n## 1. Diagnostics for Identification Assumptions\n\nThe core identification assumptions are:\n\n- **Consistency**  \n- **Exchangeability (No unmeasured confounding)**  \n- **Positivity**  \n- **Correct nuisance model specification**\n\nWhile not directly testable, their **empirical implications** can be evaluated.\n\n### 1.1 Data structure and quality checks\n\nBefore causal modeling:\n\n- Verify time ordering  \n- Confirm treatment and outcome timestamps  \n- Inspect missingness patterns  \n- Look for coding shifts (ICD-9 to ICD-10)  \n- Examine distributions and implausible values  \n\n### 1.2 DAG review\n\nA DAG clarifies:\n\n- Adjustment sets  \n- Potential unmeasured confounding  \n- Variables that should **not** be adjusted for (mediators, colliders)\n\nUse DAGs as a communication tool with clinical partners.\n\n---\n\n## 2. Positivity Diagnostics\n\nPositivity requires:\n\n> Every combination of covariates has a non-zero probability of receiving each treatment.\n\nViolations cause unstable weights and unreliable estimates.\n\n### 2.1 Propensity score overlap\n\n```r\nps <- predict(glm(A ~ W1 + W2, family = binomial), type = \"response\")\n\nlibrary(ggplot2)\nggplot(tibble(ps = ps, A = A), aes(x = ps, fill = factor(A))) +\n  geom_density(alpha = 0.4)\n```\n\nIndicators of concern:\n\n- Mass near 0 or 1  \n- Disjoint distributions  \n\n### 2.2 Clever covariate range (TMLE)\n\n```r\nH <- A/ps - (1-A)/(1-ps)\nsummary(H)\n```\n\nExtreme values imply near-positivity violations.\n\n### 2.3 Remedies\n\n- Restrict to regions with support (“overlap population”)\n- Truncate weights  \n- Use **stochastic interventions** instead of static ones  \n- Simplify interventions  \n\n---\n\n## 3. Diagnostics for Nuisance Functions (Q and g)\n\nAccurate nuisance models are crucial for TMLE, AIPW, and LMTP.\n\n### 3.1 Predictive accuracy\n\n- AUC for binary outcomes  \n- MSE/R² for continuous  \n- Cross-validated risk from SuperLearner  \n\n### 3.2 Calibration plots\n\n```r\ndat %>% \n  mutate(pred = Qhat) %>% \n  ggplot(aes(x = pred, y = Y)) +\n    geom_point(alpha = 0.3) +\n    geom_smooth()\n```\n\n### 3.3 Overfitting assessment\n\nCompare:\n\n- Training loss  \n- Cross-validated loss  \n\nLarge discrepancy → overfitting.\n\n### 3.4 Variable-importance sanity check\n\nEnsure top predictors are *clinically plausible*.\n\n---\n\n## 4. Weight Diagnostics\n\nFor IPTW, MSMs, and censoring weights.\n\n### 4.1 Weight summaries\n\n```r\nsummary(weights)\nquantile(weights, probs = c(0.01, 0.99))\n```\n\nRed flags:\n\n- Mean far from 1  \n- Very heavy tail  \n- Huge max weights  \n\n### 4.2 Visual check\n\n```r\nggplot(tibble(w = weights), aes(x = w)) +\n  geom_histogram()\n```\n\n### 4.3 Truncation\n\n```r\nlower <- quantile(weights, 0.01)\nupper <- quantile(weights, 0.99)\nw_trunc <- pmin(pmax(weights, lower), upper)\n```\n\n---\n\n## 5. Sensitivity Analyses for Unmeasured Confounding\n\n### 5.1 E-values\n\nMeasures minimum strength of confounding needed to explain away an effect.\n\n### 5.2 Quantitative Bias Analysis (QBA)\n\nSimulates impact of:\n\n- Unmeasured confounder prevalence  \n- Unmeasured confounder associations  \n\nR packages: **episensr**, **causalsens**\n\n### 5.3 Rosenbaum sensitivity\n\nFor matched studies.\n\n### 5.4 Sensitivity using stochastic interventions\n\nLMTP can quantify robustness of static intervention effects.\n\n---\n\n## 6. Negative Controls\n\nNegative control outcomes (NCOs):\n\n- Causally unrelated to treatment  \n- Share confounding structures  \n\nIf TMLE of treatment → NCO ≠ 0 → likely confounding remains.\n\nExample:\n\n```r\ntmle_nco <- tmle(\n  Y = dat$negative_event,\n  A = dat$treatment,\n  W = dat[, confounders]\n)\n```\n\nNegative control exposures are also useful.\n\n---\n\n## 7. TMLE-Specific Diagnostics\n\n### 7.1 Clever covariate behavior\n\nExtreme clever covariate values lead to unstable targeting.\n\n### 7.2 Targeting step convergence\n\nCheck for warnings in logistic fluctuation:\n\n```\nglm.fit: algorithm did not converge\n```\n\n### 7.3 Influence-curve distribution\n\n```r\nIC <- tmle_fit$ic\nmean(IC); var(IC)\n```\n\nHeavy tails → avoid Wald intervals, use bootstrap.\n\n---\n\n## 8. Longitudinal Diagnostics (LMTP & Longitudinal TMLE)\n\n### 8.1 Sequential positivity\n\nCheck treatment probabilities at each timepoint.\n\n### 8.2 Cumulative weights\n\n```r\ncumw <- apply(weight_matrix, 1, prod)\nhist(cumw)\n```\n\nExtreme cumulative weights → instability.\n\n### 8.3 Truncation across time\n\nTruncate weights at each time step or truncate cumulative weights.\n\n### 8.4 Time-varying confounding sanity checks\n\nEnsure intermediate variables are not inappropriate colliders.\n\n---\n\n## 9. Recommended Diagnostics Workflow\n\n### Before estimation\n\n- Confirm time-ordering  \n- Draw a DAG  \n- Check missingness  \n- Summarize covariate distributions by treatment  \n\n### During estimation\n\n- Check PS overlap  \n- Evaluate weight distributions  \n- Inspect Q and g predictions  \n- Check TMLE targeting step  \n\n### After estimation\n\n- Sensitivity analyses  \n- Negative controls  \n- Compare across estimators (IPTW, TMLE, AIPW)  \n- Robustness checks (population restriction, alternative confounder sets)\n\n---\n\n## 10. Summary\n\nTo produce defensible causal evidence, diagnostics must be:\n\n- Systematic  \n- Transparent  \n- Documented in the analysis plan  \n- Interpreted with domain expertise  \n\nWith these tools, analysts can judge the **credibility**, **robustness**, and **transportability** of causal findings — essential for regulatory, clinical, and scientific decision‑making.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26200)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.49        jsonlite_2.0.0    xfun_0.49        \n[13] digest_0.6.37     rlang_1.1.6       evaluate_1.0.5   \n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}