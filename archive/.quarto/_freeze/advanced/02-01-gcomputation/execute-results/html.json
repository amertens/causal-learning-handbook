{
  "hash": "0ee223fb4b37f06f16b4a8e6f946430b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 2.1: Outcome Modeling and Standardization (G-Computation)\"\nformat: html\n---\n\n\n\n\n# Chapter 2.1: Outcome Modeling and Standardization  \n*G-computation as a foundation for causal estimation*\n\nOutcome modeling and standardization—often referred to as **g-computation**—is one of the oldest and most intuitive approaches to causal inference. In this chapter, we'll build intuition, walk carefully through why the method works, show where it fails, and provide fully reproducible examples in R (using tidyverse style).  \n\nThis chapter is intentionally thorough, designed for students new to causal inference but with working knowledge of regression.\n\n---\n\n# 1. Motivation: Why Start With G-Computation?\n\nG-computation provides a way to estimate causal effects without relying on regression coefficients. Instead, it reconstructs the potential outcomes:\n\n- **Y(1)** = outcome if treated  \n- **Y(0)** = outcome if untreated  \n\nG-computation estimates the **Average Treatment Effect (ATE)**:\n\n$$\nATE = E[Y(1) - Y(0)]\n$$\n\nby using the identification formula:\n\n$$\nE[ Y(1) ] = E_W[ E[Y | A=1, W] ] \\\\\nE[ Y(0) ] = E_W[ E[Y | A=0, W] ] \n$$\n\nThen:\n\n$$\nATE = E[Y(1)] - E[Y(0)].\n$$\n\nThis is conceptually simple and is often the easiest method for students to grasp when beginning causal inference.\n\n---\n\n# 2. Intuition: What Does G-Computation Actually Do?\n\nG-computation reconstructs what would have happened if *everyone* in the dataset had received treatment **A=1**, and independently what would have happened if *everyone* had received **A=0**.\n\nIt does this by:\n\n1. Fitting an outcome model:  \n   $$ E[Y | A, W] $$  \n\n2. Predicting outcomes for *each* individual under:\n   - Treatment  \n   - Control  \n\n3. Averaging those predicted outcomes (standardization)\n\nThis produces population-level risks that correspond to the causal estimand.\n\n---\n\n# 3. Implementation Example: Simulated Osteoporosis Cohort\n\nWe simulate a small cohort similar to the denosumab vs zoledronic acid motivating example.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'stringr' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.6.0\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(2025)\nn <- 3000\n\n# baseline covariates\nage <- rnorm(n, 75, 6)\ncvd <- rbinom(n, 1, plogis(0.12 * (age - 70)))\n\n# treatment assignment\nA <- rbinom(n, 1, plogis(-1 + 0.07 * (age - 70) + 1.4 * cvd))\n\n# outcome\nY <- rbinom(n, 1, plogis(-2 + 0.6*A + 0.12*(age - 70) + 1.2*cvd))\n\ndat <- tibble(age, cvd, A, Y)\n```\n:::\n\n\n\n\nThe treatment is strongly confounded by cardiovascular history—perfect for demonstrating g-computation.\n\n---\n\n# 4. Step-by-Step G-Computation\n\n## Step 1: Fit an Outcome Model\n\nWe fit a logistic regression model predicting the outcome from treatment and confounders:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- glm(Y ~ A + age + cvd, family = binomial, data = dat)\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Y ~ A + age + cvd, family = binomial, data = dat)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -9.445955   0.594115 -15.899  < 2e-16 ***\nA            0.498637   0.088087   5.661 1.51e-08 ***\nage          0.107847   0.007951  13.563  < 2e-16 ***\ncvd          1.271786   0.094031  13.525  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4127.2  on 2999  degrees of freedom\nResidual deviance: 3422.0  on 2996  degrees of freedom\nAIC: 3430\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n\n\nThis alone is *not* a causal effect. Instead, it's a conditional log-odds ratio.  \nWe will now standardize using predictions.\n\n---\n\n## Step 2: Predict Counterfactual Outcomes\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create counterfactual datasets\ndat1 <- dat %>% mutate(A = 1)\ndat0 <- dat %>% mutate(A = 0)\n\n# predict potential outcomes\np1 <- predict(mod, newdata = dat1, type = \"response\")\np0 <- predict(mod, newdata = dat0, type = \"response\")\n```\n:::\n\n\n\n\nHere:\n- `p1[i]` = predicted outcome for person *i* if treated  \n- `p0[i]` = predicted outcome for person *i* if untreated  \n\n---\n\n## Step 3: Standardize (Average Over Covariates)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrisk1 <- mean(p1)\nrisk0 <- mean(p0)\nate  <- risk1 - risk0\n\nlist(risk1 = risk1, risk0 = risk0, ate = ate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$risk1\n[1] 0.4890903\n\n$risk0\n[1] 0.3890304\n\n$ate\n[1] 0.1000599\n```\n\n\n:::\n:::\n\n\n\n\nThis gives:\n- **Risk under treatment**\n- **Risk under control**\n- **Risk difference (ATE)**\n\nInterpretation example:\n\n> “Initiating denosumab rather than ZA is estimated to increase/decrease 3-year MI/stroke risk by X percentage points.”\n\n---\n\n# 5. Why G-Computation Works\n\nIt directly implements the identification formula:\n\n$$\nE_W[ E[Y | A=a, W] ].\n$$\n\nThis contrasts with regression coefficients, which estimate:\n\n$$\n\text{conditional log-odds ratios given W}\n$$\n\n—completely different from the marginal risk difference.\n\nStandardization always yields marginal (population-level) effects.\n\n---\n\n# 6. Diagnostics: When Can G-Computation Fail?\n\n## 6.1 Model Misspecification\n\nIf your model for \\(E[Y | A, W]\\) is wrong (e.g., omits interactions, assumes linearity), g-computation may be biased.\n\nCheck residuals, fit alternative models, or use machine learning (next chapter).\n\n---\n\n## 6.2 Poor Positivity\n\nIf some strata almost never receive a treatment:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nps <- predict(glm(A ~ age + cvd, family = binomial, data = dat), type = \"response\")\nsummary(ps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1569  0.3715  0.6402  0.5647  0.7218  0.9025 \n```\n\n\n:::\n:::\n\n\n\n\nLook for:\n- Scores near 0 or 1 → dangerous for extrapolation  \n- G-computation may have to predict outcomes in unobserved regions\n\n---\n\n## 6.3 Unmeasured Confounding\n\nNo modeling strategy fixes missing confounders.\n\nBut g-computation makes assumptions very clear, which is an advantage for interpretation.\n\n---\n\n# 7. Using Flexible Models (Teaser for TMLE + SuperLearner)\n\nYou can replace logistic regression with:\n- random forests  \n- gradient boosting  \n- generalized additive models  \n- SuperLearner ensembles  \n\nThis reduces reliance on parametric assumptions.\n\nExample:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(SuperLearner)\n\nsl_mod <- SuperLearner(\n  Y = dat$Y,\n  X = dat %>% select(A, age, cvd),\n  family = binomial(),\n  SL.library = c(\"SL.glm\", \"SL.ranger\", \"SL.gam\")\n)\n\n# predict for counterfactuals\np1_sl <- predict(sl_mod, newdata = dat1)$pred\np0_sl <- predict(sl_mod, newdata = dat0)$pred\n\nmean(p1_sl - p0_sl)\n```\n:::\n\n\n\n\nIn later chapters, we will systematically integrate SuperLearner with **TMLE**.\n\n---\n\n# 8. Summary\n\nIn this chapter you learned:\n\n- What g-computation is and why it is foundational  \n- How to compute standardized risk differences  \n- How g-computation connects to the identification formula  \n- Where g-computation can fail (positivity, misspecification)  \n- How flexible ML-based models can help  \n\nNext: **IPTW**, another way to estimate causal effects by reweighting the data instead of modeling the outcome.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}