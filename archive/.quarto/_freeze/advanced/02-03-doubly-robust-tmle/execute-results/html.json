{
  "hash": "be15417be21c8d5a39ffb25bcc2fb21b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 2.3: Doubly Robust Estimators and Targeted Learning (AIPW + TMLE)\"\nformat: html\n---\n\n\n\n\n# Chapter 2.3: Doubly Robust Estimation and Targeted Learning  \n*Bridging outcome modeling and weighting for more robust causal effect estimation*\n\nIn previous chapters, you learned:\n\n- **G-computation** depends on correctly modeling the *outcome*  \n- **IPTW** depends on correctly modeling the *treatment mechanism*  \n\nBut what if you could use **both models**, and as long as **either one is correct**, your estimator is still consistent?\n\nThis is exactly what **doubly robust estimators** provide.\n\nWe explore:\n\n- Why doubly robust estimators matter  \n- AIPW (Augmented IPTW)  \n- TMLE (Targeted Maximum Likelihood Estimation)  \n- How to implement both with tidyverse-friendly R code  \n- Why TMLE is preferred in modern causal inference  \n\n---\n\n# 1. Why Doubly Robust Estimators?\n\nA doubly robust estimator is consistent if **either**:\n\n- the treatment model is correct, **or**\n- the outcome model is correct\n\nThis is especially important in real-world data where:\n\n- all models are approximations  \n- parametric models are easily misspecified  \n- ML-based models can have high variance  \n\nBy combining both sources of information, doubly robust estimators often outperform either G-computation or IPTW individually.\n\n---\n\n# 2. AIPW: Augmented Inverse Probability Weighting\n\nAIPW adds a correction term to IPTW using the outcome regression.\n\nIt is constructed so that:\n\n- If the treatment model is right → IPTW part works  \n- If the outcome model is right → augmentation part works  \n- If both are right → estimator achieves the **semiparametric efficiency bound**  \n\n## 2.1 Formula\n\nLet:\n\n- \\( e(W) = P(A = 1 | W) \\)  \n- \\( m(a, W) = E[Y | A = a, W] \\)  \n\nThen:\n\n\\[\n\\hat\\mu_1 = \frac{1}{n} \\sum_i \n\\left[\n\frac{A_i Y_i}{e(W_i)} - \n\frac{A_i - e(W_i)}{e(W_i)} m(1, W_i)\n\night]\n\\]\n\n\\[\n\\hat\\mu_0 = \frac{1}{n} \\sum_i \n\\left[\n\frac{(1-A_i) Y_i}{1 - e(W_i)} +\n\frac{A_i - e(W_i)}{1 - e(W_i)} m(0, W_i)\n\night]\n\\]\n\nATE = \\( \\hat\\mu_1 - \\hat\\mu_0 \\)\n\n---\n\n# 3. Simulated Osteoporosis Dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'stringr' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.6.0\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(2026)\nn <- 4000\n\nage <- rnorm(n, 75, 6)\ncvd <- rbinom(n, 1, plogis(0.12 * (age - 70)))\n\n# Treatment model\nA <- rbinom(n, 1, plogis(-1 + 0.08*(age - 70) + 1.4*cvd))\n\n# Outcome model\nY <- rbinom(n, 1, plogis(-2 + 0.6*A + 0.10*(age - 70) + 1.0*cvd))\n\ndat <- tibble(age, cvd, A, Y)\n```\n:::\n\n\n\n\n---\n\n# 4. Fit Outcome and Propensity Score Models\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nQmod <- glm(Y ~ A + age + cvd, family = binomial, data = dat)\ngmod <- glm(A ~ age + cvd, family = binomial, data = dat)\n\ndat <- dat %>%\n  mutate(\n    ps = predict(gmod, type = \"response\"),\n    Q1 = predict(Qmod, newdata = mutate(dat, A=1), type = \"response\"),\n    Q0 = predict(Qmod, newdata = mutate(dat, A=0), type = \"response\")\n  )\n```\n:::\n\n\n\n\n---\n\n# 5. Compute AIPW Estimator\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(dat, {\n  mu1 <- mean( A*Y/ps + (1 - A/ps)*Q1 )\n  mu0 <- mean( (1-A)*Y/(1-ps) + (A/(1-ps))*Q0 )\n  ate <- mu1 - mu0\n  c(mu1=mu1, mu0=mu0, ate=ate)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       mu1        mu0        ate \n 0.4555965  1.1246145 -0.6690181 \n```\n\n\n:::\n:::\n\n\n\n\n---\n\n# 6. TMLE: Targeted Maximum Likelihood Estimation\n\nAIPW is good—TMLE is better.\n\nTMLE:\n\n- Integrates ML naturally  \n- Ensures predictions stay within bounds  \n- Is efficient  \n- Automatically respects model structure  \n- Solves the efficient influence curve (EIC) equation  \n- Works with SuperLearner for flexible modeling  \n\n---\n\n# 7. TMLE Step-by-Step\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit <- function(p) log(p/(1-p))\nexpit <- function(x) 1/(1+exp(-x))\n\n# Step 1: Initial Q\nQinit <- predict(Qmod, type=\"response\")\n\n# Step 2: Propensity\nps <- dat$ps\n\n# Step 3: Clever covariate\nH <- with(dat, A/ps - (1-A)/(1-ps))\n\n# Step 4: Fluctuation model\nepsilon <- glm(dat$Y ~ -1 + offset(logit(Qinit)) + H,\n               family = binomial)$coef\n\nQstar <- expit(logit(Qinit) + epsilon * H)\n```\n:::\n\n\n\n\n---\n\n# 8. Counterfactual Predictions for TMLE\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nQ1_star <- predict(Qmod, newdata = mutate(dat, A=1), type=\"response\")\nQ0_star <- predict(Qmod, newdata = mutate(dat, A=0), type=\"response\")\n\ntmle_ate <- mean(Q1_star - Q0_star)\ntmle_ate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1334543\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n# 9. TMLE + SuperLearner (Recommended)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(SuperLearner)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: nnls\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: gam\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: splines\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: foreach\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'foreach'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded gam 1.22-5\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSuper Learner\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nVersion: 2.0-29\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPackage created on 2024-02-06\n```\n\n\n:::\n\n```{.r .cell-code}\nSL_lib <- c(\"SL.glm\", \"SL.glmnet\", \"SL.ranger\", \"SL.mean\")\n\nQ_SL <- SuperLearner(\n  Y = dat$Y,\n  X = dat %>% select(A, age, cvd),\n  family = binomial(),\n  SL.library = SL_lib\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required namespace: glmnet\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required namespace: ranger\n```\n\n\n:::\n\n```{.r .cell-code}\ng_SL <- SuperLearner(\n  Y = dat$A,\n  X = dat %>% select(age, cvd),\n  family = binomial(),\n  SL.library = SL_lib\n)\n```\n:::\n\n\n\n\nYou can plug these flexible nuisance models directly into TMLE.\n\n---\n\n# 10. Comparison: AIPW vs TMLE\n\n| Property | AIPW | TMLE |\n|---|---|---|\n| Doubly robust | ✓ | ✓ |\n| Efficient | ✗ | ✓ |\n| Handles ML well | ⚠️ requires cross-fitting | ✓ |\n| Predictions bounded | ✗ | ✓ |\n| Implementation complexity | simple | moderate |\n\n---\n\n# 11. Summary\n\nYou now understand:\n\n- The motivation for doubly robust estimators  \n- How AIPW works and how to compute it  \n- Why TMLE offers advantages  \n- How TMLE integrates with SuperLearner  \n- How these fit into the Causal Roadmap  \n\nIn the next chapter, we move to **SuperLearner and machine learning integration**.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26200)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] splines   stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] SuperLearner_2.0-29 gam_1.22-5          foreach_1.5.2      \n [4] nnls_1.6            lubridate_1.9.3     forcats_1.0.0      \n [7] stringr_1.6.0       dplyr_1.1.4         purrr_1.0.2        \n[10] readr_2.1.5         tidyr_1.3.1         tibble_3.2.1       \n[13] ggplot2_3.5.2       tidyverse_2.0.0    \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4        generics_0.1.3    shape_1.4.6.1     lattice_0.22-6   \n [5] stringi_1.8.7     hms_1.1.3         digest_0.6.37     magrittr_2.0.3   \n [9] evaluate_1.0.5    grid_4.4.2        timechange_0.3.0  iterators_1.0.14 \n[13] fastmap_1.2.0     jsonlite_2.0.0    glmnet_4.1-8      Matrix_1.7-1     \n[17] survival_3.7-0    fansi_1.0.6       scales_1.3.0      codetools_0.2-20 \n[21] cli_3.6.5         rlang_1.1.6       munsell_0.5.1     withr_3.0.2      \n[25] yaml_2.3.10       tools_4.4.2       tzdb_0.4.0        colorspace_2.1-1 \n[29] ranger_0.17.0     vctrs_0.6.5       R6_2.6.1          lifecycle_1.0.4  \n[33] htmlwidgets_1.6.4 pkgconfig_2.0.3   pillar_1.9.0      gtable_0.3.6     \n[37] Rcpp_1.0.13-1     glue_1.8.0        xfun_0.49         tidyselect_1.2.1 \n[41] rstudioapi_0.17.1 knitr_1.49        htmltools_0.5.8.1 rmarkdown_2.29   \n[45] compiler_4.4.2   \n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}