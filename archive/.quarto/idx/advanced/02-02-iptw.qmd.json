{"title":"Chapter 2.2: Inverse Probability of Treatment Weighting (IPTW)","markdown":{"yaml":{"title":"Chapter 2.2: Inverse Probability of Treatment Weighting (IPTW)","format":"html"},"headingText":"Chapter 2.2: Inverse Probability of Treatment Weighting","containsRefs":false,"markdown":"\n\n\nInverse probability of treatment weighting (IPTW) is a core method in modern causal inference. Instead of modeling the outcome directly as in g computation, IPTW uses a model for **treatment assignment** to create a pseudo population where treatment is independent of confounders.  \n\nIn this chapter we will\n\n- Build intuition for IPTW\n- Derive the weights and connect them to the causal estimand\n- Show how to estimate and diagnose propensity scores\n- Implement IPTW in R with tidyverse style\n- Discuss stabilized weights and truncation\n- Compare IPTW to g computation on the same simulated data\n\nThis is still point treatment only. Longitudinal extensions come later.\n\n---\n\n# 1. Intuition: Reweighting to Mimic a Trial\n\nIn observational data, some types of patients are more likely to receive one treatment than another. This creates **confounding by indication**.\n\nIPTW tackles this by\n\n> Giving more weight to underrepresented patients and less weight to overrepresented ones, so that in the weighted sample, treatment looks as if it were randomized given the measured covariates.\n\nFormally, we use the **propensity score**\n\n$$\ne(W) = P(A = 1 \\mid W)\n$$\n\nand define weights such as\n\n- For treated:  \\( \\displaystyle w_i = \\frac{1}{e(W_i)} \\)\n- For control: \\( \\displaystyle w_i = \\frac{1}{1 - e(W_i)} \\)\n\nIntuition\n\n- Patients who got a treatment that was unlikely for their covariate pattern receive a large weight\n- Patients who got the most expected treatment get a small weight\n\nIn the weighted pseudo population, confounders are balanced between treatment groups (if the model is correct).\n\n---\n\n# 2. Identification Using IPTW\n\nUnder the same causal assumptions as before\n\n- Consistency\n- Exchangeability: \\( Y(a) \\perp A \\mid W \\)\n- Positivity\n\nthe average potential outcome under treatment can be expressed as\n\n$$\nE[Y(1)] = E\\left[ \\frac{I(A = 1) Y}{e(W)} \\right]\n$$\n\nand under control\n\n$$\nE[Y(0)] = E\\left[ \\frac{I(A = 0) Y}{1 - e(W)} \\right]\n$$\n\nThe IPTW estimator replaces the expectation with the sample average and replaces the true \\( e(W) \\) with an estimated one.\n\nThis is why IPTW is sometimes called a **Horvitz Thompson type** estimator.\n\n---\n\n# 3. Simulated Example\n\nWe reuse a familiar setup: an osteoporosis like population with confounding by age and cardiovascular history.\n\n```{r}\nlibrary(tidyverse)\n\nset.seed(2025)\nn <- 4000\n\nage <- rnorm(n, 75, 6)\ncvd <- rbinom(n, 1, plogis(0.12 * (age - 70)))\n\n# Treatment assignment with strong confounding\nA <- rbinom(n, 1, plogis(-1 + 0.09 * (age - 70) + 1.5 * cvd))\n\n# Outcome model\nY <- rbinom(n, 1, plogis(-2 + 0.5*A + 0.10*(age - 70) + 1.0*cvd))\n\ndat <- tibble(age, cvd, A, Y)\ndat %>% head()\n```\n\nFor simplicity\n\n- A = 1 can be thought of as denosumab\n- A = 0 can be thought of as zoledronic acid\n- Y = 1 indicates occurrence of a cardiovascular outcome\n\n---\n\n# 4. Estimating the Propensity Score\n\nWe model the probability of receiving treatment given covariates.\n\nIn practice, one might use logistic regression, SuperLearner, or other ML. Here we start with logistic regression for clarity.\n\n```{r}\nps_mod <- glm(A ~ age + cvd, family = binomial, data = dat)\nsummary(ps_mod)\n\ndat <- dat %>%\n  mutate(\n    ps = predict(ps_mod, type = \"response\")\n  )\n\ndat %>% \n  summarize(\n    min_ps = min(ps),\n    max_ps = max(ps)\n  )\n```\n\nWe can visualize the propensity score distribution.\n\n```{r}\nggplot(dat, aes(x = ps, fill = factor(A))) +\n  geom_density(alpha = 0.4) +\n  labs(\n    x = \"Estimated propensity score P(A=1 | W)\",\n    fill = \"Treatment\",\n    title = \"Propensity score overlap\"\n  ) +\n  theme_minimal()\n```\n\nInterpretation\n\n- Good overlap is important for positivity\n- If treatment groups have little overlap in ps, IPTW will rely on extrapolation and produce unstable weights\n\n---\n\n# 5. Constructing IPTW Weights\n\n### 5.1 Unstabilized weights\n\n```{r}\ndat <- dat %>%\n  mutate(\n    w_ipw = if_else(A == 1, 1 / ps, 1 / (1 - ps))\n  )\n\nsummary(dat$w_ipw)\nquantile(dat$w_ipw, probs = c(0.01, 0.99))\n```\n\nKeep an eye on\n\n- Very large weights\n- Range and extreme quantiles\n\n### 5.2 Stabilized weights\n\nStabilized weights can improve variance properties and interpretability.\n\nFor a binary treatment\n\n$$\nSW_i = \\frac{P(A_i)}{e(W_i)} \\text{ if } A_i = 1\n$$\n\nand\n\n$$\nSW_i = \\frac{P(A_i)}{1 - e(W_i)} \\text{ if } A_i = 0\n$$\n\nHere \\( P(A_i) \\) is the marginal probability of treatment.\n\n```{r}\npA <- mean(dat$A)\n\ndat <- dat %>%\n  mutate(\n    sw_ipw = case_when(\n      A == 1 ~ pA / ps,\n      A == 0 ~ (1 - pA) / (1 - ps)\n    )\n  )\n\nsummary(dat$sw_ipw)\nquantile(dat$sw_ipw, probs = c(0.01, 0.99))\n```\n\nStabilized weights often have a smaller variance and can lead to more stable estimates.\n\n---\n\n# 6. Estimating the ATE via IPTW\n\nThere are several equivalent ways to use the weights.\n\n## 6.1 Direct weighted mean of outcomes\n\nEstimated risk under treatment\n\n```{r}\nrisk1_ipw <- with(dat, sum(sw_ipw * Y * (A == 1)) / sum(sw_ipw * (A == 1)))\nrisk0_ipw <- with(dat, sum(sw_ipw * Y * (A == 0)) / sum(sw_ipw * (A == 0)))\n\nate_ipw <- risk1_ipw - risk0_ipw\nc(risk1 = risk1_ipw, risk0 = risk0_ipw, ate = ate_ipw)\n```\n\nThis matches the formula\n\n$$\n\\hat E[Y(1)] = \\frac{\\sum_i SW_i Y_i I(A_i = 1)}{\\sum_i SW_i I(A_i = 1)}\n$$\n\n## 6.2 Weighted regression model\n\nWe can also fit a weighted regression with treatment as the only predictor.\n\n```{r}\nlibrary(sandwich)\nlibrary(lmtest)\n\n# Fit a simple weighted model\nfit_ipw <- glm(Y ~ A, family = binomial, weights = sw_ipw, data = dat)\n\n# Robust standard errors\ncov_ipw <- vcovHC(fit_ipw, type = \"HC0\")\ncoeftest(fit_ipw, cov_ipw)\n```\n\nInterpretation\n\n- The coefficient of A (on the log odds scale) now estimates a marginal effect in the pseudo population\n- You can compute marginal risk differences or ratios by predicting from the model at A=1 and A=0 and standardizing\n\n---\n\n# 7. Comparing IPTW and G Computation\n\nWe can compare the IPTW ATE to the g computation ATE from the previous chapter.\n\n```{r}\n# g computation\ng_mod <- glm(Y ~ A + age + cvd, family = binomial, data = dat)\np1_g <- predict(g_mod, newdata = dat %>% mutate(A = 1), type = \"response\")\np0_g <- predict(g_mod, newdata = dat %>% mutate(A = 0), type = \"response\")\nate_g <- mean(p1_g - p0_g)\n\nc(ate_gcomp = ate_g, ate_iptw = ate_ipw)\n```\n\nUnder correct models and adequate positivity, these methods should converge to similar estimates as sample size grows.\n\nHowever\n\n- g computation depends on correctly specifying the outcome model\n- IPTW depends on correctly specifying the treatment model\n- Neither uses both models at once, which motivates **doubly robust** methods (next chapter)\n\n---\n\n# 8. Diagnostics and Practical Tips\n\nIPTW is powerful but fragile if diagnostics are ignored.\n\n### 8.1 Check propensity score overlap\n\n- Plot densities by treatment group\n- Flag regions where one group is missing\n\n### 8.2 Check weights\n\n```{r}\ndat %>% \n  summarize(\n    min_w = min(sw_ipw),\n    max_w = max(sw_ipw),\n    mean_w = mean(sw_ipw),\n    sd_w = sd(sw_ipw)\n  )\n```\n\nLarge extreme values are a red flag.\n\n### 8.3 Consider truncation\n\nYou can truncate weights at a chosen percentile, for example the 1st and 99th percentiles.\n\n```{r}\nlower <- quantile(dat$sw_ipw, 0.01)\nupper <- quantile(dat$sw_ipw, 0.99)\n\ndat <- dat %>%\n  mutate(\n    sw_trunc = pmin(pmax(sw_ipw, lower), upper)\n  )\n\n# recompute ATE with truncated weights\nrisk1_trunc <- with(dat, sum(sw_trunc * Y * (A == 1)) / sum(sw_trunc * (A == 1)))\nrisk0_trunc <- with(dat, sum(sw_trunc * Y * (A == 0)) / sum(sw_trunc * (A == 0)))\nate_trunc   <- risk1_trunc - risk0_trunc\n\nc(ate_truncated = ate_trunc)\n```\n\nTruncation reduces variance at the cost of some bias. In practice it often improves mean squared error.\n\n---\n\n# 9. Summary\n\nIn this chapter we covered\n\n- The intuition for IPTW as a method that reweights data to mimic a trial\n- The key role of the propensity score in computing weights\n- How to construct unstabilized and stabilized IPTW weights\n- How to estimate ATEs via weighted means or weighted regression\n- How to diagnose overlap and extreme weights\n- How IPTW compares to g computation\n\nLimitations\n\n- Depends entirely on a correct treatment model\n- Sensitive to violations of positivity\n- Does not use information from the outcome model, which motivates doubly robust approaches\n\nIn the next chapter, we will introduce **doubly robust estimators** and **Targeted Maximum Likelihood Estimation (TMLE)**, which combine the strengths of g computation and IPTW and allow the use of machine learning for both nuisance models.\n\n```{r}\nsessionInfo()\n```\n","srcMarkdownNoYaml":"\n\n# Chapter 2.2: Inverse Probability of Treatment Weighting  \n\nInverse probability of treatment weighting (IPTW) is a core method in modern causal inference. Instead of modeling the outcome directly as in g computation, IPTW uses a model for **treatment assignment** to create a pseudo population where treatment is independent of confounders.  \n\nIn this chapter we will\n\n- Build intuition for IPTW\n- Derive the weights and connect them to the causal estimand\n- Show how to estimate and diagnose propensity scores\n- Implement IPTW in R with tidyverse style\n- Discuss stabilized weights and truncation\n- Compare IPTW to g computation on the same simulated data\n\nThis is still point treatment only. Longitudinal extensions come later.\n\n---\n\n# 1. Intuition: Reweighting to Mimic a Trial\n\nIn observational data, some types of patients are more likely to receive one treatment than another. This creates **confounding by indication**.\n\nIPTW tackles this by\n\n> Giving more weight to underrepresented patients and less weight to overrepresented ones, so that in the weighted sample, treatment looks as if it were randomized given the measured covariates.\n\nFormally, we use the **propensity score**\n\n$$\ne(W) = P(A = 1 \\mid W)\n$$\n\nand define weights such as\n\n- For treated:  \\( \\displaystyle w_i = \\frac{1}{e(W_i)} \\)\n- For control: \\( \\displaystyle w_i = \\frac{1}{1 - e(W_i)} \\)\n\nIntuition\n\n- Patients who got a treatment that was unlikely for their covariate pattern receive a large weight\n- Patients who got the most expected treatment get a small weight\n\nIn the weighted pseudo population, confounders are balanced between treatment groups (if the model is correct).\n\n---\n\n# 2. Identification Using IPTW\n\nUnder the same causal assumptions as before\n\n- Consistency\n- Exchangeability: \\( Y(a) \\perp A \\mid W \\)\n- Positivity\n\nthe average potential outcome under treatment can be expressed as\n\n$$\nE[Y(1)] = E\\left[ \\frac{I(A = 1) Y}{e(W)} \\right]\n$$\n\nand under control\n\n$$\nE[Y(0)] = E\\left[ \\frac{I(A = 0) Y}{1 - e(W)} \\right]\n$$\n\nThe IPTW estimator replaces the expectation with the sample average and replaces the true \\( e(W) \\) with an estimated one.\n\nThis is why IPTW is sometimes called a **Horvitz Thompson type** estimator.\n\n---\n\n# 3. Simulated Example\n\nWe reuse a familiar setup: an osteoporosis like population with confounding by age and cardiovascular history.\n\n```{r}\nlibrary(tidyverse)\n\nset.seed(2025)\nn <- 4000\n\nage <- rnorm(n, 75, 6)\ncvd <- rbinom(n, 1, plogis(0.12 * (age - 70)))\n\n# Treatment assignment with strong confounding\nA <- rbinom(n, 1, plogis(-1 + 0.09 * (age - 70) + 1.5 * cvd))\n\n# Outcome model\nY <- rbinom(n, 1, plogis(-2 + 0.5*A + 0.10*(age - 70) + 1.0*cvd))\n\ndat <- tibble(age, cvd, A, Y)\ndat %>% head()\n```\n\nFor simplicity\n\n- A = 1 can be thought of as denosumab\n- A = 0 can be thought of as zoledronic acid\n- Y = 1 indicates occurrence of a cardiovascular outcome\n\n---\n\n# 4. Estimating the Propensity Score\n\nWe model the probability of receiving treatment given covariates.\n\nIn practice, one might use logistic regression, SuperLearner, or other ML. Here we start with logistic regression for clarity.\n\n```{r}\nps_mod <- glm(A ~ age + cvd, family = binomial, data = dat)\nsummary(ps_mod)\n\ndat <- dat %>%\n  mutate(\n    ps = predict(ps_mod, type = \"response\")\n  )\n\ndat %>% \n  summarize(\n    min_ps = min(ps),\n    max_ps = max(ps)\n  )\n```\n\nWe can visualize the propensity score distribution.\n\n```{r}\nggplot(dat, aes(x = ps, fill = factor(A))) +\n  geom_density(alpha = 0.4) +\n  labs(\n    x = \"Estimated propensity score P(A=1 | W)\",\n    fill = \"Treatment\",\n    title = \"Propensity score overlap\"\n  ) +\n  theme_minimal()\n```\n\nInterpretation\n\n- Good overlap is important for positivity\n- If treatment groups have little overlap in ps, IPTW will rely on extrapolation and produce unstable weights\n\n---\n\n# 5. Constructing IPTW Weights\n\n### 5.1 Unstabilized weights\n\n```{r}\ndat <- dat %>%\n  mutate(\n    w_ipw = if_else(A == 1, 1 / ps, 1 / (1 - ps))\n  )\n\nsummary(dat$w_ipw)\nquantile(dat$w_ipw, probs = c(0.01, 0.99))\n```\n\nKeep an eye on\n\n- Very large weights\n- Range and extreme quantiles\n\n### 5.2 Stabilized weights\n\nStabilized weights can improve variance properties and interpretability.\n\nFor a binary treatment\n\n$$\nSW_i = \\frac{P(A_i)}{e(W_i)} \\text{ if } A_i = 1\n$$\n\nand\n\n$$\nSW_i = \\frac{P(A_i)}{1 - e(W_i)} \\text{ if } A_i = 0\n$$\n\nHere \\( P(A_i) \\) is the marginal probability of treatment.\n\n```{r}\npA <- mean(dat$A)\n\ndat <- dat %>%\n  mutate(\n    sw_ipw = case_when(\n      A == 1 ~ pA / ps,\n      A == 0 ~ (1 - pA) / (1 - ps)\n    )\n  )\n\nsummary(dat$sw_ipw)\nquantile(dat$sw_ipw, probs = c(0.01, 0.99))\n```\n\nStabilized weights often have a smaller variance and can lead to more stable estimates.\n\n---\n\n# 6. Estimating the ATE via IPTW\n\nThere are several equivalent ways to use the weights.\n\n## 6.1 Direct weighted mean of outcomes\n\nEstimated risk under treatment\n\n```{r}\nrisk1_ipw <- with(dat, sum(sw_ipw * Y * (A == 1)) / sum(sw_ipw * (A == 1)))\nrisk0_ipw <- with(dat, sum(sw_ipw * Y * (A == 0)) / sum(sw_ipw * (A == 0)))\n\nate_ipw <- risk1_ipw - risk0_ipw\nc(risk1 = risk1_ipw, risk0 = risk0_ipw, ate = ate_ipw)\n```\n\nThis matches the formula\n\n$$\n\\hat E[Y(1)] = \\frac{\\sum_i SW_i Y_i I(A_i = 1)}{\\sum_i SW_i I(A_i = 1)}\n$$\n\n## 6.2 Weighted regression model\n\nWe can also fit a weighted regression with treatment as the only predictor.\n\n```{r}\nlibrary(sandwich)\nlibrary(lmtest)\n\n# Fit a simple weighted model\nfit_ipw <- glm(Y ~ A, family = binomial, weights = sw_ipw, data = dat)\n\n# Robust standard errors\ncov_ipw <- vcovHC(fit_ipw, type = \"HC0\")\ncoeftest(fit_ipw, cov_ipw)\n```\n\nInterpretation\n\n- The coefficient of A (on the log odds scale) now estimates a marginal effect in the pseudo population\n- You can compute marginal risk differences or ratios by predicting from the model at A=1 and A=0 and standardizing\n\n---\n\n# 7. Comparing IPTW and G Computation\n\nWe can compare the IPTW ATE to the g computation ATE from the previous chapter.\n\n```{r}\n# g computation\ng_mod <- glm(Y ~ A + age + cvd, family = binomial, data = dat)\np1_g <- predict(g_mod, newdata = dat %>% mutate(A = 1), type = \"response\")\np0_g <- predict(g_mod, newdata = dat %>% mutate(A = 0), type = \"response\")\nate_g <- mean(p1_g - p0_g)\n\nc(ate_gcomp = ate_g, ate_iptw = ate_ipw)\n```\n\nUnder correct models and adequate positivity, these methods should converge to similar estimates as sample size grows.\n\nHowever\n\n- g computation depends on correctly specifying the outcome model\n- IPTW depends on correctly specifying the treatment model\n- Neither uses both models at once, which motivates **doubly robust** methods (next chapter)\n\n---\n\n# 8. Diagnostics and Practical Tips\n\nIPTW is powerful but fragile if diagnostics are ignored.\n\n### 8.1 Check propensity score overlap\n\n- Plot densities by treatment group\n- Flag regions where one group is missing\n\n### 8.2 Check weights\n\n```{r}\ndat %>% \n  summarize(\n    min_w = min(sw_ipw),\n    max_w = max(sw_ipw),\n    mean_w = mean(sw_ipw),\n    sd_w = sd(sw_ipw)\n  )\n```\n\nLarge extreme values are a red flag.\n\n### 8.3 Consider truncation\n\nYou can truncate weights at a chosen percentile, for example the 1st and 99th percentiles.\n\n```{r}\nlower <- quantile(dat$sw_ipw, 0.01)\nupper <- quantile(dat$sw_ipw, 0.99)\n\ndat <- dat %>%\n  mutate(\n    sw_trunc = pmin(pmax(sw_ipw, lower), upper)\n  )\n\n# recompute ATE with truncated weights\nrisk1_trunc <- with(dat, sum(sw_trunc * Y * (A == 1)) / sum(sw_trunc * (A == 1)))\nrisk0_trunc <- with(dat, sum(sw_trunc * Y * (A == 0)) / sum(sw_trunc * (A == 0)))\nate_trunc   <- risk1_trunc - risk0_trunc\n\nc(ate_truncated = ate_trunc)\n```\n\nTruncation reduces variance at the cost of some bias. In practice it often improves mean squared error.\n\n---\n\n# 9. Summary\n\nIn this chapter we covered\n\n- The intuition for IPTW as a method that reweights data to mimic a trial\n- The key role of the propensity score in computing weights\n- How to construct unstabilized and stabilized IPTW weights\n- How to estimate ATEs via weighted means or weighted regression\n- How to diagnose overlap and extreme weights\n- How IPTW compares to g computation\n\nLimitations\n\n- Depends entirely on a correct treatment model\n- Sensitive to violations of positivity\n- Does not use information from the outcome model, which motivates doubly robust approaches\n\nIn the next chapter, we will introduce **doubly robust estimators** and **Targeted Maximum Likelihood Estimation (TMLE)**, which combine the strengths of g computation and IPTW and allow the use of machine learning for both nuisance models.\n\n```{r}\nsessionInfo()\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"02-02-iptw.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","theme":"cosmo","smooth-scroll":true,"title":"Chapter 2.2: Inverse Probability of Treatment Weighting (IPTW)","sidebar":{"style":"docked","search":true,"contents":[{"text":"Overview","href":"index.qmd"},{"section":"Advanced topics","contents":[{"text":"1.1 Regression vs Causal Inference","href":"01-01-regression-vs-causal.qmd"},{"text":"1.2 The Causal Roadmap in detail","href":"01-02-causal-roadmap.qmd"},{"text":"1.3 Identification and Estimands","href":"01-03-identification-estimands.qmd"},{"text":"2.1 G-computation","href":"02-01-gcomputation.qmd"},{"text":"2.2 Inverse Probability of Treatment Weighting (IPTW)","href":"02-02-iptw.qmd"},{"text":"2.3 Doubly Robust Estimation and Targeted Maximum Likelihood Estimation (TMLE)","href":"02-03-doubly-robust-tmle.qmd"},{"text":"2.4 Super Learner","href":"02-04-superlearner.qmd"},{"text":"3.3 Longitudinal Case Study","href":"03-03-longitudinal-case-study.qmd"},{"text":"3.4 Effect Modification","href":"03-04-effect-modification.qmd"},{"text":"3.5 Advanced Diagnostics","href":"03-05-advanced-diagnostics.qmd"},{"text":"3.6 RWD meets RCT: Hybrid Designs","href":"03-06-rwd-meets-rct-hybrid-designs.qmd"},{"text":"3.7 Longitudinal Time-Dependent Confounding","href":"03-07-longitudinal-td-confounding.qmd"},{"text":"3.8 TMLE in the Clean Room Design","href":"03-08-tmle-clean-room.qmd"},{"text":"Covariate Adjustment IN RCTs using TMLE","href":"chapter_covariate_adjustment_tmle.qmd"},{"text":"Common Pitfalls in Causal Inference","href":"common-pitfalls.qmd"}]}]}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}