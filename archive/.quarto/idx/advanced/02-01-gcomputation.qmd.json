{"title":"Chapter 2.1: Outcome Modeling and Standardization (G-Computation)","markdown":{"yaml":{"title":"Chapter 2.1: Outcome Modeling and Standardization (G-Computation)","format":"html"},"headingText":"Chapter 2.1: Outcome Modeling and Standardization","containsRefs":false,"markdown":"\n\n*G-computation as a foundation for causal estimation*\n\nOutcome modeling and standardization—often referred to as **g-computation**—is one of the oldest and most intuitive approaches to causal inference. In this chapter, we'll build intuition, walk carefully through why the method works, show where it fails, and provide fully reproducible examples in R (using tidyverse style).  \n\nThis chapter is intentionally thorough, designed for students new to causal inference but with working knowledge of regression.\n\n---\n\n# 1. Motivation: Why Start With G-Computation?\n\nG-computation provides a way to estimate causal effects without relying on regression coefficients. Instead, it reconstructs the potential outcomes:\n\n- **Y(1)** = outcome if treated  \n- **Y(0)** = outcome if untreated  \n\nG-computation estimates the **Average Treatment Effect (ATE)**:\n\n$$\nATE = E[Y(1) - Y(0)]\n$$\n\nby using the identification formula:\n\n$$\nE[ Y(1) ] = E_W[ E[Y | A=1, W] ] \\\\\nE[ Y(0) ] = E_W[ E[Y | A=0, W] ] \n$$\n\nThen:\n\n$$\nATE = E[Y(1)] - E[Y(0)].\n$$\n\nThis is conceptually simple and is often the easiest method for students to grasp when beginning causal inference.\n\n---\n\n# 2. Intuition: What Does G-Computation Actually Do?\n\nG-computation reconstructs what would have happened if *everyone* in the dataset had received treatment **A=1**, and independently what would have happened if *everyone* had received **A=0**.\n\nIt does this by:\n\n1. Fitting an outcome model:  \n   $$ E[Y | A, W] $$  \n\n2. Predicting outcomes for *each* individual under:\n   - Treatment  \n   - Control  \n\n3. Averaging those predicted outcomes (standardization)\n\nThis produces population-level risks that correspond to the causal estimand.\n\n---\n\n# 3. Implementation Example: Simulated Osteoporosis Cohort\n\nWe simulate a small cohort similar to the denosumab vs zoledronic acid motivating example.\n\n```{r}\nlibrary(tidyverse)\n\nset.seed(2025)\nn <- 3000\n\n# baseline covariates\nage <- rnorm(n, 75, 6)\ncvd <- rbinom(n, 1, plogis(0.12 * (age - 70)))\n\n# treatment assignment\nA <- rbinom(n, 1, plogis(-1 + 0.07 * (age - 70) + 1.4 * cvd))\n\n# outcome\nY <- rbinom(n, 1, plogis(-2 + 0.6*A + 0.12*(age - 70) + 1.2*cvd))\n\ndat <- tibble(age, cvd, A, Y)\n```\n\nThe treatment is strongly confounded by cardiovascular history—perfect for demonstrating g-computation.\n\n---\n\n# 4. Step-by-Step G-Computation\n\n## Step 1: Fit an Outcome Model\n\nWe fit a logistic regression model predicting the outcome from treatment and confounders:\n\n```{r}\nmod <- glm(Y ~ A + age + cvd, family = binomial, data = dat)\nsummary(mod)\n```\n\nThis alone is *not* a causal effect. Instead, it's a conditional log-odds ratio.  \nWe will now standardize using predictions.\n\n---\n\n## Step 2: Predict Counterfactual Outcomes\n\n```{r}\n# create counterfactual datasets\ndat1 <- dat %>% mutate(A = 1)\ndat0 <- dat %>% mutate(A = 0)\n\n# predict potential outcomes\np1 <- predict(mod, newdata = dat1, type = \"response\")\np0 <- predict(mod, newdata = dat0, type = \"response\")\n```\n\nHere:\n- `p1[i]` = predicted outcome for person *i* if treated  \n- `p0[i]` = predicted outcome for person *i* if untreated  \n\n---\n\n## Step 3: Standardize (Average Over Covariates)\n\n```{r}\nrisk1 <- mean(p1)\nrisk0 <- mean(p0)\nate  <- risk1 - risk0\n\nlist(risk1 = risk1, risk0 = risk0, ate = ate)\n```\n\nThis gives:\n- **Risk under treatment**\n- **Risk under control**\n- **Risk difference (ATE)**\n\nInterpretation example:\n\n> “Initiating denosumab rather than ZA is estimated to increase/decrease 3-year MI/stroke risk by X percentage points.”\n\n---\n\n# 5. Why G-Computation Works\n\nIt directly implements the identification formula:\n\n$$\nE_W[ E[Y | A=a, W] ].\n$$\n\nThis contrasts with regression coefficients, which estimate:\n\n$$\n\text{conditional log-odds ratios given W}\n$$\n\n—completely different from the marginal risk difference.\n\nStandardization always yields marginal (population-level) effects.\n\n---\n\n# 6. Diagnostics: When Can G-Computation Fail?\n\n## 6.1 Model Misspecification\n\nIf your model for \\(E[Y | A, W]\\) is wrong (e.g., omits interactions, assumes linearity), g-computation may be biased.\n\nCheck residuals, fit alternative models, or use machine learning (next chapter).\n\n---\n\n## 6.2 Poor Positivity\n\nIf some strata almost never receive a treatment:\n\n```{r}\nps <- predict(glm(A ~ age + cvd, family = binomial, data = dat), type = \"response\")\nsummary(ps)\n```\n\nLook for:\n- Scores near 0 or 1 → dangerous for extrapolation  \n- G-computation may have to predict outcomes in unobserved regions\n\n---\n\n## 6.3 Unmeasured Confounding\n\nNo modeling strategy fixes missing confounders.\n\nBut g-computation makes assumptions very clear, which is an advantage for interpretation.\n\n---\n\n# 7. Using Flexible Models (Teaser for TMLE + SuperLearner)\n\nYou can replace logistic regression with:\n- random forests  \n- gradient boosting  \n- generalized additive models  \n- SuperLearner ensembles  \n\nThis reduces reliance on parametric assumptions.\n\nExample:\n\n```{r, eval=F}\nlibrary(SuperLearner)\n\nsl_mod <- SuperLearner(\n  Y = dat$Y,\n  X = dat %>% select(A, age, cvd),\n  family = binomial(),\n  SL.library = c(\"SL.glm\", \"SL.ranger\", \"SL.gam\")\n)\n\n# predict for counterfactuals\np1_sl <- predict(sl_mod, newdata = dat1)$pred\np0_sl <- predict(sl_mod, newdata = dat0)$pred\n\nmean(p1_sl - p0_sl)\n```\n\nIn later chapters, we will systematically integrate SuperLearner with **TMLE**.\n\n---\n\n# 8. Summary\n\nIn this chapter you learned:\n\n- What g-computation is and why it is foundational  \n- How to compute standardized risk differences  \n- How g-computation connects to the identification formula  \n- Where g-computation can fail (positivity, misspecification)  \n- How flexible ML-based models can help  \n\nNext: **IPTW**, another way to estimate causal effects by reweighting the data instead of modeling the outcome.\n\n\n","srcMarkdownNoYaml":"\n\n# Chapter 2.1: Outcome Modeling and Standardization  \n*G-computation as a foundation for causal estimation*\n\nOutcome modeling and standardization—often referred to as **g-computation**—is one of the oldest and most intuitive approaches to causal inference. In this chapter, we'll build intuition, walk carefully through why the method works, show where it fails, and provide fully reproducible examples in R (using tidyverse style).  \n\nThis chapter is intentionally thorough, designed for students new to causal inference but with working knowledge of regression.\n\n---\n\n# 1. Motivation: Why Start With G-Computation?\n\nG-computation provides a way to estimate causal effects without relying on regression coefficients. Instead, it reconstructs the potential outcomes:\n\n- **Y(1)** = outcome if treated  \n- **Y(0)** = outcome if untreated  \n\nG-computation estimates the **Average Treatment Effect (ATE)**:\n\n$$\nATE = E[Y(1) - Y(0)]\n$$\n\nby using the identification formula:\n\n$$\nE[ Y(1) ] = E_W[ E[Y | A=1, W] ] \\\\\nE[ Y(0) ] = E_W[ E[Y | A=0, W] ] \n$$\n\nThen:\n\n$$\nATE = E[Y(1)] - E[Y(0)].\n$$\n\nThis is conceptually simple and is often the easiest method for students to grasp when beginning causal inference.\n\n---\n\n# 2. Intuition: What Does G-Computation Actually Do?\n\nG-computation reconstructs what would have happened if *everyone* in the dataset had received treatment **A=1**, and independently what would have happened if *everyone* had received **A=0**.\n\nIt does this by:\n\n1. Fitting an outcome model:  \n   $$ E[Y | A, W] $$  \n\n2. Predicting outcomes for *each* individual under:\n   - Treatment  \n   - Control  \n\n3. Averaging those predicted outcomes (standardization)\n\nThis produces population-level risks that correspond to the causal estimand.\n\n---\n\n# 3. Implementation Example: Simulated Osteoporosis Cohort\n\nWe simulate a small cohort similar to the denosumab vs zoledronic acid motivating example.\n\n```{r}\nlibrary(tidyverse)\n\nset.seed(2025)\nn <- 3000\n\n# baseline covariates\nage <- rnorm(n, 75, 6)\ncvd <- rbinom(n, 1, plogis(0.12 * (age - 70)))\n\n# treatment assignment\nA <- rbinom(n, 1, plogis(-1 + 0.07 * (age - 70) + 1.4 * cvd))\n\n# outcome\nY <- rbinom(n, 1, plogis(-2 + 0.6*A + 0.12*(age - 70) + 1.2*cvd))\n\ndat <- tibble(age, cvd, A, Y)\n```\n\nThe treatment is strongly confounded by cardiovascular history—perfect for demonstrating g-computation.\n\n---\n\n# 4. Step-by-Step G-Computation\n\n## Step 1: Fit an Outcome Model\n\nWe fit a logistic regression model predicting the outcome from treatment and confounders:\n\n```{r}\nmod <- glm(Y ~ A + age + cvd, family = binomial, data = dat)\nsummary(mod)\n```\n\nThis alone is *not* a causal effect. Instead, it's a conditional log-odds ratio.  \nWe will now standardize using predictions.\n\n---\n\n## Step 2: Predict Counterfactual Outcomes\n\n```{r}\n# create counterfactual datasets\ndat1 <- dat %>% mutate(A = 1)\ndat0 <- dat %>% mutate(A = 0)\n\n# predict potential outcomes\np1 <- predict(mod, newdata = dat1, type = \"response\")\np0 <- predict(mod, newdata = dat0, type = \"response\")\n```\n\nHere:\n- `p1[i]` = predicted outcome for person *i* if treated  \n- `p0[i]` = predicted outcome for person *i* if untreated  \n\n---\n\n## Step 3: Standardize (Average Over Covariates)\n\n```{r}\nrisk1 <- mean(p1)\nrisk0 <- mean(p0)\nate  <- risk1 - risk0\n\nlist(risk1 = risk1, risk0 = risk0, ate = ate)\n```\n\nThis gives:\n- **Risk under treatment**\n- **Risk under control**\n- **Risk difference (ATE)**\n\nInterpretation example:\n\n> “Initiating denosumab rather than ZA is estimated to increase/decrease 3-year MI/stroke risk by X percentage points.”\n\n---\n\n# 5. Why G-Computation Works\n\nIt directly implements the identification formula:\n\n$$\nE_W[ E[Y | A=a, W] ].\n$$\n\nThis contrasts with regression coefficients, which estimate:\n\n$$\n\text{conditional log-odds ratios given W}\n$$\n\n—completely different from the marginal risk difference.\n\nStandardization always yields marginal (population-level) effects.\n\n---\n\n# 6. Diagnostics: When Can G-Computation Fail?\n\n## 6.1 Model Misspecification\n\nIf your model for \\(E[Y | A, W]\\) is wrong (e.g., omits interactions, assumes linearity), g-computation may be biased.\n\nCheck residuals, fit alternative models, or use machine learning (next chapter).\n\n---\n\n## 6.2 Poor Positivity\n\nIf some strata almost never receive a treatment:\n\n```{r}\nps <- predict(glm(A ~ age + cvd, family = binomial, data = dat), type = \"response\")\nsummary(ps)\n```\n\nLook for:\n- Scores near 0 or 1 → dangerous for extrapolation  \n- G-computation may have to predict outcomes in unobserved regions\n\n---\n\n## 6.3 Unmeasured Confounding\n\nNo modeling strategy fixes missing confounders.\n\nBut g-computation makes assumptions very clear, which is an advantage for interpretation.\n\n---\n\n# 7. Using Flexible Models (Teaser for TMLE + SuperLearner)\n\nYou can replace logistic regression with:\n- random forests  \n- gradient boosting  \n- generalized additive models  \n- SuperLearner ensembles  \n\nThis reduces reliance on parametric assumptions.\n\nExample:\n\n```{r, eval=F}\nlibrary(SuperLearner)\n\nsl_mod <- SuperLearner(\n  Y = dat$Y,\n  X = dat %>% select(A, age, cvd),\n  family = binomial(),\n  SL.library = c(\"SL.glm\", \"SL.ranger\", \"SL.gam\")\n)\n\n# predict for counterfactuals\np1_sl <- predict(sl_mod, newdata = dat1)$pred\np0_sl <- predict(sl_mod, newdata = dat0)$pred\n\nmean(p1_sl - p0_sl)\n```\n\nIn later chapters, we will systematically integrate SuperLearner with **TMLE**.\n\n---\n\n# 8. Summary\n\nIn this chapter you learned:\n\n- What g-computation is and why it is foundational  \n- How to compute standardized risk differences  \n- How g-computation connects to the identification formula  \n- Where g-computation can fail (positivity, misspecification)  \n- How flexible ML-based models can help  \n\nNext: **IPTW**, another way to estimate causal effects by reweighting the data instead of modeling the outcome.\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"02-01-gcomputation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","theme":"cosmo","smooth-scroll":true,"title":"Chapter 2.1: Outcome Modeling and Standardization (G-Computation)","sidebar":{"style":"docked","search":true,"contents":[{"text":"Overview","href":"index.qmd"},{"section":"Advanced topics","contents":[{"text":"1.1 Regression vs Causal Inference","href":"01-01-regression-vs-causal.qmd"},{"text":"1.2 The Causal Roadmap in detail","href":"01-02-causal-roadmap.qmd"},{"text":"1.3 Identification and Estimands","href":"01-03-identification-estimands.qmd"},{"text":"2.1 G-computation","href":"02-01-gcomputation.qmd"},{"text":"2.2 Inverse Probability of Treatment Weighting (IPTW)","href":"02-02-iptw.qmd"},{"text":"2.3 Doubly Robust Estimation and Targeted Maximum Likelihood Estimation (TMLE)","href":"02-03-doubly-robust-tmle.qmd"},{"text":"2.4 Super Learner","href":"02-04-superlearner.qmd"},{"text":"3.3 Longitudinal Case Study","href":"03-03-longitudinal-case-study.qmd"},{"text":"3.4 Effect Modification","href":"03-04-effect-modification.qmd"},{"text":"3.5 Advanced Diagnostics","href":"03-05-advanced-diagnostics.qmd"},{"text":"3.6 RWD meets RCT: Hybrid Designs","href":"03-06-rwd-meets-rct-hybrid-designs.qmd"},{"text":"3.7 Longitudinal Time-Dependent Confounding","href":"03-07-longitudinal-td-confounding.qmd"},{"text":"3.8 TMLE in the Clean Room Design","href":"03-08-tmle-clean-room.qmd"},{"text":"Covariate Adjustment IN RCTs using TMLE","href":"chapter_covariate_adjustment_tmle.qmd"},{"text":"Common Pitfalls in Causal Inference","href":"common-pitfalls.qmd"}]}]}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}