---
title: "Chapter 2.2: Inverse Probability of Treatment Weighting (IPTW)"
format: html
---

# Chapter 2.2: Inverse Probability of Treatment Weighting  

Inverse probability of treatment weighting (IPTW) is a core method in modern causal inference. Instead of modeling the outcome directly as in g computation, IPTW uses a model for **treatment assignment** to create a pseudo population where treatment is independent of confounders.  

In this chapter we will

- Build intuition for IPTW
- Derive the weights and connect them to the causal estimand
- Show how to estimate and diagnose propensity scores
- Implement IPTW in R with tidyverse style
- Discuss stabilized weights and truncation
- Compare IPTW to g computation on the same simulated data

This is still point treatment only. Longitudinal extensions come later.

---

# 1. Intuition: Reweighting to Mimic a Trial

:::{.callout-tip}
## Key Insight: IPTW Creates a Pseudo-Population That Mimics a Randomized Trial

In observational data, some types of patients are more likely to receive one treatment than another. This creates **confounding by indication**. Think of it this way: if sicker patients are preferentially given a new drug, a naive comparison of outcomes between treated and untreated groups will be biased.

IPTW tackles this by

> Giving more weight to underrepresented patients and less weight to overrepresented ones, so that in the weighted sample, treatment looks as if it were randomized given the measured covariates.

The result is a **pseudo-population** --- a reweighted version of your study sample where the distribution of confounders is the same across treatment groups, just as it would be in a well-conducted randomized trial. This is the core idea that makes IPTW so powerful for epidemiologic research.

- Patients who got a treatment that was unlikely for their covariate pattern receive a large weight (they are "surprising" and therefore informative)
- Patients who got the most expected treatment get a small weight (they are "predictable" and already well-represented)

In the weighted pseudo-population, confounders are balanced between treatment groups (if the propensity score model is correctly specified).
:::

:::{.callout-note}
## Definition: The Propensity Score

The **propensity score** is the probability of receiving treatment given measured covariates:

$$
e(W) = P(A = 1 \mid W)
$$

IPTW weights are constructed from the propensity score:

- For treated:  \( \displaystyle w_i = \frac{1}{e(W_i)} \)
- For control: \( \displaystyle w_i = \frac{1}{1 - e(W_i)} \)

The propensity score is a **balancing score**: conditioning on it is sufficient to remove confounding by all measured covariates $W$. This is why a single scalar summary can replace adjustment for many covariates.
:::

---

# 2. Identification Using IPTW

:::{.callout-important}
## The IPTW Identification Result and the Horvitz-Thompson Connection

Under the same causal assumptions as before

- **Consistency**: the observed outcome equals the potential outcome under the treatment actually received
- **Exchangeability**: \( Y(a) \perp A \mid W \) --- no unmeasured confounding
- **Positivity**: every covariate stratum has a non-zero probability of receiving each treatment

the average potential outcome under treatment can be expressed as

$$
E[Y(1)] = E\left[ \frac{I(A = 1) Y}{e(W)} \right]
$$

and under control

$$
E[Y(0)] = E\left[ \frac{I(A = 0) Y}{1 - e(W)} \right]
$$

The IPTW estimator replaces the expectation with the sample average and replaces the true \( e(W) \) with an estimated one.

This is called a **Horvitz-Thompson type** estimator because it has the same structure as the survey sampling estimator that reweights observations by the inverse of their selection probability. In our setting, "selection" is selection into a treatment group. Just as survey statisticians upweight undersampled populations, we upweight patients whose treatment assignment was unlikely given their covariates.
:::

---

# 3. Simulated Example

We reuse a familiar setup: an osteoporosis like population with confounding by age and cardiovascular history.

```{r}
library(tidyverse)

set.seed(2025)
n <- 4000

age <- rnorm(n, 75, 6)
cvd <- rbinom(n, 1, plogis(0.12 * (age - 70)))

# Treatment assignment with strong confounding
A <- rbinom(n, 1, plogis(-1 + 0.09 * (age - 70) + 1.5 * cvd))

# Outcome model
Y <- rbinom(n, 1, plogis(-2 + 0.5*A + 0.10*(age - 70) + 1.0*cvd))

dat <- tibble(age, cvd, A, Y)
dat %>% head()
```

For simplicity

- A = 1 can be thought of as denosumab
- A = 0 can be thought of as zoledronic acid
- Y = 1 indicates occurrence of a cardiovascular outcome

---

# 4. Estimating the Propensity Score

We model the probability of receiving treatment given covariates.

In practice, one might use logistic regression, SuperLearner, or other ML. Here we start with logistic regression for clarity.

```{r}
ps_mod <- glm(A ~ age + cvd, family = binomial, data = dat)
summary(ps_mod)

dat <- dat %>%
  mutate(
    ps = predict(ps_mod, type = "response")
  )

dat %>% 
  summarize(
    min_ps = min(ps),
    max_ps = max(ps)
  )
```

We can visualize the propensity score distribution.

```{r}
ggplot(dat, aes(x = ps, fill = factor(A))) +
  geom_density(alpha = 0.4) +
  labs(
    x = "Estimated propensity score P(A=1 | W)",
    fill = "Treatment",
    title = "Propensity score overlap"
  ) +
  theme_minimal()
```

:::{.callout-tip}
## Reading the Propensity Score Overlap Plot

**Good overlap** looks like two density curves that cover mostly the same range of propensity scores, even if their peaks differ. This means that for most covariate patterns, there are both treated and untreated patients --- exactly what positivity requires.

**Bad overlap** looks like two density curves that barely touch, with the treated group concentrated at high propensity scores and the untreated group concentrated at low ones. When this happens:

- IPTW produces extreme weights for patients in the non-overlapping tails
- Estimates become highly variable and unreliable
- You may need to restrict your target population to the region of overlap (trimming) or reconsider your study design

As a rule of thumb, be concerned if propensity scores for either group pile up near 0 or 1.
:::

---

# 5. Constructing IPTW Weights

:::{.callout-note}
## Unstabilized Weights

Unstabilized weights are the simplest form of IPTW weights. For each individual, the weight is the inverse of the probability of receiving the treatment they actually received:

- Treated ($A = 1$): $w_i = 1 / e(W_i)$
- Untreated ($A = 0$): $w_i = 1 / (1 - e(W_i))$

These weights create a pseudo-population where treatment is independent of measured confounders. However, they can be highly variable --- especially when some propensity scores are close to 0 or 1.
:::

```{r}
dat <- dat %>%
  mutate(
    w_ipw = if_else(A == 1, 1 / ps, 1 / (1 - ps))
  )

summary(dat$w_ipw)
quantile(dat$w_ipw, probs = c(0.01, 0.99))
```

Keep an eye on

- Very large weights (suggest near-violations of positivity)
- Range and extreme quantiles (the 99th percentile should not be orders of magnitude larger than the median)

:::{.callout-tip}
## Why Stabilized Weights Are Preferred in Practice

Stabilized weights multiply the standard IPTW weights by the **marginal probability of treatment**, which brings the weights closer to 1 on average and reduces their variance without introducing bias for the marginal causal effect.

For a binary treatment:

$$
SW_i = \frac{P(A_i)}{e(W_i)} \text{ if } A_i = 1
$$

and

$$
SW_i = \frac{P(A_i)}{1 - e(W_i)} \text{ if } A_i = 0
$$

Here \( P(A_i) \) is the marginal probability of treatment (estimated as the sample proportion treated).

**Why this helps:** Unstabilized weights create a pseudo-population that is larger than the original sample, which inflates standard errors. Stabilized weights preserve the original sample size in expectation, leading to:

- Narrower confidence intervals
- More stable point estimates
- Better finite-sample performance

In epidemiologic practice, **always prefer stabilized weights** unless you have a specific reason to use unstabilized ones.
:::

```{r}
pA <- mean(dat$A)

dat <- dat %>%
  mutate(
    sw_ipw = case_when(
      A == 1 ~ pA / ps,
      A == 0 ~ (1 - pA) / (1 - ps)
    )
  )

summary(dat$sw_ipw)
quantile(dat$sw_ipw, probs = c(0.01, 0.99))
```

---

# 6. Estimating the ATE via IPTW

There are several equivalent ways to use the weights.

## 6.1 Direct weighted mean of outcomes

Estimated risk under treatment

```{r}
risk1_ipw <- with(dat, sum(sw_ipw * Y * (A == 1)) / sum(sw_ipw * (A == 1)))
risk0_ipw <- with(dat, sum(sw_ipw * Y * (A == 0)) / sum(sw_ipw * (A == 0)))

ate_ipw <- risk1_ipw - risk0_ipw
c(risk1 = risk1_ipw, risk0 = risk0_ipw, ate = ate_ipw)
```

This matches the formula

$$
\hat E[Y(1)] = \frac{\sum_i SW_i Y_i I(A_i = 1)}{\sum_i SW_i I(A_i = 1)}
$$

## 6.2 Weighted regression model

We can also fit a weighted regression with treatment as the only predictor.

```{r, eval=FALSE}
library(sandwich)
library(lmtest)

# Fit a simple weighted model
fit_ipw <- glm(Y ~ A, family = binomial, weights = sw_ipw, data = dat)

# Robust standard errors
cov_ipw <- vcovHC(fit_ipw, type = "HC0")
coeftest(fit_ipw, cov_ipw)
```

Interpretation

- The coefficient of A (on the log odds scale) now estimates a marginal effect in the pseudo population
- You can compute marginal risk differences or ratios by predicting from the model at A=1 and A=0 and standardizing

---

# 7. Comparing IPTW and G Computation

We can compare the IPTW ATE to the g computation ATE from the previous chapter.

```{r}
# g computation
g_mod <- glm(Y ~ A + age + cvd, family = binomial, data = dat)
p1_g <- predict(g_mod, newdata = dat %>% mutate(A = 1), type = "response")
p0_g <- predict(g_mod, newdata = dat %>% mutate(A = 0), type = "response")
ate_g <- mean(p1_g - p0_g)

c(ate_gcomp = ate_g, ate_iptw = ate_ipw)
```

Under correct models and adequate positivity, these methods should converge to similar estimates as sample size grows.

:::{.callout-important}
## Critical Takeaway: IPTW and G-Computation Depend on Different Models

This is one of the most important conceptual points in causal inference methodology:

- **G-computation** depends on correctly specifying the **outcome model** $E[Y \mid A, W]$
- **IPTW** depends on correctly specifying the **treatment model** $P(A \mid W)$
- **Neither uses both models at once**

In practice, you rarely know which model is correct. This motivates **doubly robust** methods (next chapter), which combine both approaches and provide consistent estimates if *either* the treatment model *or* the outcome model is correctly specified --- giving you two chances to get it right.

For MPH epidemiologists: when you present IPTW results, always show propensity score diagnostics (overlap, balance) alongside your estimates. When you present g-computation results, discuss outcome model fit. When the two approaches give very different answers, that is a signal that at least one model may be misspecified.
:::

---

# 8. Diagnostics and Practical Tips

IPTW is powerful but fragile if diagnostics are ignored.

:::{.callout-important}
## Diagnostic Checklist: Never Report IPTW Without These Checks

Every IPTW analysis should include diagnostics. Reviewers and readers should expect to see them. Omitting diagnostics is a red flag in any manuscript.
:::

### 8.1 Check propensity score overlap

:::{.callout-note}
## Assessing Positivity via Overlap

- Plot propensity score densities by treatment group (as we did in Section 4)
- Flag regions where one group is missing or sparse
- If the treated and untreated distributions do not overlap, the ATE may not be identifiable in that region
- Consider restricting inference to the overlap population or reporting the ATT instead
:::

### 8.2 Check weights

```{r}
dat %>%
  summarize(
    min_w = min(sw_ipw),
    max_w = max(sw_ipw),
    mean_w = mean(sw_ipw),
    sd_w = sd(sw_ipw)
  )
```

:::{.callout-warning}
## When Extreme Weights Signal Trouble

Large extreme weights are a red flag. Watch for these warning signs:

- **Maximum weight > 10--20**: a single observation is being counted as 10--20 people. Your estimate may be driven by one or two individuals.
- **Mean of stabilized weights far from 1**: stabilized weights should average approximately 1. Deviations suggest model misspecification.
- **Ratio of max to median weight > 20**: indicates near-violation of positivity for some covariate patterns.

When you see extreme weights, consider: (1) improving your propensity score model, (2) checking for data errors in the extreme-weight observations, and (3) applying weight truncation (see below).
:::

### 8.3 Consider truncation

You can truncate weights at a chosen percentile, for example the 1st and 99th percentiles.

```{r}
lower <- quantile(dat$sw_ipw, 0.01)
upper <- quantile(dat$sw_ipw, 0.99)

dat <- dat %>%
  mutate(
    sw_trunc = pmin(pmax(sw_ipw, lower), upper)
  )

# recompute ATE with truncated weights
risk1_trunc <- with(dat, sum(sw_trunc * Y * (A == 1)) / sum(sw_trunc * (A == 1)))
risk0_trunc <- with(dat, sum(sw_trunc * Y * (A == 0)) / sum(sw_trunc * (A == 0)))
ate_trunc   <- risk1_trunc - risk0_trunc

c(ate_truncated = ate_trunc)
```

:::{.callout-caution}
## The Truncation Trade-Off: Bias vs. Variance

Truncation (also called "winsorizing") reduces variance at the cost of introducing some bias. By capping extreme weights, you are effectively saying: "I am willing to accept a small amount of bias in exchange for a much more stable estimate."

**The trade-off in practice:**

- **Without truncation**: unbiased (if the propensity model is correct) but potentially high variance, with confidence intervals that may be uninformatively wide
- **With truncation**: slightly biased but often lower mean squared error, producing more useful estimates

**Common approaches:**

- Truncate at the 1st and 99th percentiles (as shown above)
- Truncate at fixed values (e.g., cap weights at 10 or 20)
- Report results with and without truncation as a sensitivity analysis

There is no single "right" truncation threshold. In your manuscripts, always report which threshold you used and show sensitivity to this choice.
:::

---

# 9. Summary

:::{.callout-tip}
## Chapter Takeaways and Limitations

**What we covered:**

- The intuition for IPTW as a method that reweights observational data to mimic a randomized trial
- The key role of the propensity score in computing weights, and its connection to the Horvitz-Thompson estimator from survey sampling
- How to construct unstabilized and stabilized IPTW weights (prefer stabilized in practice)
- How to estimate ATEs via weighted means or weighted regression
- How to diagnose propensity score overlap and extreme weights --- never skip these diagnostics
- How IPTW compares to g-computation, and why each depends on a different model

**Limitations to keep in mind:**

- Depends entirely on a correctly specified treatment model --- if you miss a confounder or misspecify the functional form, your weights will not balance the confounders
- Sensitive to violations of positivity --- extreme weights can dominate your estimates
- Does not use information from the outcome model, which motivates doubly robust approaches
- For rare exposures or outcomes, IPTW can be particularly unstable

**Looking ahead:** In the next chapter, we will introduce **doubly robust estimators** and **Targeted Maximum Likelihood Estimation (TMLE)**, which combine the strengths of g-computation and IPTW and allow the use of machine learning for both nuisance models.
:::

```{r}
sessionInfo()
```
