---
title: 'Chapter 1.1: Limitations of Standard Regression Analyses'
format: html
---

In this chapter, we motivate the need for modern causal inference tools by walking through real-world examples where traditional regression fails. We also introduce the counterfactual (potential outcomes) framework as the foundation for defining causal effects.

::: {.callout-note}
## Learning objectives
- Understand why regression coefficients do not, in general, equal causal effects
- Recognize confounding, collider bias, and table-2 fallacy in routine analyses
- Articulate the counterfactual (potential outcomes) framework for defining causal contrasts
- Distinguish conditional associations from marginal causal effects
- Motivate the need for the Causal Roadmap and modern estimation methods (g-computation, IPTW, TMLE)
:::

::: {.callout-important}
## Sources and scope
This chapter is educational. Causal conclusions depend on identification assumptions (e.g., consistency, exchangeability, positivity) and on diagnostic evidence that the data support the target estimand. When flexible machine learning is used for nuisance estimation, valid inference typically requires cross-fitting or a cross-validated TMLE variant, plus appropriate rate conditions.
:::

```{r}
#| include: false
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 4,
  fig.align = "center",
  message = FALSE,
  warning = FALSE
)
```

## Why Not Just Use Regression?

:::{.callout-caution}
## The "Just Run a Regression" Trap

Multivariable regression has long been the standard tool in observational research for "adjusting" for confounders. But regression coefficients don't always represent causal effects. In fact, they almost never do without additional assumptions and deliberate analytic choices that go beyond what standard regression provides.

If you've been trained to interpret an adjusted regression coefficient as "the effect of X on Y, holding all else equal," you're in good company — but that interpretation quietly smuggles in causal language that the model itself cannot justify. The coefficient tells you about the **conditional association** within strata defined by the other variables in the model. Whether that association equals a causal effect depends on assumptions that regression alone cannot verify.
:::

Let's consider a motivating example based on a real-world study comparing two osteoporosis treatments: **denosumab** (Prolia) and **zoledronic acid**.

### Motivating Example: Comparing Osteoporosis Treatments

Imagine we're interested in whether patients who initiate denosumab have higher 3-year risk of heart attack or stroke compared to those initiating zoledronic acid.

We might be tempted to run the following:

```{r}
# simulate crude comparison (not real data)
set.seed(123)
n <- 2000
treatment <- rbinom(n, 1, 0.5) # 1 = denosumab, 0 = zoledronic acid
age <- rnorm(n, 75, 6)
cvd_history <- rbinom(n, 1, plogis(0.1 * (age - 70)))

# outcome depends on age and history
risk <- plogis(-2 + 0.4 * treatment + 0.05 * (age - 70) + 1 * cvd_history)
outcome <- rbinom(n, 1, risk)

model1 <- glm(outcome ~ treatment, family = binomial)
summary(model1)
```

:::{.callout-warning}
## What the Crude Estimate Conflates

This crude estimate of association is confounded by differences in age and comorbidities between the treatment groups. Because older patients and those with cardiovascular disease history may be preferentially channeled toward one treatment over another, the crude coefficient bundles together (1) any true treatment effect, (2) the influence of age on the outcome, and (3) the influence of CVD history on the outcome. You cannot disentangle these from a simple unadjusted regression — the estimate is a mixture of the treatment's association with the outcome and the confounders' effects. This is the classic **confounding bias** problem that motivates adjustment, but as we will see, adjustment alone is not enough.
:::

### Adjusting for Confounders

We add adjustment:

```{r}
model2 <- glm(outcome ~ treatment + age + cvd_history, family = binomial)
summary(model2)
```

:::{.callout-important}
## The Adjusted Coefficient Is Still NOT a Causal Effect

Even after adjustment, the treatment coefficient from logistic regression is a **conditional log-odds ratio** — it tells you how the log-odds of the outcome change when comparing treated vs. untreated individuals *who share the same values of age and CVD history*. This is fundamentally different from a **marginal causal effect** (like a risk difference or risk ratio averaged over the whole population).

Here is why this matters for your research:

- **Conditional vs. marginal:** The conditional log-odds ratio describes an association *within strata*. The marginal risk difference describes what would happen *at the population level* if everyone were treated vs. if everyone were untreated. These answer different questions, and in nonlinear models (like logistic regression), they are generally not equal — even if the model is correctly specified.
- **Non-collapsibility of the odds ratio:** Even without confounding, a conditional odds ratio and a marginal odds ratio will differ. This is a mathematical property of the odds ratio, not a bias — but it means you cannot simply interpret the adjusted odds ratio as "the population-level effect."
- **Parametric assumptions baked in:** The model assumes the effect of age is log-linear, that there are no treatment-covariate interactions, and that the logistic link is correct. If any of these are wrong, the coefficient is biased for *any* causal quantity.

Bottom line: adjustment gets you closer, but the regression coefficient from a logistic model is not, in general, the average treatment effect you are looking for.
:::

## Enter the Counterfactual Framework

:::{.callout-note}
## Defining Causal Effects with Potential Outcomes

To define a causal effect precisely, we introduce **counterfactual (potential) outcomes**. For each person in the study, we imagine two hypothetical worlds:

- **Y(1):** the outcome that *would* occur if the person received denosumab
- **Y(0):** the outcome that *would* occur if the person received zoledronic acid

The **individual causal effect** is Y(1) - Y(0) for a single person. The **average treatment effect (ATE)** is the population average of these individual effects:

$$
ATE = E[Y(1) - Y(0)]
$$

The fundamental problem of causal inference is that each person only receives one treatment — we observe Y(1) or Y(0), never both. The counterfactual under the other treatment is missing data.

Under three key identification assumptions — **exchangeability** (no unmeasured confounding), **consistency** (the observed outcome under the treatment actually received equals the potential outcome), and **positivity** (every individual has a nonzero probability of receiving either treatment given their covariates) — we can identify the causal ATE from observed data using the **g-computation formula**:

$$
ATE = E_W\big[ E[Y \mid A = 1, W] - E[Y \mid A = 0, W] \big]
$$

where W represents the measured confounders and A is the treatment. This expression says: within each confounder stratum, compute the difference in expected outcomes between treated and untreated, then average over the distribution of confounders in the population.

This formula motivates **standardization (g-computation)**, **inverse probability of treatment weighting (IPTW)**, and **targeted minimum loss-based estimation (TMLE)** — all of which are designed to estimate this population-level causal quantity.
:::

## When Regression Fails

:::{.callout-warning}
## Model Misspecification Can Silently Bias Your Results

Even if we include the right confounders, regression can still give misleading answers if the **functional form** is wrong. Standard regression assumes specific relationships — linearity on the log-odds scale, no interactions between treatment and covariates, and correct specification of how confounders relate to the outcome. In practice, these assumptions are rarely checked and often violated.

The danger is that misspecification bias is **silent**: the model will still produce coefficients, standard errors, and p-values. Nothing in the output warns you that the answer is wrong. This is one of the strongest motivations for using more flexible, semiparametric estimators (like TMLE with machine learning) that can relax these assumptions.
:::

Let's compare regression to a nonparametric substitution estimator (g-computation):

```{r}
# Fit model
model3 <- glm(outcome ~ treatment + age + cvd_history, family = binomial)

# Predict counterfactual outcomes
newdata1 <- data.frame(treatment = 1, age = age, cvd_history = cvd_history)
p1 <- predict(model3, newdata = newdata1, type = "response")

newdata0 <- data.frame(treatment = 0, age = age, cvd_history = cvd_history)
p0 <- predict(model3, newdata = newdata0, type = "response")

# Estimate marginal risk difference
mean(p1 - p0)  # this is g-computation
```

:::{.callout-tip}
## Why G-Computation Gives You a Population-Level Causal Effect

The g-computation procedure above does something fundamentally different from reading off a regression coefficient — even though it *uses* the same regression model under the hood.

Here is the key insight: instead of reporting a conditional coefficient, g-computation **predicts** what would happen to *every person in the dataset* under treatment (A = 1) and under control (A = 0), then **averages** the difference across the whole population. This is the standardization step, and it maps directly onto the g-computation formula from the counterfactual framework.

The result is a **marginal risk difference** — the estimated change in population-level risk if everyone were treated with denosumab versus if everyone were treated with zoledronic acid. This is the quantity that policymakers, clinicians, and patients actually care about.

By contrast, the regression coefficient `coef(model3)["treatment"]` gives you a **conditional log-odds ratio**, which lives on a different scale, answers a different question, and (because the odds ratio is non-collapsible) cannot be directly translated into a population-level effect.
:::

Compare this to the regression coefficient:

```{r}
coef(model3)["treatment"]
```

The coefficient gives you a conditional odds ratio, but the g-comp version gives you a marginal risk difference — a more interpretable, population-level quantity.

## Summary

:::{.callout-tip}
## Key Takeaways

- **Regression is not inherently causal** — it estimates conditional associations under strong model assumptions. An adjusted coefficient is not the same as a causal effect.
- **Causal inference starts with a question, not a model** — define your causal question and target estimand (e.g., the ATE on the risk difference scale) *before* choosing an analytic method.
- **The counterfactual framework clarifies what we want to estimate** — Y(1), Y(0), and the ATE give us precise, model-free definitions of causal effects.
- **G-computation, IPTW, and TMLE estimate marginal causal effects** — these methods target the population-level quantity defined by the g-computation formula, rather than relying on regression coefficients that conflate the estimand with the model.
- **The conditional log-odds ratio from logistic regression is not the ATE** — this is perhaps the single most important distinction for epidemiologists moving from traditional regression to causal inference methods.
:::

## Next

In the next chapter, we'll introduce the Causal Roadmap — a structured workflow for planning and executing causal analyses.

---

## Sources and further reading

- Hernan MA, Robins JM (2020). *Causal Inference: What If*. Chapman & Hall/CRC. [Free online](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)
- Greenland S, Pearl J, Robins JM (1999). Causal diagrams for epidemiologic research. *Epidemiology* 10(1):37-48.
- Pearl J (2009). *Causality: Models, Reasoning, and Inference*. 2nd ed. Cambridge University Press.
- Westreich D, Greenland S (2013). The table 2 fallacy: presenting and interpreting confounder and modifier coefficients. *Am J Epidemiol* 177(4):292-298.
- van der Laan MJ, Rose S (2011). *Targeted Learning*. Springer.
- Petersen ML, van der Laan MJ (2014). Causal models and learning from data. *Epidemiology* 25(3):418-426.
- `tmle` R package: [CRAN](https://cran.r-project.org/package=tmle)
- `SuperLearner` R package: [CRAN](https://cran.r-project.org/package=SuperLearner)

---

## Software Implementation (R)

This example uses the `tmle` package to estimate the **average treatment effect (ATE)** for a point treatment, and contrasts the result with a naive regression coefficient.

- Simulate a simple confounded dataset: $W$ causes both $A$ and $Y$
- Naive regression of $Y$ on $A$ (ignoring confounding) yields a biased coefficient
- The `tmle` package estimates the ATE using outcome modeling + targeting
- Compare: the naive coefficient ≠ the causal ATE

```{r}
#| eval: false
set.seed(1)
n <- 500
W <- rnorm(n)
A <- rbinom(n, 1, plogis(0.5 * W))
Y <- 0.8 * A + 1.2 * W + rnorm(n, sd = 0.5)  # true ATE = 0.8

## ── Naive regression (biased) ──
naive_fit <- lm(Y ~ A)
cat("Naive regression coefficient on A:", round(coef(naive_fit)["A"], 3), "\n")

## ── TMLE (targets the ATE) ──
if (requireNamespace("tmle", quietly = TRUE)) {
  library(tmle)
  tmle_fit <- tmle(
    Y = Y, A = A,
    W = data.frame(W = W),
    Q.SL.library = "SL.glm",
    g.SL.library  = "SL.glm"
  )
  cat("TMLE ATE estimate:", round(tmle_fit$estimates$ATE$psi, 3), "\n")
  cat("95% CI:", round(tmle_fit$estimates$ATE$CI, 3), "\n")
} else {
  message("Install the 'tmle' package:  install.packages('tmle')")
}
```
