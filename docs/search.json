[
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "(to do: add descriptions of each)\n\nBeyondTheATE.com\nTLverse handbook\nIntroduction to modern causal inference\nVisual guides to causal inference\n\n-https://vanderlaan-lab.org/ -https://ehsanx.github.io/TMLEworkshop/\n-2 workshops from Laura"
  },
  {
    "objectID": "cases/index.html",
    "href": "cases/index.html",
    "title": "Case study applications of the causal roadmap and targeted learning",
    "section": "",
    "text": "Note all data is simulated and estimates of drug effects are for teaching purposes only and do not reflect real-world efficacy.\n\nART adherence and HIV virologic suppression\n(Draft in progress) HIV adherence analysis plan and simulation results\nThis roadmap analysis plan defines a longitudinal causal estimand to evaluate how imperfect adherence to antiretroviral therapy affects HIV virologic suppression under realistic, observed treatment patterns. Using a structural equation model and directed acyclic graph, the plan formalizes the impact of intercurrent events such as treatment switching, disenrollment, and differential monitoring. An example analysis is implemented on HealthVerity claims data to estimate adherence–outcome relationships using longitudinal targeted maximum likelihood estimation (TMLE), adjusted for time-varying confounding and censoring. A simulation study then replicates key data-generating features—pharmacy fill patterns, lab engagement, and switching behavior—under varying degrees of adherence and confounding, to benchmark estimator performance and assess bias in standard Cox models versus TMLE under the causal roadmap framework\n\n\nCausal Roadmap case study of an acute kidney injury safety analysis\nhttps://bookdown.org/amertens/causal-roadmap-tutorial/"
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html",
    "href": "workshop/tmle_osteoporosis_workshop.html",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "",
    "text": "Learn to use the Targeted Learning roadmap for causal inference.\nApply G-computation, IPTW, and TMLE using R.\nImplement Super Learner for robust data-adaptive estimation.\nDesign a Targeted Learning-based Statistical Analysis Plan (SAP) aligned with ICH E9(R1).\n\n\n\n\n\nDefine and identify causal estimands.\nCompare classical and modern estimators.\nApply TMLE to estimate treatment safety effects.\nDevelop a pre-specified, reproducible SAP for real-world evidence."
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#workshop-goals",
    "href": "workshop/tmle_osteoporosis_workshop.html#workshop-goals",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "",
    "text": "Learn to use the Targeted Learning roadmap for causal inference.\nApply G-computation, IPTW, and TMLE using R.\nImplement Super Learner for robust data-adaptive estimation.\nDesign a Targeted Learning-based Statistical Analysis Plan (SAP) aligned with ICH E9(R1)."
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#learning-outcomes",
    "href": "workshop/tmle_osteoporosis_workshop.html#learning-outcomes",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "",
    "text": "Define and identify causal estimands.\nCompare classical and modern estimators.\nApply TMLE to estimate treatment safety effects.\nDevelop a pre-specified, reproducible SAP for real-world evidence."
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#motivation",
    "href": "workshop/tmle_osteoporosis_workshop.html#motivation",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "Motivation",
    "text": "Motivation\n\nEstimating the causal effect of osteoporosis drug use on fracture and cardiovascular safety. ## The Causal Roadmap\n\n\nDefine the question and estimand.\nSpecify causal and statistical models.\nAssess identifiability.\nChoose estimators and perform inference. ## Data Example\n\n\nObservational dataset: women &gt;50 with osteoporosis treatment.\nExposure: bisphosphonate vs. selective estrogen receptor modulator.\nOutcome: cardiovascular adverse event."
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#data-preprocessing",
    "href": "workshop/tmle_osteoporosis_workshop.html#data-preprocessing",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nImport, clean, and summarize variables.\nIdentify confounders and missing data patterns. ## Variable Definitions\nA: exposure (treatment type)\nY: outcome (adverse event)\nW: baseline covariates (age, comorbidities, medication history) ## Visualizing Data\nCovariate balance plots\nDAG for causal structure"
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#concept",
    "href": "workshop/tmle_osteoporosis_workshop.html#concept",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "Concept",
    "text": "Concept\n\nModel outcome E[Y|A,W].\nPredict counterfactual outcomes for all individuals under each treatment. ## Implementation in R\n\n\n\nCode\nlibrary(SuperLearner)\nQ.SL &lt;- SuperLearner(Y, X, family=binomial(), SL.library=c(\"SL.glm\",\"SL.glmnet\",\"SL.xgboost\"))"
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#interpretation",
    "href": "workshop/tmle_osteoporosis_workshop.html#interpretation",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "Interpretation",
    "text": "Interpretation\n\nEstimate mean difference in predicted risk (ATE).\nDiscuss bias under model misspecification."
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#concept-1",
    "href": "workshop/tmle_osteoporosis_workshop.html#concept-1",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "Concept",
    "text": "Concept\n\nEstimate propensity score P(A|W).\nWeight each subject by inverse probability of observed treatment. ## Implementation in R\n\n\n\nCode\ng.SL &lt;- SuperLearner(A, W, family=binomial(), SL.library=c(\"SL.glm\",\"SL.gam\",\"SL.xgboost\"))"
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#diagnostics",
    "href": "workshop/tmle_osteoporosis_workshop.html#diagnostics",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nCheck covariate balance after weighting.\nPlot stabilized weights. ## Interpretation\nCompare with G-computation estimates."
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#conceptual-framework",
    "href": "workshop/tmle_osteoporosis_workshop.html#conceptual-framework",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "Conceptual Framework",
    "text": "Conceptual Framework\n\nInitial outcome regression Q^0\nPropensity score model g\nTargeting step using clever covariate H(A,W)\nCompute updated estimate Q^*\nEstimate ATE ## Implementation\n\n\n\nCode\nlibrary(tmle)\ntmle_fit &lt;- tmle(Y, A, W, family=\"binomial\", Q.SL.library=Q.SL.library, g.SL.library=g.SL.library)\nsummary(tmle_fit)"
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#inference",
    "href": "workshop/tmle_osteoporosis_workshop.html#inference",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "Inference",
    "text": "Inference\n\nEfficient influence curve\nStandard errors and 95% CI"
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#why-super-learner",
    "href": "workshop/tmle_osteoporosis_workshop.html#why-super-learner",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "Why Super Learner?",
    "text": "Why Super Learner?\n\nCombine algorithms for optimal predictive performance. ## Building a Library\nInclude SL.glm, SL.gam, SL.xgboost, SL.ranger, SL.mean. ## Practical Tips\nCross-validation folds\nScreening and variable selection\nParallel computation"
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#assessing-assumptions",
    "href": "workshop/tmle_osteoporosis_workshop.html#assessing-assumptions",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "Assessing Assumptions",
    "text": "Assessing Assumptions\n\nExchangeability, positivity, consistency ## Sensitivity Analyses\nTruncating extreme weights\nComparing parametric vs. SL fits\nAssessing violations (e.g., unmeasured confounding)"
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#introduction-1",
    "href": "workshop/tmle_osteoporosis_workshop.html#introduction-1",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "Introduction",
    "text": "Introduction\n\nTMLE for time-to-event data (cardiovascular safety)\nCensoring and competing risks ## Using the lmtp Package\n\n\n\nCode\nlibrary(lmtp)\nfit_lmtp &lt;- lmtp_tmle(data, trt=\"drug\", outcome=\"cv_event\", time_vary=list(...))"
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#interpretation-1",
    "href": "workshop/tmle_osteoporosis_workshop.html#interpretation-1",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "Interpretation",
    "text": "Interpretation\n\nRisk difference or hazard difference at time t\nVisualizing survival curves"
  },
  {
    "objectID": "workshop/tmle_osteoporosis_workshop.html#alignment-with-ich-e9r1",
    "href": "workshop/tmle_osteoporosis_workshop.html#alignment-with-ich-e9r1",
    "title": "Targeted Learning Workshop: Safety and Effectiveness of Osteoporosis Treatment",
    "section": "Alignment with ICH E9(R1)",
    "text": "Alignment with ICH E9(R1)\n\nPopulation, treatment, variable, intercurrent events, summary measure. ## Checklist\n\n\nDefine estimand.\nSpecify identification assumptions.\nDescribe analytic strategy (TMLE + SL).\nPlan sensitivity analyses. ## Example Output\n\n\nStructured SAP for osteoporosis safety study."
  },
  {
    "objectID": "workshop/05-integrated-analysis.html",
    "href": "workshop/05-integrated-analysis.html",
    "title": "5. Putting It All Together",
    "section": "",
    "text": "Integrating the Steps\nIn this final section, we bring together all steps of the Causal Roadmap and Targeted Learning pipeline.\n\n\nCode\n# Full workflow\n\n\n Figure 5. Integrating the Causal Roadmap and Targeted Learning for real-world evidence."
  },
  {
    "objectID": "workshop/03-tmle.html",
    "href": "workshop/03-tmle.html",
    "title": "3. TMLE Concepts and Implementation",
    "section": "",
    "text": "Why TMLE?\nTMLE provides a robust, efficient estimator for causal effects combining outcome regression and propensity score modeling.\nIt is doubly robust and allows the use of machine learning while preserving valid inference.\n\n\nCode\nlibrary(tmle)\nlibrary(SuperLearner)\nset.seed(123)\nfit &lt;- tmle(Y, A, W, family = \"binomial\",\n            Q.SL.library = c(\"SL.glm\", \"SL.glmnet\", \"SL.ranger\"),\n            g.SL.library = c(\"SL.glm\", \"SL.ranger\"))\nsummary(fit)\n\n\n Initial estimation of Q\n     Procedure: cv-SuperLearner, ensemble\n     Model:\n         Y ~  SL.glm_All + SL.glmnet_All + SL.ranger_All\n\n     Coefficients: \n          SL.glm_All    1 \n       SL.glmnet_All    0 \n       SL.ranger_All    0 \n\n     Cross-validated pseudo R squared :  0.0489 \n\n Estimation of g (treatment mechanism)\n     Procedure: SuperLearner, ensemble\n     Model:\n         A ~  SL.glm_All + SL.ranger_All \n\n     Coefficients: \n          SL.glm_All    1 \n       SL.ranger_All    0 \n\n Estimation of g.Z (intermediate variable assignment mechanism)\n     Procedure: No intermediate variable \n\n Estimation of g.Delta (missingness mechanism)\n     Procedure: No missingness, ensemble\n\n Bounds on g: (0.0229, 1) \n\n Bounds on g for ATT/ATC: (0.0229, 0.9771) \n\n Marginal Mean under Treatment (EY1)\n   Parameter Estimate:  0.8545\n   Estimated Variance:  0.00021034\n              p-value:  &lt;2e-16\n    95% Conf Interval:  (0.82607, 0.88292)\n\n Marginal Mean under Comparator (EY0)\n   Parameter Estimate:  0.7213\n   Estimated Variance:  0.00049874\n              p-value:  &lt;2e-16\n    95% Conf Interval:  (0.67753, 0.76508)\n\n Additive Effect\n   Parameter Estimate:  0.1332\n   Estimated Variance:  0.00070162\n              p-value:  4.9435e-07\n    95% Conf Interval:  (0.081279, 0.18511)\n\n Additive Effect among the Treated\n   Parameter Estimate:  0.12591\n   Estimated Variance:  0.00069858\n              p-value:  1.9004e-06\n    95% Conf Interval:  (0.074105, 0.17771)\n\n Additive Effect among the Controls\n   Parameter Estimate:  0.14433\n   Estimated Variance:  0.0007439\n              p-value:  1.2115e-07\n    95% Conf Interval:  (0.090873, 0.19779)\n\n Relative Risk\n   Parameter  Estimate:  1.1847\n   Variance(log scale):  0.0012346\n               p-value:  1.4158e-06\n     95% Conf Interval:  (1.1058, 1.2691)\n\n Odds Ratio\n    Parameter  Estimate:  2.2691\n    Variance(log scale):  0.02565\n                p-value:  3.1176e-07\n      95% Conf Interval:  (1.6578, 3.1059)\n\n\n Figure 3. TMLE step-by-step schematic."
  },
  {
    "objectID": "workshop/01-intro-case-study.html",
    "href": "workshop/01-intro-case-study.html",
    "title": "1. Introduction & Case Study: Post-Market Safety of Prolia",
    "section": "",
    "text": "Background\nThis lesson introduces our motivating example: evaluating the post-market cardiovascular safety of denosumab (Prolia) compared to zoledronic acid.\nWe’ll use this as our running example throughout the workshop to illustrate the Causal Roadmap.\n Figure 1. Study schematic for the Prolia vs. Zoledronic Acid cardiovascular safety study.\n\n\nCode\n#romain's EHR simulation package\n\nremotes::install_github(\"romainkp/LtAtStructuR\")\nlibrary(LtAtStructuR)\nlibrary(data.table)\nlibrary(lubridate)\nlibrary(future) # optional (for parallel processing)\n#plan(multiprocess) # optional (for parallel processing)\n\n## Define one cohort dataset, one exposure dataset, and one or more covariate\n## datasets\ncohort &lt;- setCohort(cohortDT, \"ID\", \"IndexDate\", \"EOFDate\", \"EOFtype\",\n                    \"AMI\", c(\"ageEntry\", \"sex\", \"race\", \"A1c\", \"eGFR\"),\n                    list(\"ageEntry\"=list(\"categorical\"=FALSE,\n                                        \"impute\"=NA,\n                                        \"impute_default_level\"=NA),\n                        \"sex\"=list(\"categorical\"=TRUE,\n                                   \"impute\"=NA,\n                                   \"impute_default_level\"=NA),\n                        \"race\"=list(\"categorical\"=TRUE,\n                                    \"impute\"=NA,\n                                    \"impute_default_level\"=NA)) )\nexposure &lt;- setExposure(expDT, \"ID\", \"startA\", \"endA\")\ncovariate1 &lt;- setCovariate(a1cDT, \"sporadic\", \"ID\", \"A1cDate\", \"A1c\",\n                           categorical = FALSE)\ncovariate2 &lt;- setCovariate(egfrDT, \"sporadic\", \"ID\", \"eGFRDate\", \"eGFR\",\n                           categorical = TRUE)\n\n## Gather each input dataset into a single object that specifies the content of\n## the output dataset to be constructed\nLtAt.specification &lt;- cohort + exposure + covariate1 + covariate2\n\n## Construct the output dataset\nLtAt.data &lt;- construct(LtAt.specification, time_unit = 15, first_exp_rule = 1,\n                       exp_threshold = 0.75)\nhead(LtAt.data)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Targeted Learning for Real-World Evidence",
    "section": "",
    "text": "Welcome!\nThis site introduces the Causal Roadmap, Targeted Maximum Likelihood Estimation (TMLE), and Super Learner for modern causal inference in epidemiology, with an emphasis on pharmacoepidemiology and clinical trial analysis.\n\n\nCode\n# to build the site\n\nlibrary(quarto)\nquarto::quarto_preview()\n\n#code to deploy\nquarto::quarto_publish_site(title=\"causal-learning-handbook\")"
  },
  {
    "objectID": "workshop/02-causal-roadmap.html",
    "href": "workshop/02-causal-roadmap.html",
    "title": "2. The Causal Roadmap",
    "section": "",
    "text": "Defining the Causal Question\nWe start by expressing our scientific goal in causal terms:\n\nWhat is the 3-year risk difference in cardiovascular events if everyone were treated with denosumab versus zoledronic acid?\n\n\n\nIdentify the Target Population and Assumptions\nWe use the Causal Roadmap to structure the study design and analysis.\n Figure 2. The Causal Roadmap applied to the Prolia example.\n\n\nCode\n# Simulated example of data setup\nset.seed(123)\nn &lt;- 1000\nW &lt;- data.frame(age = rnorm(n, 70, 8), ckd = rbinom(n, 1, 0.2))\nA &lt;- rbinom(n, 1, plogis(-1 + 0.02*W$age + 0.6*W$ckd))\nY &lt;- rbinom(n, 1, plogis(-2 + 0.8*A + 0.04*W$age + 0.3*W$ckd))"
  },
  {
    "objectID": "workshop/04-superlearner.html",
    "href": "workshop/04-superlearner.html",
    "title": "4. SuperLearner and Ensemble Methods",
    "section": "",
    "text": "The Idea Behind Super Learner\nSuper Learner combines multiple models into one optimal prediction algorithm using cross-validation.\n\n\nCode\nlibrary(SuperLearner)\n\n\nLoading required package: nnls\n\n\nLoading required package: gam\n\n\nLoading required package: splines\n\n\nLoading required package: foreach\n\n\nLoaded gam 1.22-5\n\n\nSuper Learner\n\n\nVersion: 2.0-29\n\n\nPackage created on 2024-02-06\n\n\nCode\nset.seed(123)\n#sl_lib &lt;- c(\"SL.glm\", \"SL.mean\", \"SL.glmnet\", \"SL.randomForest\")\nsl_lib &lt;- c(\"SL.glm\") #simpler library for speed\nSL_fit &lt;- SuperLearner(Y, X = W, newX = W, family = binomial(), SL.library = sl_lib)\nsummary(SL_fit)\n\n\n                  Length Class  Mode       \ncall                 6   -none- call       \nlibraryNames         1   -none- character  \nSL.library           2   -none- list       \nSL.predict        1000   -none- numeric    \ncoef                 1   -none- numeric    \nlibrary.predict   1000   -none- numeric    \nZ                 1000   -none- numeric    \ncvRisk               1   -none- numeric    \nfamily              13   family list       \nfitLibrary           1   -none- list       \ncvFitLibrary         0   -none- NULL       \nvarNames             2   -none- character  \nvalidRows           10   -none- list       \nmethod               3   -none- list       \nwhichScreen          2   -none- logical    \ncontrol              3   -none- list       \ncvControl            4   -none- list       \nerrorsInCVLibrary    1   -none- logical    \nerrorsInLibrary      1   -none- logical    \nmetaOptimizer        8   nnls   list       \nenv                  9   -none- environment\ntimes                3   -none- list       \n\n\n Figure 4. Ensemble learning in Super Learner."
  },
  {
    "objectID": "workshop/index.html",
    "href": "workshop/index.html",
    "title": "Workshop: The Causal Roadmap and TMLE in Pharmacoepidemiology",
    "section": "",
    "text": "This workshop introduces the Causal Roadmap and Targeted Maximum Likelihood Estimation (TMLE) through a motivating example: a post-market evaluation of cardiovascular safety among patients treated with denosumab (Prolia) vs. zoledronic acid for osteoporosis. The goal is to show how causal inference frameworks help produce transparent, reproducible real-world evidence.\nIn 2023, Amgen conducted a large-scale retrospective cohort study across two US claims databases, comparing denosumab with zoledronic acid. After adjusting for confounding using inverse probability weighting, the study found no increased risk of myocardial infarction or stroke up to 36 months of follow-up【204†source】. Here, we will reconstruct a simplified version of this question using the causal roadmap and a TMLE implementation.\n\n\n\n\n\n\nNote\n\n\n\nGoal: Learn how to define, identify, and estimate a causal effect with TMLE, using machine learning for nuisance function estimation.\n\n\n Figure 1. The Causal Roadmap, adapted for pharmacoepidemiologic safety analysis."
  },
  {
    "objectID": "workshop/index.html#why-tmle",
    "href": "workshop/index.html#why-tmle",
    "title": "Workshop: The Causal Roadmap and TMLE in Pharmacoepidemiology",
    "section": "4.1 Why TMLE?",
    "text": "4.1 Why TMLE?\nAs described by Katherine Hoffman in An Illustrated Guide to TMLE【202†source】【203†source】, TMLE: - Targets a specific estimand (e.g., ATE), rather than model coefficients. - Incorporates data-adaptive methods like Super Learner. - Provides valid standard errors and confidence intervals."
  },
  {
    "objectID": "workshop/index.html#why-super-learner",
    "href": "workshop/index.html#why-super-learner",
    "title": "Workshop: The Causal Roadmap and TMLE in Pharmacoepidemiology",
    "section": "4.2 Why Super Learner?",
    "text": "4.2 Why Super Learner?\nSuper Learner (SL) combines multiple algorithms (GLM, LASSO, random forests, etc.) through cross-validation to minimize prediction error. This ensemble approach ensures flexibility and robustness when estimating nuisance parameters.\n\n\nCode\nlibrary(SuperLearner)\n\n\nLoading required package: nnls\n\n\nLoading required package: gam\n\n\nLoading required package: splines\n\n\nLoading required package: foreach\n\n\nLoaded gam 1.22-5\n\n\nSuper Learner\n\n\nVersion: 2.0-29\n\n\nPackage created on 2024-02-06\n\n\nCode\nset.seed(123)\nsl_libs &lt;- c('SL.glm', 'SL.glmnet', 'SL.ranger', 'SL.earth')"
  },
  {
    "objectID": "advanced/index.html",
    "href": "advanced/index.html",
    "title": "Advanced Targeted Learning Methods",
    "section": "",
    "text": "to add:\n\n-dynamic and stoachastic treatment regimes\n-ltmle labs"
  },
  {
    "objectID": "bibliography/index.html",
    "href": "bibliography/index.html",
    "title": "Annotated Bibliography: Modern Causal Inference and Targeted Learning",
    "section": "",
    "text": "This bibliography introduces key readings in modern causal inference, targeted learning, and real-world evidence generation. Each annotation summarizes why the paper is important and what it contributes to causal reasoning and applied biostatistics.\n\n\nTargeted learning in real-world comparative effectiveness research with time-varying interventions https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6099\n\n\n\n\nDang et al. (2023) – A causal roadmap for generating high-quality real-world evidence.\nIntroduces the Causal Roadmap framework for structuring causal analyses in observational data. Emphasizes transparency, assumptions, and reproducibility in real-world evidence.\nGruber et al. (2023) – Evaluating and improving real-world evidence with Targeted Learning.\nApplies the roadmap to re-analyze published results using TMLE, highlighting the link between causal identification and robust estimation.\nWilliamson et al. (2023) – An application of the Causal Roadmap in two safety monitoring case studies.\nDemonstrates roadmap principles in practice for safety monitoring and outcome prediction using electronic health records.\nHernán & Robins (2016) – Using big data to emulate a target trial when a randomized trial is not available.\nDefines target trial emulation, a cornerstone idea for translating causal inference principles to observational study design.\n\n\n\n\nICH E9 (R1) Addendum (2019) – Addendum on Estimands and Sensitivity Analyses in Clinical Trials.\nEstablishes the estimand framework to align clinical trial objectives, analyses, and interpretations.\nRufibach (2019) – Treatment effect quantification for time-to-event endpoints – Estimands, analysis strategies, and beyond.\nApplies the estimand framework to survival outcomes, clarifying how censoring and non-proportional hazards affect effect interpretation.\n\n\n\n\nvan der Laan, Polley & Hubbard (2007) – Super Learner.\nA foundational paper introducing ensemble learning via cross-validation for optimal prediction and causal estimation.\nPhillips et al. (2023) – Practical considerations for specifying a Super Learner.\nA practical tutorial on constructing and validating Super Learners, including library specification, cross-validation, and reproducibility.\n\n\n\n\nGruber & van der Laan (2010) – Targeted Maximum Likelihood Estimation: A Gentle Introduction.\nProvides a clear step-by-step introduction to TMLE, integrating machine learning and influence function theory for efficient, doubly robust estimation.\n\n\n\n\nPetersen (2014) – Applying a Causal Road Map in Settings with Time-dependent Confounding.\nDiscusses longitudinal causal inference and how to handle time-dependent confounding through g-methods and TMLE.\nStensrud et al. (2019) – Limitations of hazard ratios in clinical trials.\nExplains why hazard ratios can be misleading as causal measures and encourages absolute or survival-based contrasts.\nMartinussen (2022) – Causality and the Cox Regression Model.\nClarifies the mathematical and conceptual limitations of hazard ratios, advocating more interpretable causal estimands.\n\n\n\n\nChakraborty & Murphy (2014) – Dynamic Treatment Regimes.\nA comprehensive overview of adaptive treatment strategies and their estimation through sequential designs and reinforcement learning methods.\nKennedy (2019) – Nonparametric causal effects based on incremental propensity score interventions.\nIntroduces stochastic interventions that shift treatment probabilities incrementally, addressing positivity violations and improving policy relevance.\n\n\n\n\nLendle et al. (2017) – ltmle: An R package implementing targeted ML for longitudinal data.\nPresents practical TMLE tools for longitudinal data in R, connecting theory to applied estimation in complex settings with time-varying confounders.\n\n\n\n\nDang et al. (2023) and Hernán & Robins (2016) – conceptual grounding\n\nvan der Laan et al. (2007) and Gruber & van der Laan (2010) – estimation and implementation\n\nLendle et al. (2017) and Kennedy (2019) – advanced longitudinal and stochastic methods\n\n\n\n\n\n\n01 Causal Intro – Dang et al. (2023)\n\n03 TL Intro – Gruber et al. (2023)\n\nWilliamson et al. (2023)\n\nRufibach (2019)\n\nPracticalSL (Phillips et al., 2023)\n\nGruber & van der Laan (2010)\n\nPetersen (2014)\n\nStensrud (2019)\n\nMartinussen (2022)\n\nKennedy (2019)\n\nLendle et al. (2017)"
  },
  {
    "objectID": "bibliography/index.html#causal-inference-roadmap-and-target-trial-emulation",
    "href": "bibliography/index.html#causal-inference-roadmap-and-target-trial-emulation",
    "title": "Annotated Bibliography: Modern Causal Inference and Targeted Learning",
    "section": "",
    "text": "Dang et al. (2023) – A causal roadmap for generating high-quality real-world evidence.\nIntroduces the Causal Roadmap framework for structuring causal analyses in observational data. Emphasizes transparency, assumptions, and reproducibility in real-world evidence.\nGruber et al. (2023) – Evaluating and improving real-world evidence with Targeted Learning.\nApplies the roadmap to re-analyze published results using TMLE, highlighting the link between causal identification and robust estimation.\nWilliamson et al. (2023) – An application of the Causal Roadmap in two safety monitoring case studies.\nDemonstrates roadmap principles in practice for safety monitoring and outcome prediction using electronic health records.\nHernán & Robins (2016) – Using big data to emulate a target trial when a randomized trial is not available.\nDefines target trial emulation, a cornerstone idea for translating causal inference principles to observational study design."
  },
  {
    "objectID": "bibliography/index.html#estimand-specification-in-clinical-studies",
    "href": "bibliography/index.html#estimand-specification-in-clinical-studies",
    "title": "Annotated Bibliography: Modern Causal Inference and Targeted Learning",
    "section": "",
    "text": "ICH E9 (R1) Addendum (2019) – Addendum on Estimands and Sensitivity Analyses in Clinical Trials.\nEstablishes the estimand framework to align clinical trial objectives, analyses, and interpretations.\nRufibach (2019) – Treatment effect quantification for time-to-event endpoints – Estimands, analysis strategies, and beyond.\nApplies the estimand framework to survival outcomes, clarifying how censoring and non-proportional hazards affect effect interpretation."
  },
  {
    "objectID": "bibliography/index.html#super-learner-ensemble-learning",
    "href": "bibliography/index.html#super-learner-ensemble-learning",
    "title": "Annotated Bibliography: Modern Causal Inference and Targeted Learning",
    "section": "",
    "text": "van der Laan, Polley & Hubbard (2007) – Super Learner.\nA foundational paper introducing ensemble learning via cross-validation for optimal prediction and causal estimation.\nPhillips et al. (2023) – Practical considerations for specifying a Super Learner.\nA practical tutorial on constructing and validating Super Learners, including library specification, cross-validation, and reproducibility."
  },
  {
    "objectID": "bibliography/index.html#targeted-maximum-likelihood-estimation-tmle",
    "href": "bibliography/index.html#targeted-maximum-likelihood-estimation-tmle",
    "title": "Annotated Bibliography: Modern Causal Inference and Targeted Learning",
    "section": "",
    "text": "Gruber & van der Laan (2010) – Targeted Maximum Likelihood Estimation: A Gentle Introduction.\nProvides a clear step-by-step introduction to TMLE, integrating machine learning and influence function theory for efficient, doubly robust estimation."
  },
  {
    "objectID": "bibliography/index.html#time-dependent-confounding-and-intercurrent-events",
    "href": "bibliography/index.html#time-dependent-confounding-and-intercurrent-events",
    "title": "Annotated Bibliography: Modern Causal Inference and Targeted Learning",
    "section": "",
    "text": "Petersen (2014) – Applying a Causal Road Map in Settings with Time-dependent Confounding.\nDiscusses longitudinal causal inference and how to handle time-dependent confounding through g-methods and TMLE.\nStensrud et al. (2019) – Limitations of hazard ratios in clinical trials.\nExplains why hazard ratios can be misleading as causal measures and encourages absolute or survival-based contrasts.\nMartinussen (2022) – Causality and the Cox Regression Model.\nClarifies the mathematical and conceptual limitations of hazard ratios, advocating more interpretable causal estimands."
  },
  {
    "objectID": "bibliography/index.html#dynamic-treatment-regimes-and-stochastic-interventions",
    "href": "bibliography/index.html#dynamic-treatment-regimes-and-stochastic-interventions",
    "title": "Annotated Bibliography: Modern Causal Inference and Targeted Learning",
    "section": "",
    "text": "Chakraborty & Murphy (2014) – Dynamic Treatment Regimes.\nA comprehensive overview of adaptive treatment strategies and their estimation through sequential designs and reinforcement learning methods.\nKennedy (2019) – Nonparametric causal effects based on incremental propensity score interventions.\nIntroduces stochastic interventions that shift treatment probabilities incrementally, addressing positivity violations and improving policy relevance."
  },
  {
    "objectID": "bibliography/index.html#longitudinal-tmle-and-related-methods",
    "href": "bibliography/index.html#longitudinal-tmle-and-related-methods",
    "title": "Annotated Bibliography: Modern Causal Inference and Targeted Learning",
    "section": "",
    "text": "Lendle et al. (2017) – ltmle: An R package implementing targeted ML for longitudinal data.\nPresents practical TMLE tools for longitudinal data in R, connecting theory to applied estimation in complex settings with time-varying confounders.\n\n\n\n\nDang et al. (2023) and Hernán & Robins (2016) – conceptual grounding\n\nvan der Laan et al. (2007) and Gruber & van der Laan (2010) – estimation and implementation\n\nLendle et al. (2017) and Kennedy (2019) – advanced longitudinal and stochastic methods\n\n\n\n\n\n\n01 Causal Intro – Dang et al. (2023)\n\n03 TL Intro – Gruber et al. (2023)\n\nWilliamson et al. (2023)\n\nRufibach (2019)\n\nPracticalSL (Phillips et al., 2023)\n\nGruber & van der Laan (2010)\n\nPetersen (2014)\n\nStensrud (2019)\n\nMartinussen (2022)\n\nKennedy (2019)\n\nLendle et al. (2017)"
  },
  {
    "objectID": "workshop/SER_workshop.html",
    "href": "workshop/SER_workshop.html",
    "title": "The Causal Roadmap Tutorial",
    "section": "",
    "text": "Make a bit more narrative versus bullet pointed\nAdd more on the causal roadmap\nInclude a Worked Interpretation Section (Not Just the computation)\nMaybe add Q&A’s throughout for those taking it asynchronously?"
  },
  {
    "objectID": "workshop/SER_workshop.html#introduction",
    "href": "workshop/SER_workshop.html#introduction",
    "title": "The Causal Roadmap Tutorial",
    "section": "Introduction",
    "text": "Introduction\nThis tutorial provides a gentle introduction to the Causal Roadmap and its applications in pharmaco-epidemiologic research. It is designed for a broad audience, including learners from both academia and industry. We systematically walk through each step of the Causal Roadmap—from explicitly formulating a research question, to translating it into a formal causal estimand, to identifying and estimating that estimand from observed data, and finally to drawing valid inferences and interpreting results. Each step is illustrated using a working example from a pharmaco-epidemiology setting, accompanied by interactive, built-in code to facilitate hands-on learning. The structure and content of this tutorial follow, in an analytical way, the Introduction to Causal Inference and the causal Roadmap course (htp://www.ucbbiostat.com/) Petersen and Balzer."
  },
  {
    "objectID": "workshop/SER_workshop.html#why-venture-down-a-new-path",
    "href": "workshop/SER_workshop.html#why-venture-down-a-new-path",
    "title": "The Causal Roadmap Tutorial",
    "section": "Why venture down a new path?",
    "text": "Why venture down a new path?\nAdopting the Causal Roadmap in our approach to research in causal inference enables us to clearly state a scientific question and select an analtyic approach that matches the question being asked while ensuring systematic assessment of our ability/feasibility to answer this question from the data we observe (identifiability). Head to head analysis method comparison lets us select the best approach.\nWe will now formally introduce the Causal Roadmap but before let us go over some notation!"
  },
  {
    "objectID": "workshop/SER_workshop.html#notation",
    "href": "workshop/SER_workshop.html#notation",
    "title": "The Causal Roadmap Tutorial",
    "section": "Notation",
    "text": "Notation\n\nA: Exposure/Treatement\n\nThe term treatment is often used in causal inference even with exposures that are not medical treatments. We shall use A=1 for exposed (treated) and A=0 for unexposed (untreated)\n\nY: outcome\nW: set of measured confounding variables\nU: set of unmeasured factors\n\\(\\mathbb{E}[Y|A=a]\\): expected outcome Y among those who experience exposure A=a in our population. This is a descriptive measure\n\\(\\mathbb{E}[Y_{a}]\\): expected counterfactual outcome \\(Y_a\\) when all experience exposure A=a in our population. This is a causal quantity. Generally \\(\\mathbb{E}[Y|A=a]\\) does not equal to \\(\\mathbb{E}[Y_{a}]\\) and this is the fundamental problem of causal inference\n\\(\\mathbb{E}[Y|A=a,W=w]\\): expected outcome Y among those who expereince exposure A=a and have covariates W=w, in our population. For example this can be the mean outcome among exposed men. These conditional expectations are often estimated using multivariable regression models.\n\\(\\mathbb{E}[\\mathbb{E}[Y|A=a,W=w]]\\):expected outcome Y among those who experience exposure A=a and have covariates W=w,averaged across covariate strata in the population. This is a marginal expectation."
  },
  {
    "objectID": "workshop/SER_workshop.html#motivation",
    "href": "workshop/SER_workshop.html#motivation",
    "title": "The Causal Roadmap Tutorial",
    "section": "Motivation",
    "text": "Motivation\n\nSuppose we are interested in the impact of Drug A vs Drug B on risk of cardiovascular disease among postmenopausal women with osteoporosis.\nOur usual approach would be to collect data on the intervention, outcome (cardiovascular disease ) and some covariates. Since the outcome is binary, we would use a logistic regression to estimate the conditional odds ratio by exponentiating the regression coefficient on the intervention (treatment).\nThe problem with is approach is that it allows the tool i.e. logistic regression to define the question we answer rather than starting with the question and picking amongst tools that allow us to answer the question.\nTo address this problem, we introduce the Causal Roadmap!"
  },
  {
    "objectID": "workshop/SER_workshop.html#the-causal-roadmap",
    "href": "workshop/SER_workshop.html#the-causal-roadmap",
    "title": "The Causal Roadmap Tutorial",
    "section": "The Causal Roadmap",
    "text": "The Causal Roadmap\nThe Causal Roadmap is a framework that provides a systematic process to move from a research question to estimation and interpretation which guides investigators on how to design and analyse their studies a priori. This framework has the following steps;\n\nStating the research question and hypothetical experiment\nDefining the causal model and parameter of interest\nLinking the causal model to the observed data and defining the statistical model\nAssessing identifiability: linking the causal effect to a parameter estimable from the observed data\nSelecting and applying the estimator\nDeriving an estimate of the sampling distribution (statistical uncertainty)\nMaking inference (interpreting findings)\n\nWe shall now delve into each of these steps in details!"
  },
  {
    "objectID": "workshop/SER_workshop.html#step-0-state-the-question",
    "href": "workshop/SER_workshop.html#step-0-state-the-question",
    "title": "The Causal Roadmap Tutorial",
    "section": "Step 0: State the question",
    "text": "Step 0: State the question\n\nThis is the very first step of the roadmap. A helpful way to be clear about the scientific question is to explicitly state the experiment that would unambiguously yield estimates of the causal effect of interest.\nFor example: What is the effect of a certain medication on the incidence of cardiovascular disease among postmenopausal women who initiated Drug A vs Drug B in the United States?\nWe can consider a hypothetical experiment where we ask what would be the the difference in CVD incidence if patients received the intervention drug A vs if all patients received the control drug B (or standard of care).\nTo sharply frame our research question, we want to be more specific about;\n\nThe target population (What age group? where?)\nThe exposure (What dosage? Frequency?)\nThe outcome (over what timeframe?)\nWays to change the exposure and their plausibility\n\nOther interesting hypothetical experiments could include:\n\nWhat would be the difference in CVD incidence if patients were initiated on drug A once they reached a certain risk threshold vs if all patients are initiated on Drug A regardless of their risk profile?\nWhat would be the difference in CVD incidence if an additional 10% of patients received the intervention compared to if the intervention uptake remained as observed?\n\nWe note that there is massive flexibility in how we can define our desired hypothetical experiments."
  },
  {
    "objectID": "workshop/SER_workshop.html#step-1-define-the-causal-model",
    "href": "workshop/SER_workshop.html#step-1-define-the-causal-model",
    "title": "The Causal Roadmap Tutorial",
    "section": "Step 1: Define the causal model",
    "text": "Step 1: Define the causal model\n\nCausal modeling formalizes our knowledge however limited. We are able to explore which variables affect each other, examine the role of unmeasured factors and the functional form of the relationships between variables.\nIn this tutorial, we shall focus on structural causal models and corresponding causal graphs (Pearl 2000). However, we do note that their are many other causal frameworks.\nThe figure 1 below corresponds to a simple causal graph with corresponding structural casual model as follows;\n\n\\(W= f_w(U_w)\\)\n\\(A= f_A(W,U_A)\\)\n\\(Y = f_Y(W,A,U_Y)\\)\n\nWe make no assumptions on the background factors \\((U_w,U_A,U_Y)\\) or on the functional forms of functions \\((f_w,f_A,f_Y)\\)\n\n\n\n\n\n\n\n\n\n\n\nIf you believed no unmeasured confounding, a possible causal model and graph (figure 2) would be;\n\n\\(W= f_w(U_w)\\)\n\\(A= f_A(W,U_A)\\)\n\\(Y = f_Y(W,A,U_Y)\\)\n\nHere we assume that the background factors are all independent but still make no assumption on the functional forms of \\((f_w,f_A,f_Y)\\)\nHowever, it is important to note that wishing for something does not make it true."
  },
  {
    "objectID": "workshop/SER_workshop.html#step-2-define-the-causal-parameter-of-interest",
    "href": "workshop/SER_workshop.html#step-2-define-the-causal-parameter-of-interest",
    "title": "The Causal Roadmap Tutorial",
    "section": "Step 2: Define the causal parameter of interest",
    "text": "Step 2: Define the causal parameter of interest\n\nWe now define counterfactuals by intervening on the causal model. We can do this by setting the exposure to a specific level e.g A=1 for all units.\n\n\\(W= f_w(U_w)\\)\n\\(A= 1\\)\n\\(Y_1 = f_Y(W,1,U_Y)\\) where \\(Y_1\\) is the outcome if possibly-contrary to fact, the unit was exposed (A=1)\n\n\n\n\n\n\n\n\n\n\n\n\nAnalogously, we can intervene on the causal model by setting A=0\n\n\\(W= f_w(U_w)\\)\n\\(A= 0\\)\n\\(Y_0 = f_Y(W,0,U_Y)\\) where \\(Y_0\\) is the outcome if possibly-contrary to fact, the unit was exposed (A=0)\n\n\n\n\n\n\n\n\n\n\n\n\nWe use counterfactuals to define the causal parameter;\n\nFor example, the difference between the expected counterfactual outcomes under these two interventions i.e \\(\\mathbb{E}[Y_1]-\\mathbb{E}[Y_0]\\) which is known as the average treatment effect(ATE)\nFor a binary outcome, we define the causal risk difference (CRD) as \\(\\mathbb{P}(Y_1=1)-\\mathbb{P}(Y_0=1)\\).\n\nMany other causal parameters are possible!!"
  },
  {
    "objectID": "workshop/SER_workshop.html#step-3-link-to-observed-data",
    "href": "workshop/SER_workshop.html#step-3-link-to-observed-data",
    "title": "The Causal Roadmap Tutorial",
    "section": "Step 3: Link to observed data",
    "text": "Step 3: Link to observed data\n\nObserved data are denoted O=(W,A,Y) where W reprensents measured covariates, A is the exposure and Y is the outcome.\nWe assume that the causal model provides a description of our study under existing conditions(i.e. the real world) and under interventions (i.e.the counterfactual world)\nThis provides a link between the causal world and the real (observed) world and therefore our causal model implies our statistical model which is the set of possible distributions of observed data.\nThe causal model may but often does not place any restrictions on the statistical model in which case the statistical model is non parametric.\nFor example our model says that A is a function of W and \\(U_A\\) but does not specify the form of that function: A= \\(f_A(W,U_A)\\). However, if we know the form, that should be specified in the causal model."
  },
  {
    "objectID": "workshop/SER_workshop.html#step-4-assess-identifiablity",
    "href": "workshop/SER_workshop.html#step-4-assess-identifiablity",
    "title": "The Causal Roadmap Tutorial",
    "section": "Step 4: Assess Identifiablity",
    "text": "Step 4: Assess Identifiablity\n\nThis process involves linking the causal effect to the parameter estimable from observed data. This requires some assumptions as follows:\n\nTemporality: exposure precedes the outcome. This is indicated by an arrow on the causal graph from A to Y\nConsistency: \\(Y_a\\)=Y where A=a. If an individual received treatment A=a, then their observed outcome Y is equal to their potential outcome under that treatment \\(Y_a\\).\nStability: We require no interference between units. This is indicated by the fact that the outcomes Y are only a function of each unit’s exposure A in the causal model and graph.\nRandomization:No unmeasured confounding such that \\(Y_a \\perp A \\mid W\\)\nPositivity: We require sufficient variability in exposure within confounder values i.e. \\(0 &lt; \\mathbb{P}(A=1|W)&lt;1\\).\n\nWith these assumptions, we can express our causal target parameter which is a function of counterfactuals in terms of our observed data i.e \\[\n\\begin{aligned}\n\\mathbb{E}(Y_a)\n  &= \\mathbb{E}\\big[ \\mathbb{E}(Y_a \\mid W) \\big] \\\\\n  &= \\mathbb{E}\\big[ \\mathbb{E}(Y_a \\mid A=a, W) \\big]  under \\ randomization\\\\\n  &= \\mathbb{E}\\big[ \\mathbb{E}(Y \\mid A=a, W) \\big] under \\ consistency\n\\end{aligned}\n\\]\nAgain wishing for something does not make it true.\nUnder the above assumptions we can have the G-computation identifiability result (Robins 1986) as\n\n\\(\\mathbb{E}[Y_1-Y_0] = \\mathbb{E}\\big[\\mathbb{E}(Y\\mid A=1,W)-\\mathbb{E}(Y\\mid A=0,W)]\\) where the right handside is our parameter of interest a.k.a our statistical estimand.\nFor a binary outcome, we have the marginal risk difference as $. This is marginal because the outer expectation averages over the confounder distribution.\n\nWhat if the assumptions are not all met? For example one might be worried about unmeasured confounders or that the data structure does not assure temporality.Possible options include;\n\nGiving up!!\nChanging the research question, the exposure, the outcome or the target population\nProceeding to do the best job possible estimating the target parameter provided the question is still well-defined and interpretable and that we can still get as close as possible to the wished for causal parameter given the limitations in the data."
  },
  {
    "objectID": "workshop/SER_workshop.html#step-5-choose-and-apply-the-estimator",
    "href": "workshop/SER_workshop.html#step-5-choose-and-apply-the-estimator",
    "title": "The Causal Roadmap Tutorial",
    "section": "Step 5: Choose and apply the estimator",
    "text": "Step 5: Choose and apply the estimator\n\nAn estimator is an algorithm that when applied to the data generates an estimate of the parameter of interest.\nThere are several estimators available for the statistical parameter (which equals the ATE if the identifiability assumptions hold). Among these are:\n\nSubstitution estimators (e.g. paramteric G-computation)\nPropensity score based estimators (e.g. IPTW, matching)\nDouble robust estimators (e.g.TMLE, A-IPTW)\n\nBut before we dive into these estimators, let us pause to recall the usual approach. One would usually run a logistic regression of the outcome Y( risk of Cardiovasicular disease) on exposure (drug A or B) and baseline confounders W:\n\n\\[\n\\text{logit}\\big( \\mathbb{E}(Y \\mid A, W) \\big)\n    = \\beta_0 + \\beta_1 A + \\beta_2 W_1 + \\cdots + \\beta_{19} W_{18}.\n\\]\n\nThey would then exponentiate the coefficient on the exposure and interpret the association in terms of an odds ratio.\n\nConditional OR: “While holding other factors constant”\n\nThe problem here is that our target parameter (ATE) does not equal \\(e^{\\beta_{1}}\\). Rather we are letting the estimation approach drive the question. Additionally, this estimation approach relies on the main terms logisitic regression being correct.\nA parametric estimation approach assumes we know the relationship between covariates W, the exposure A and the outcome Y and have correctly specified this relation with a finite set of constants called “parameters”.\n\nFor example, we can specify a regression with main terms for covaritaes and a few interactions or squared terms that we think are reasonable.\nIf we had this knowledge, we should encode it in our causal model so we avoid introducing new assumptions during estimation.\nWith parametric regression models, we are likely assuming we know more than we actually know.\n\nA non-parametric estimation approach acknowledges that we do not know the form of the relations beetween the covariates W, the exposure A and the outcome Y.\n\nFor example, one could divide the data into all combinations of (A,W), calculate and average the stratum specific A and Y relations.\nUnfortunately we typically have too many covariates and/or continuous covariates which would result into empty cells. This is known as the curse of dimensionality as the number of strata increases exponentially with dimension of W!\n\nIn a semi parametric estimation approach, we often “know nothing” (i.e. have a non-paramteric statistical model) but also need to smooth over data with weak support during estimation.\n\nWe utilize “data-adaptive estimation” or “machine learning”.\nOne could choose an algorithm (e.g. stepwise regression, loess or polynomial splines), but we have no basis for choosing one over the other.\nInstead we allow a large class of algorthims to compete and we select the best algorithm with cross-validation. This is the basis of Super Learner which we will focus on in this tutorial.\n\nRecall our statistical parameter is \\(\\mathbb{E}\\big[\\mathbb{E}(Y\\mid A=1,W)-\\mathbb{E}(Y\\mid A=0,W)\\big]\\) which equals the ATE if the identifiability results hold.\nWe shall now discuss the following estimators with example implementation code in R.\n\nSimple substitution estimator a.k.a paramteric G-computation or parametric g-formula\nInverse probability of treatment weighting (IPTW)\nTargeted maximum likelihood estimation (TMLE) with Super Learner\n\n\n\nSimple substitution estimator\n\nTo get some intuition behind this estimator, let us think of causal inference as a problem of missing information where we know the outcome under the observed exposure but are missing the outcome under the other exposure condition.\nWe therefore use parametric regression to estimate outcomes for all under both exposed and unexposed conditions after controlling for the measured confounders.\nWe then average and compare predicted outcomes. The algorithm is as follows. First we shall load in our simulated dataset “CausalWorkshop.csv” and set the random seed.\n\n\n\nCode\nlibrary(readr)\ndata &lt;-  read_csv(\"CausalWorkshop.csv\")\n\n\nRows: 200 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): W1, W2, W3, W4, A, Y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(data)\n\n\n# A tibble: 6 × 6\n     W1    W2     W3     W4     A      Y\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1 0.704  0.312  0.441     0 0.0467\n2     0 0.696  0.438  1.38      0 0.0771\n3     0 0.394 -0.538 -1.44      0 0.177 \n4     0 0.642  0.767  1.25      0 0.0330\n5     1 0.426 -0.203 -0.146     0 0.105 \n6     1 0.458  1.60   0.709     0 0.0228\n\n\nCode\ntail(data)\n\n\n# A tibble: 6 × 6\n     W1     W2     W3      W4     A       Y\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1     0 0.856   1.17   2.10       1 0.00278\n2     0 0.434   0.545 -0.238      0 0.0469 \n3     1 0.243  -1.80  -0.769      0 0.0675 \n4     0 0.629  -0.229  0.0750     1 0.109  \n5     0 0.0913 -0.100  0.761      0 0.117  \n6     1 0.543  -1.48  -0.794      0 0.0531 \n\n\nCode\ndim(data)\n\n\n[1] 200   6\n\n\nCode\nset.seed(1)\n\n\n\nWe the estimate the mean outcome Y as a function of exposure (treatment) A and measured confounders W. In this example we run a main terms logistic regression.\n\n\n\nCode\noutcome.regression &lt;- glm(Y ~ A + W1+W2+W3+W4, family='binomial', data=data)\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\n\nWe use estimates from 1 above to predict outcomes for each unit while “setting” the exposure to different values e.g. A=1 and A=0\n\n\n\nCode\ndata.A1 &lt;- data.A0 &lt;- data\ndata.A1$A &lt;- 1\ndata.A0$A &lt;- 0\ncolMeans(data.A1)\n\n\n         W1          W2          W3          W4           A           Y \n 0.52000000  0.51459003  0.02187934 -0.01958053  1.00000000  0.06244803 \n\n\nCode\ncolMeans(data.A0)\n\n\n         W1          W2          W3          W4           A           Y \n 0.52000000  0.51459003  0.02187934 -0.01958053  0.00000000  0.06244803 \n\n\nCode\npredict.outcome.A1 &lt;- predict( outcome.regression, newdata=data.A1, \n                               type='response')\npredict.outcome.A0 &lt;- predict(outcome.regression, newdata=data.A0, \n                              type='response')\n\n\n\nAverage predictions to estimate the marginal risks in the population under exposure and no exposure. To compare estimates, take the difference in means.\n\n\n\nCode\nmean(predict.outcome.A1)\n\n\n[1] 0.03877997\n\n\nCode\nmean(predict.outcome.A0)\n\n\n[1] 0.07258282\n\n\nCode\nSimple.Subs &lt;- mean(predict.outcome.A1 - predict.outcome.A0)\nSimple.Subs\n\n\n[1] -0.03380285\n\n\n\n\nIPTW- Inverse Probability of Treatment Weighting estimator\n\nThe intuition behind this estimation approach is to think of confounding as a problem of biased sampling, where certain exposure–covariate subgroups are overrepresented relative to what we would observe in a randomized trial, while others are underrepresented.\nWe apply weights to up-weight under-represented units and down-weight over-represented units\nWe then average and compare weighted outcomes. The algorithm is as follows;\n\n\nEstimate the probability of being exposed/treated A as a function of measured confounders W:\\(\\mathbb{P}(A=1\\mid W)\\). This is often referred to as the propensity score. We can estimate the propensity score by running a main terms logistic regression as illustrated below\n\n\n\nCode\npscore.regression &lt;- glm(A~ W1+W2+W3+W4, family='binomial',\n                         data=data)\nsummary(pscore.regression)\n\n\n\nCall:\nglm(formula = A ~ W1 + W2 + W3 + W4, family = \"binomial\", data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.3445     0.3821  -3.519 0.000433 ***\nW1            0.4637     0.3130   1.482 0.138399    \nW2            0.5113     0.5629   0.908 0.363744    \nW3            0.4483     0.3208   1.397 0.162360    \nW4           -0.2676     0.3095  -0.865 0.387227    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 247.64  on 199  degrees of freedom\nResidual deviance: 241.99  on 195  degrees of freedom\nAIC: 251.99\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nWe then use estimates from 1 above to calculate exposed/treated weights: 1/\\(\\mathbb{P}(A=1\\mid W)\\) and unexposed/untreated weights:1/\\(\\mathbb{P}(A=0\\mid W)\\)\n\n\n\nCode\npredict.prob.A1 &lt;- predict(pscore.regression, type='response')\nsummary(predict.prob.A1)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1555  0.2521  0.3034  0.3100  0.3658  0.5145 \n\n\nCode\npredict.prob.A0 &lt;- 1 - predict.prob.A1\nwt1 &lt;- as.numeric( data$A==1)/predict.prob.A1\nwt0 &lt;- as.numeric( data$A==0)/predict.prob.A0\nhead(data.frame(cbind(A=data$A, 1/predict.prob.A1, wt1, wt0)))\n\n\n  A       V2 wt1      wt0\n1 0 2.646982   0 1.607171\n2 0 4.197342   0 1.312760\n3 0 3.718918   0 1.367793\n4 0 3.735871   0 1.365514\n5 0 3.044577   0 1.489099\n6 0 2.128806   0 1.885892\n\n\n\nWe then apply the weights and average the weighted outcomes to estimate the marginal risks in the population under A=1 and A=0. To compare estimates, we take the difference in weighted means.\n\n\n\nCode\nmean(wt1*data$Y)\n\n\n[1] 0.03973564\n\n\nCode\nmean(wt0*data$Y)\n\n\n[1] 0.07234258\n\n\nCode\nIPW &lt;- mean(wt1*data$Y) - mean(wt0*data$Y)\nIPW\n\n\n[1] -0.03260694\n\n\nCode\nmean( (wt1-wt0)*data$Y)\n\n\n[1] -0.03260694\n\n\n\n\nTMLE- targeted maximum likelihood estimation\n\nFor intuition here, we again think of causal inference as a problem of missing information.\nWe predict the outcomes for all units under both exposed and unexposed conditions. We use a flexible estimation approach e.g. Super Learner to avoid assuming regression models that are not known or we use parametric knowledge if known.\nWe incorporate information on the covariate-exposure relation to improve the initial estimator. Why TMLE?\n\nAgain here we use a flexible estimation approach or parameteric knowledge if available.\nWe have a second chance to control for confounding hence this method is doubly robust\nWe are able to hone our estimator towards the parameter of interest.\nThis estimator is asymptotically linear and therefore we can obtain normal curve inference\n\nFinally, we average and compare the targeted predictions under exposure and no exposure.\n\n\nWhat is Super Learner?\n\nThis is a supervised machine learning algorithm that offers a flexible and data daptive approach to learn complex relationships from data.\nThis algorithm uses cross-validation(sample splitting) to evaluate the performance of a library of candidate estimators.\n\nThe library should be diverse including simple (e.g expert informed parametric regressions) and more adaptive algorithms (e.g penalized regressions, stepwise regression, adaptive splines)\nPerformance is measured by a loss function e.g squared prediction error\n\nCross-validation allows us to compare algorithms based on how they perform on independent data. Here we partition data in “folds”, fit each algorithm on the training set and evaluate its performance (called “risk”) on the validation set. We rotate through the folds and average the cross-validated risk estimates across the folds to obtain one measure of performance for each algorithm.\nWe could choose the algorithm with the best performance (e.g the lowest cross-validated MSE)\nInstead, Super Learner builds the best combination of algorithm- specific predictions. We now illustrate below how to fit a Super Learner.\n\n\n\nCode\nlibrary('SuperLearner')\nSL.library &lt;- c('SL.glm', 'SL.step.interaction', 'SL.gam'\n)\nSL.outcome.regression &lt;- suppressWarnings(SuperLearner(Y=data$Y, \n                                      X=subset(data, select=-Y),\n                                      SL.library=SL.library, \n                                      family='binomial'))\nSL.outcome.regression\n\n\n\nCall:  \nSuperLearner(Y = data$Y, X = subset(data, select = -Y), family = \"binomial\",  \n    SL.library = SL.library) \n\n                               Risk      Coef\nSL.glm_All              0.002745489 0.0000000\nSL.step.interaction_All 0.001407506 0.1134315\nSL.gam_All              0.001164333 0.8865685\n\n\nCode\nSL.predict.outcome &lt;- predict(SL.outcome.regression, \n                              newdata=subset(data, select=-Y))$pred\nhead(SL.predict.outcome)\n\n\n           [,1]\n[1,] 0.06057923\n[2,] 0.03497205\n[3,] 0.09365561\n[4,] 0.02889946\n[5,] 0.08963042\n[6,] 0.01211903\n\n\n\n\nWhy do I need to target?\n\nWe could use Super Learner to predict the outcomes for each unit while “setting” the exposure to different levels and then average and contrast the predictions.\n\n\n\nCode\nSL.predict.outcome.A1 &lt;- predict(SL.outcome.regression, \n                                 newdata=subset(data.A1, select=-Y))$pred\nhead(SL.predict.outcome.A1)\n\n\n           [,1]\n[1,] 0.03789361\n[2,] 0.02165598\n[3,] 0.05940978\n[4,] 0.01781923\n[5,] 0.05674338\n[6,] 0.00741711\n\n\nCode\nSL.predict.outcome.A0 &lt;- predict(SL.outcome.regression, newdata=subset(data.A0, select=-Y))$pred\n\n# simple subst estimator\nmean(SL.predict.outcome.A1) - mean(SL.predict.outcome.A0)\n\n\n[1] -0.02523562\n\n\n\nBut Super Learner is focused on \\(\\mathbb{E}(Y\\mid A,W)\\) and not our parameter of interest. It makes the wrong bias-variance trade-off and specifically incurs too much bias.\nThere is also no reliable way to obtain statistical inference (i.e create 95% confidence intervals)\n\n\n\nWhat is targeting?\n\nTargeting involves using information in the estimated propensity score \\(\\mathbb{P}(A=1\\mid W)\\) to update the initial (Super Learner) estimator of \\(\\mathbb{E}(Y\\mid A,W)\\).\nIt involves running a univariate regression of the outcome Y on a clever covariate with offset the initial estimator. Why “clever”? It ensures that the targeting step moves the initial estimator in a direction that removes bias.\nWe then use the estimated coefficient to update our initial predictions of the outcome under the exposure and no exposure.\n\n\n\nHow do i target? (One approach)\n\nUse Super Learner to estimate the propensity score \\(\\mathbb{P}(A=1\\mid W)\\)\n\n\n\nCode\nSL.pscore &lt;- SuperLearner(Y=data$A, X=subset(data, select=-c(A,Y)),\n                          SL.library=SL.library, family='binomial')\nSL.pscore\n\n\n\nCall:  \nSuperLearner(Y = data$A, X = subset(data, select = -c(A, Y)), family = \"binomial\",  \n    SL.library = SL.library) \n\n                             Risk      Coef\nSL.glm_All              0.2193296 0.0000000\nSL.step.interaction_All 0.2073331 0.2987384\nSL.gam_All              0.2048886 0.7012616\n\n\nCode\nSL.predict.prob.A1 &lt;- SL.pscore$SL.predict\nsummary(SL.predict.prob.A1 - predict(SL.pscore, newdata=subset(data,select=-c(A,Y)))$pred)\n\n\n       V1   \n Min.   :0  \n 1st Qu.:0  \n Median :0  \n Mean   :0  \n 3rd Qu.:0  \n Max.   :0  \n\n\nCode\nsummary(SL.predict.prob.A1)\n\n\n       V1        \n Min.   :0.1560  \n 1st Qu.:0.2378  \n Median :0.2748  \n Mean   :0.3100  \n 3rd Qu.:0.3501  \n Max.   :0.9270  \n\n\nCode\nSL.predict.prob.A0 &lt;- 1 - SL.predict.prob.A1\n\n\nNote: As you run this code you might encounter a warning “non-integer #successes in a binomial glm!”. This simply means that our outcome Y is not binary much as it’s bounded between 0 and 1. This is okay and can be ignored.\n\nCalculate the “clever covariate”\n\n\\[H(A,W)= \\frac{\\mathbb{I}(A=1)}{\\mathbb{P}(A=1\\mid W)}- \\frac{\\mathbb{I}(A=0)}{\\mathbb{P}(A=0\\mid W)}\\]\nHere’s code to evaluate the “clever covariate”\n\n\nCode\nH.AW &lt;- (data$A==1)/SL.predict.prob.A1 - (data$A==0)/SL.predict.prob.A0\nsummary(H.AW)\n\n\n       V1           \n Min.   :-2.187049  \n 1st Qu.:-1.407167  \n Median :-1.301617  \n Mean   : 0.006881  \n 3rd Qu.: 1.988445  \n Max.   : 5.879351  \n\n\nCode\nH.1W &lt;- 1/SL.predict.prob.A1\nH.0W &lt;- -1/SL.predict.prob.A0\ntail(data.frame(A=data$A, H.AW, H.1W, H.0W))\n\n\n    A      H.AW     H.1W      H.0W\n195 1  2.475845 2.475845 -1.677578\n196 0 -1.394130 3.537231 -1.394130\n197 0 -1.326580 4.062038 -1.326580\n198 1  5.371727 5.371727 -1.228743\n199 0 -1.184828 6.410444 -1.184828\n200 0 -1.339595 3.944684 -1.339595\n\n\n3.Run logistic regression of the outcome on this covariate using logit of the initial estimator \\(\\mathbb{E}(Y\\mid A,W)\\) as offset where logit(x)= log[x/(1-x)]\n\n\nCode\nlogitUpdate &lt;- glm( data$Y ~ -1 +offset( qlogis(SL.predict.outcome)) +\n                      H.AW, family='binomial')\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nCode\nepsilon &lt;- logitUpdate$coef\nepsilon\n\n\n       H.AW \n0.004130973 \n\n\n\nPlug in the estimated coefficient \\(\\epsilon\\) to yield our targeted estimator \\(\\mathbb{E^*}(Y\\mid A,W)\\) and use the targeted estimator \\(\\mathbb{E^*}(Y\\mid A,W)\\) to predict outcomes for all under A=1 and A=0\n\n\n\nCode\ntarget.predict.outcome.A1 &lt;- plogis( qlogis(SL.predict.outcome.A1)+\n                                       epsilon*H.1W)\ntarget.predict.outcome.A0 &lt;- plogis( qlogis(SL.predict.outcome.A0)+\n                                      epsilon*H.0W)\n\n\n\nAverage the predictions to estimate the marginal risks in the population under exposure and no exposure. Compare the estimates by taking the difference.\n\n\n\nCode\nTMLE &lt;- mean( target.predict.outcome.A1 - target.predict.outcome.A0)\nTMLE\n\n\n[1] -0.02419015\n\n\n\n\n\nEstimator Properties\n\nWe have discussed three estimators and gone through their implementation. We shall now go over the properties and each of them and points of consideration.\nSimple substitution estimator\n\nRelies on consistently estimating the mean outcome \\(\\mathbb{E^*}(Y\\mid A,W)\\). Sometimes we have a lot of knowledge about how the relationship between the outcome Y and the exposure-covariates (A,W) but other times, our knowldege is limited and assuming a parametric regression model can result in bias and misleading inferences.\n\nIPTW\n\nRelies on consistently estimating the propensity score \\(\\mathbb{P}(A=1\\mid W)\\). While sometimes we have a lot of knowledge about how the exposure was assigned, other times our knowledge is limited and assuming a parametric regression model can result in bias and misleading inference.\nThis estimator is unstable under positivity violations. When covariate groups only have a few exposed or unexposed observations, weights can blow up!!. When there are covariate groups with 0 exposed or unexposed observations, weights will not blow up but the estimator will likely be biased and varaince will be underestimated.\n\nTMLE\n\nThis estimator is doubly robust i.e. yields a consistent estimate if either the conditional mean \\(\\mathbb{E^*}(Y\\mid A,W)\\) or the propensity score \\(\\mathbb{P}(A=1\\mid W)\\) is consistently estimated. We get two chances to get it right !!\nIt is also semi-parametric efficient which means it achieves the lowest asymptotic variance (most precision) among a large class of estimators if both the conditional mean and propensity score are consistently estimated at reasonable rates.\nThis estimator has formal theory to support valid statistical inference under mild conditions even when using machine learning.\nBeing a substitution estimator (plug-in), it is robust under positivity violations,strong confounding and rare outcomes.\nThere is readily available software to implement this estimator e.g. ltmle package, lmptp package in R among others\n\nWe have now come to the end of our estimation exercise assuming we have selected an estimating approach and estimated our parameter of interest. We now move back to the general Roadmap."
  },
  {
    "objectID": "workshop/SER_workshop.html#step-6-statistical-uncertainty",
    "href": "workshop/SER_workshop.html#step-6-statistical-uncertainty",
    "title": "The Causal Roadmap Tutorial",
    "section": "Step 6: Statistical Uncertainty",
    "text": "Step 6: Statistical Uncertainty\n\nTo do statistical inference, we need to derive an estimate of the sampling distribution.\nWe can consider doing a non-parametric bootstrap where we re-sample the observed data with replacement, apply the entire estimation process (including machine learning algorithms) to the re-sampled data, repeat X times and estimate the variance with the bootstrapped point estimates.\nAlternatively, we can use influence curve based inference. We shall not discuss this here but this form of inference is available in the R packages."
  },
  {
    "objectID": "workshop/SER_workshop.html#step-7-interpret-findings",
    "href": "workshop/SER_workshop.html#step-7-interpret-findings",
    "title": "The Causal Roadmap Tutorial",
    "section": "Step 7: Interpret findings",
    "text": "Step 7: Interpret findings\n\nThe final step of the Causal Roadmap is to interpret the findings. At this stage, we evaluate whether and to what extent the underlying assumptions have been met in order to determine the strength of interpretation.\nFindings support a statistical interpretation if (1) the statistical estimator has negligible bias and its variance is well estimated\nFindings support a causal interpretation if 1 holds and (2) if the non testable identifiability assumptions hold.\nCan be interpreted as if implemented in the real-world if 1 and 2 hold and if (3) the intervention is feasible and applicable to the real world population.\nFindings can be interpreted as if we had emulated a randomized trial if 1-3 hold and the exposure could have been randomized to that population.\nIf there are concerns about causal assumptions (e.g. temporal odering is unclear, unmeasured confounding), the results can be interpreted as associational. In this case the estimand, \\(\\mathbb{E}\\big[\\mathbb{E}(Y\\mid A=1,W)-\\mathbb{E}(Y\\mid A=0,W)]\\) can be interpreted as;\n\nThe marginal difference in the expected outcome associated with the exposure, after accounting for the measured confounders\nThe difference in the mean outcome between persons exposed versus unexposed but with the same values of the adjustment covariates (averaged with respect to the distribution of those covariates in the population).e.g The difference in the risk of cardiovascular disease with intervention A vs B is X, accounting for region,age,sex,SES etc\nAlternatively one can report that this is as close as we can get to the causal effet of A on Y given the limitations of the data detailing all limitations and including a causal graph to empower the reader to assess the plausibility of assumptions.\n\nIf the authors believe causal assumptions are met, the parameter can be interpreted as the population average treatment effect \\(\\mathbb{E}\\big[Y_1-Y_0\\big]\\).\n\nIn words, this would be the difference in the expected outcome if everyone were exposed compared if everyone were unexposed. For example, there would be an X difference in the risk of cardiovascular disease if all patients in the population received intervention A vs B."
  },
  {
    "objectID": "workshop/SER_workshop.html#summary-and-discussion",
    "href": "workshop/SER_workshop.html#summary-and-discussion",
    "title": "The Causal Roadmap Tutorial",
    "section": "Summary and Discussion",
    "text": "Summary and Discussion\n\nCongratulations!! You have successfully gone through the causal roadmap tutorial and successfully implemented the simple substitution, IPTW and TMLE estimators.\nHopefully, you have increased your intuitive and technical understanding of these estimators."
  },
  {
    "objectID": "workshop/SER_workshop.html#caution-use-your-tools-well.",
    "href": "workshop/SER_workshop.html#caution-use-your-tools-well.",
    "title": "The Causal Roadmap Tutorial",
    "section": "Caution: Use your tools well.",
    "text": "Caution: Use your tools well.\n\nUse TMLE with Super Learner as part of a toolbox. Recall, fancy estimation tools cannot replace careful thinking throughout the rest of the Roadmap.\nRemember to formally derive adjustment sets and the statistical parameter.\n\nAvoid errors of “causal model neglect”, occuring when estimating something differing meaningfully from any interpretable causal effect.\n\nDoubly robust estimators (e.g TMLE or A-IPW) can incorporate machine learning while maintaining basis for valid statistical inference. This helps us avoid errors of “statistical model neglect”, occurring when relying on unsubstantiated (parametric) assumptions during estimation. However, not without conditions.\n\nSpecify the Super Learner library with care.\nDiversity is key\nAvoid overfitting by using sample splitting\n\nPractical positivity violations can happen. This can result from poor support for exposures of interest and can lead to bias and/or underestimates of variance. Some solutions to this include;\n\nUsing a substitution estimator (G-comp,TMLE)\nDoing targeting in TMLE through weighted regression instead of a clever covariate\nUsing a robust variance estimator e.g Tran et al.(2018), Benkeser et al. (2017)\nBounding the estimated propensity score away from O\n\nRun a simulation study mimicking key patterns of the observed data for example sample size, confounding structure, missing data mechanisms, practical violations, sparsity of the exposure and/or outcome, dependence structure etc and use results to guide analyses.\nYou have also survived a high speed tour through the Roadmap, and hopefully can appreciate some of its strengths.\n\nIt necessitates clearly defined research questions, and ensures the parameters estimated will match the questions posed.\nElaborates what assumptions are necessary to interpret estimates as a causal effect\nWhen assumptions are not met, the unmet assumptions provide clear guidance on how future research must be improved to increase the potential of causal interpretation.\nWorking in this framework can improve interpretability and relevance of epidemiologic research.\nDespite focusing on the ATE, this framework is applicable to other causal questions and data structures such as estimating effects among treated/untreated, mediation, longitudinal interventions, dynamic regimes etc.\n\nIf you are interested in learning about more advanced settings, here are some links to other resources."
  },
  {
    "objectID": "workshop/SER_workshop.html#to-add",
    "href": "workshop/SER_workshop.html#to-add",
    "title": "The Causal Roadmap Tutorial",
    "section": "",
    "text": "Make a bit more narrative versus bullet pointed\nAdd more on the causal roadmap\nInclude a Worked Interpretation Section (Not Just the computation)\nMaybe add Q&A’s throughout for those taking it asynchronously?"
  },
  {
    "objectID": "advanced/01-01-regression-vs-causal.html",
    "href": "advanced/01-01-regression-vs-causal.html",
    "title": "Chapter 1.1: Limitations of Standard Regression Analyses",
    "section": "",
    "text": "In this chapter, we motivate the need for modern causal inference tools by walking through real-world examples where traditional regression fails. We also introduce the counterfactual (potential outcomes) framework as the foundation for defining causal effects."
  },
  {
    "objectID": "advanced/01-01-regression-vs-causal.html#why-not-just-use-regression",
    "href": "advanced/01-01-regression-vs-causal.html#why-not-just-use-regression",
    "title": "Chapter 1.1: Limitations of Standard Regression Analyses",
    "section": "Why Not Just Use Regression?",
    "text": "Why Not Just Use Regression?\nMultivariable regression has long been the standard tool in observational research for “adjusting” for confounders. But regression coefficients don’t always represent causal effects.\nLet’s consider a motivating example based on a real-world study comparing two osteoporosis treatments: denosumab (Prolia) and zoledronic acid.\n\nMotivating Example: Comparing Osteoporosis Treatments\nImagine we’re interested in whether patients who initiate denosumab have higher 3-year risk of heart attack or stroke compared to those initiating zoledronic acid.\nWe might be tempted to run the following:\n\n\nCode\n# simulate crude comparison (not real data)\nset.seed(123)\nn &lt;- 2000\ntreatment &lt;- rbinom(n, 1, 0.5) # 1 = denosumab, 0 = zoledronic acid\nage &lt;- rnorm(n, 75, 6)\ncvd_history &lt;- rbinom(n, 1, plogis(0.1 * (age - 70)))\n\n# outcome depends on age and history\nrisk &lt;- plogis(-2 + 0.4 * treatment + 0.05 * (age - 70) + 1 * cvd_history)\noutcome &lt;- rbinom(n, 1, risk)\n\nmodel1 &lt;- glm(outcome ~ treatment, family = binomial)\nsummary(model1)\n\n\n\nCall:\nglm(formula = outcome ~ treatment, family = binomial)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.05939    0.07197 -14.720  &lt; 2e-16 ***\ntreatment    0.33425    0.09887   3.381 0.000723 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2414.0  on 1999  degrees of freedom\nResidual deviance: 2402.5  on 1998  degrees of freedom\nAIC: 2406.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nThis gives us a crude estimate of association, which is confounded by differences in age and comorbidities.\n\n\nAdjusting for Confounders\nWe add adjustment:\n\n\nCode\nmodel2 &lt;- glm(outcome ~ treatment + age + cvd_history, family = binomial)\nsummary(model2)\n\n\n\nCall:\nglm(formula = outcome ~ treatment + age + cvd_history, family = binomial)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.398207   0.694981  -9.206  &lt; 2e-16 ***\ntreatment    0.366412   0.103506   3.540    4e-04 ***\nage          0.060479   0.009213   6.564 5.22e-11 ***\ncvd_history  1.090838   0.121918   8.947  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2414.0  on 1999  degrees of freedom\nResidual deviance: 2223.1  on 1996  degrees of freedom\nAIC: 2231.1\n\nNumber of Fisher Scoring iterations: 4\n\n\nBut is this causal? Not necessarily. We’re making strong parametric assumptions (e.g., log-linear effect of age), and regression implicitly defines the estimand. The treatment coefficient reflects a log-odds ratio comparing denosumab vs ZA, conditional on covariates. But this is not the average treatment effect in the population."
  },
  {
    "objectID": "advanced/01-01-regression-vs-causal.html#enter-the-counterfactual-framework",
    "href": "advanced/01-01-regression-vs-causal.html#enter-the-counterfactual-framework",
    "title": "Chapter 1.1: Limitations of Standard Regression Analyses",
    "section": "Enter the Counterfactual Framework",
    "text": "Enter the Counterfactual Framework\nTo define a causal effect, we introduce counterfactual outcomes:\n\nY(1): outcome if a person received denosumab\nY(0): outcome if a person received zoledronic acid\n\nThe average treatment effect (ATE) is:\n\\[\nE[Y(1) - Y(0)]\n\\]\nBut in reality, each person only receives one treatment. The counterfactual under the other treatment is missing.\nUnder assumptions (exchangeability, consistency, positivity), we can identify the causal effect from observed data. For example, we can estimate:\n\\[\nE_W[ E[Y | A = 1, W] - E[Y | A = 0, W] ]\n\\]\nThis motivates standardization, IPTW, and targeted learning."
  },
  {
    "objectID": "advanced/01-01-regression-vs-causal.html#when-regression-fails",
    "href": "advanced/01-01-regression-vs-causal.html#when-regression-fails",
    "title": "Chapter 1.1: Limitations of Standard Regression Analyses",
    "section": "When Regression Fails",
    "text": "When Regression Fails\nEven if we include the right confounders, misspecification of functional form (e.g., assuming linearity, ignoring interactions) can bias results.\nLet’s compare regression to a nonparametric substitution estimator (g-computation):\n\n\nCode\n# Fit model\nmodel3 &lt;- glm(outcome ~ treatment + age + cvd_history, family = binomial)\n\n# Predict counterfactual outcomes\nnewdata1 &lt;- data.frame(treatment = 1, age = age, cvd_history = cvd_history)\np1 &lt;- predict(model3, newdata = newdata1, type = \"response\")\n\nnewdata0 &lt;- data.frame(treatment = 0, age = age, cvd_history = cvd_history)\np0 &lt;- predict(model3, newdata = newdata0, type = \"response\")\n\n# Estimate marginal risk difference\nmean(p1 - p0)  # this is g-computation\n\n\n[1] 0.06897059\n\n\nCompare this to the regression coefficient:\n\n\nCode\ncoef(model3)[\"treatment\"]\n\n\ntreatment \n0.3664119 \n\n\nThe coefficient gives you a conditional odds ratio, but the g-comp version gives you a marginal risk difference — a more interpretable, population-level quantity."
  },
  {
    "objectID": "advanced/01-01-regression-vs-causal.html#summary",
    "href": "advanced/01-01-regression-vs-causal.html#summary",
    "title": "Chapter 1.1: Limitations of Standard Regression Analyses",
    "section": "Summary",
    "text": "Summary\n\nRegression is not inherently causal — it estimates conditional associations under model assumptions.\nCausal inference starts by defining a causal question and target estimand.\nThe counterfactual framework clarifies what we want to estimate.\nG-computation, IPTW, and TMLE allow estimating causal effects without relying on regression coefficients."
  },
  {
    "objectID": "advanced/01-01-regression-vs-causal.html#next",
    "href": "advanced/01-01-regression-vs-causal.html#next",
    "title": "Chapter 1.1: Limitations of Standard Regression Analyses",
    "section": "Next",
    "text": "Next\nIn the next chapter, we’ll introduce the Causal Roadmap — a structured workflow for planning and executing causal analyses."
  },
  {
    "objectID": "advanced/02-01-gcomputation.html",
    "href": "advanced/02-01-gcomputation.html",
    "title": "Chapter 2.1: Outcome Modeling and Standardization (G-Computation)",
    "section": "",
    "text": "G-computation as a foundation for causal estimation\nOutcome modeling and standardization—often referred to as g-computation—is one of the oldest and most intuitive approaches to causal inference. In this chapter, we’ll build intuition, walk carefully through why the method works, show where it fails, and provide fully reproducible examples in R (using tidyverse style).\nThis chapter is intentionally thorough, designed for students new to causal inference but with working knowledge of regression."
  },
  {
    "objectID": "advanced/02-01-gcomputation.html#step-1-fit-an-outcome-model",
    "href": "advanced/02-01-gcomputation.html#step-1-fit-an-outcome-model",
    "title": "Chapter 2.1: Outcome Modeling and Standardization (G-Computation)",
    "section": "Step 1: Fit an Outcome Model",
    "text": "Step 1: Fit an Outcome Model\nWe fit a logistic regression model predicting the outcome from treatment and confounders:\n\n\nCode\nmod &lt;- glm(Y ~ A + age + cvd, family = binomial, data = dat)\nsummary(mod)\n\n\n\nCall:\nglm(formula = Y ~ A + age + cvd, family = binomial, data = dat)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -9.445955   0.594115 -15.899  &lt; 2e-16 ***\nA            0.498637   0.088087   5.661 1.51e-08 ***\nage          0.107847   0.007951  13.563  &lt; 2e-16 ***\ncvd          1.271786   0.094031  13.525  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4127.2  on 2999  degrees of freedom\nResidual deviance: 3422.0  on 2996  degrees of freedom\nAIC: 3430\n\nNumber of Fisher Scoring iterations: 4\n\n\nThis alone is not a causal effect. Instead, it’s a conditional log-odds ratio.\nWe will now standardize using predictions."
  },
  {
    "objectID": "advanced/02-01-gcomputation.html#step-2-predict-counterfactual-outcomes",
    "href": "advanced/02-01-gcomputation.html#step-2-predict-counterfactual-outcomes",
    "title": "Chapter 2.1: Outcome Modeling and Standardization (G-Computation)",
    "section": "Step 2: Predict Counterfactual Outcomes",
    "text": "Step 2: Predict Counterfactual Outcomes\n\n\nCode\n# create counterfactual datasets\ndat1 &lt;- dat %&gt;% mutate(A = 1)\ndat0 &lt;- dat %&gt;% mutate(A = 0)\n\n# predict potential outcomes\np1 &lt;- predict(mod, newdata = dat1, type = \"response\")\np0 &lt;- predict(mod, newdata = dat0, type = \"response\")\n\n\nHere: - p1[i] = predicted outcome for person i if treated\n- p0[i] = predicted outcome for person i if untreated"
  },
  {
    "objectID": "advanced/02-01-gcomputation.html#step-3-standardize-average-over-covariates",
    "href": "advanced/02-01-gcomputation.html#step-3-standardize-average-over-covariates",
    "title": "Chapter 2.1: Outcome Modeling and Standardization (G-Computation)",
    "section": "Step 3: Standardize (Average Over Covariates)",
    "text": "Step 3: Standardize (Average Over Covariates)\n\n\nCode\nrisk1 &lt;- mean(p1)\nrisk0 &lt;- mean(p0)\nate  &lt;- risk1 - risk0\n\nlist(risk1 = risk1, risk0 = risk0, ate = ate)\n\n\n$risk1\n[1] 0.4890903\n\n$risk0\n[1] 0.3890304\n\n$ate\n[1] 0.1000599\n\n\nThis gives: - Risk under treatment - Risk under control - Risk difference (ATE)\nInterpretation example:\n\n“Initiating denosumab rather than ZA is estimated to increase/decrease 3-year MI/stroke risk by X percentage points.”"
  },
  {
    "objectID": "advanced/02-01-gcomputation.html#model-misspecification",
    "href": "advanced/02-01-gcomputation.html#model-misspecification",
    "title": "Chapter 2.1: Outcome Modeling and Standardization (G-Computation)",
    "section": "6.1 Model Misspecification",
    "text": "6.1 Model Misspecification\nIf your model for (E[Y | A, W]) is wrong (e.g., omits interactions, assumes linearity), g-computation may be biased.\nCheck residuals, fit alternative models, or use machine learning (next chapter)."
  },
  {
    "objectID": "advanced/02-01-gcomputation.html#poor-positivity",
    "href": "advanced/02-01-gcomputation.html#poor-positivity",
    "title": "Chapter 2.1: Outcome Modeling and Standardization (G-Computation)",
    "section": "6.2 Poor Positivity",
    "text": "6.2 Poor Positivity\nIf some strata almost never receive a treatment:\n\n\nCode\nps &lt;- predict(glm(A ~ age + cvd, family = binomial, data = dat), type = \"response\")\nsummary(ps)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1569  0.3715  0.6402  0.5647  0.7218  0.9025 \n\n\nLook for: - Scores near 0 or 1 → dangerous for extrapolation\n- G-computation may have to predict outcomes in unobserved regions"
  },
  {
    "objectID": "advanced/02-01-gcomputation.html#unmeasured-confounding",
    "href": "advanced/02-01-gcomputation.html#unmeasured-confounding",
    "title": "Chapter 2.1: Outcome Modeling and Standardization (G-Computation)",
    "section": "6.3 Unmeasured Confounding",
    "text": "6.3 Unmeasured Confounding\nNo modeling strategy fixes missing confounders.\nBut g-computation makes assumptions very clear, which is an advantage for interpretation."
  },
  {
    "objectID": "advanced/01-02-causal-roadmap.html",
    "href": "advanced/01-02-causal-roadmap.html",
    "title": "Chapter 1.2: The Causal Roadmap",
    "section": "",
    "text": "In this chapter, we introduce the Causal Roadmap, a structured approach for asking and answering causal questions with observational data. The roadmap helps researchers define their scientific question, translate it into a formal causal model, assess whether the causal effect is identifiable from data, and choose appropriate estimators to answer that question."
  },
  {
    "objectID": "advanced/01-02-causal-roadmap.html#why-use-a-causal-roadmap",
    "href": "advanced/01-02-causal-roadmap.html#why-use-a-causal-roadmap",
    "title": "Chapter 1.2: The Causal Roadmap",
    "section": "Why Use a Causal Roadmap?",
    "text": "Why Use a Causal Roadmap?\nTraditional analysis often starts with: “Let’s run a regression and see what we find.” But modern causal inference begins by asking: What is the causal question we are trying to answer?\nA causal roadmap ensures: - Transparency in how evidence is generated - Separation of scientific questions from statistical methods - Clear articulation of assumptions and their implications - Structured thinking in both trial and non-randomized settings\nWe’ll walk step-by-step through each part of the roadmap, with illustrative examples."
  },
  {
    "objectID": "advanced/01-02-causal-roadmap.html#step-1-define-the-causal-question",
    "href": "advanced/01-02-causal-roadmap.html#step-1-define-the-causal-question",
    "title": "Chapter 1.2: The Causal Roadmap",
    "section": "Step 1: Define the Causal Question",
    "text": "Step 1: Define the Causal Question\nThis includes: - Population: Who are we studying? - Intervention: What exposure or treatment are we manipulating? - Outcome: What effect are we interested in? - Timeframe: Over what duration?\nExample:\n\nWhat is the 3-year risk of cardiovascular events if all postmenopausal women with osteoporosis initiated denosumab compared to if all initiated zoledronic acid?\n\nThis PICO-style framing is the foundation of causal inference."
  },
  {
    "objectID": "advanced/01-02-causal-roadmap.html#step-2-specify-the-causal-model",
    "href": "advanced/01-02-causal-roadmap.html#step-2-specify-the-causal-model",
    "title": "Chapter 1.2: The Causal Roadmap",
    "section": "Step 2: Specify the Causal Model",
    "text": "Step 2: Specify the Causal Model\nWe now draw out how we believe the data were generated. This includes:\n\nVariables: Treatment (A), outcome (Y), and covariates (W)\nDAGs: Directed acyclic graphs to encode assumptions\nPotential Outcomes: \\(Y(1)\\) and \\(Y(0)\\) for each individual\n\nWe are making an implicit Structural Causal Model (SCM):\n\\[\nW = f_W(U_W),\\\nA = f_A(W, U_A),\\\nY = f_Y(W, A, U_Y)\n\\]\nWhere \\(U\\) terms represent unmeasured variables.\nA simple DAG:\nW → A → Y\n \\    ↘\n  →────→\nThis DAG implies: - W (confounders) affect both A and Y - A (treatment) affects Y - No unmeasured confounding (U nodes omitted for simplicity)"
  },
  {
    "objectID": "advanced/01-02-causal-roadmap.html#step-3-define-the-target-causal-estimand",
    "href": "advanced/01-02-causal-roadmap.html#step-3-define-the-target-causal-estimand",
    "title": "Chapter 1.2: The Causal Roadmap",
    "section": "Step 3: Define the Target Causal Estimand",
    "text": "Step 3: Define the Target Causal Estimand\nWe often want the Average Treatment Effect (ATE):\n\\[\nATE = E[Y(1) - Y(0)]\n\\]\nOr equivalently, Risk Difference:\n\\[\nE[Y | do(A=1)] - E[Y | do(A=0)]\n\\]\nWe might also want: - Risk ratios - Hazard ratios (with caveats) - Median survival differences - Effects in subgroups (CATEs)"
  },
  {
    "objectID": "advanced/01-02-causal-roadmap.html#step-4-link-the-causal-estimand-to-the-observed-data",
    "href": "advanced/01-02-causal-roadmap.html#step-4-link-the-causal-estimand-to-the-observed-data",
    "title": "Chapter 1.2: The Causal Roadmap",
    "section": "Step 4: Link the Causal Estimand to the Observed Data",
    "text": "Step 4: Link the Causal Estimand to the Observed Data\nThis is the identification step. It answers the question: Can we estimate the causal effect from the data we have?\nRequires three assumptions: - Exchangeability (No unmeasured confounding): \\(Y(a) \\perp A | W\\) - Positivity: Everyone has a nonzero probability of receiving each treatment, given W - Consistency: The observed outcome equals the potential outcome under the received treatment\nIf satisfied, we can identify:\n\\[\nE[Y(1) - Y(0)] = E_W[ E[Y | A = 1, W] - E[Y | A = 0, W] ]\n\\]"
  },
  {
    "objectID": "advanced/01-02-causal-roadmap.html#step-5-choose-a-statistical-estimator",
    "href": "advanced/01-02-causal-roadmap.html#step-5-choose-a-statistical-estimator",
    "title": "Chapter 1.2: The Causal Roadmap",
    "section": "Step 5: Choose a Statistical Estimator",
    "text": "Step 5: Choose a Statistical Estimator\nThis step translates the identified quantity into an algorithm we can apply to data.\nOptions include: - G-computation: Predict outcomes under each treatment, average over W - IPTW: Weight observations by inverse probability of treatment - TMLE: Targeted learning that combines outcome and treatment models\nWe’ll explore these in later chapters. The choice depends on: - Assumptions you’re willing to make - Sample size - Tolerance for model misspecification"
  },
  {
    "objectID": "advanced/01-02-causal-roadmap.html#step-6-evaluate-and-interpret",
    "href": "advanced/01-02-causal-roadmap.html#step-6-evaluate-and-interpret",
    "title": "Chapter 1.2: The Causal Roadmap",
    "section": "Step 6: Evaluate and Interpret",
    "text": "Step 6: Evaluate and Interpret\nOnce you’ve estimated your parameter, interpretation is not automatic.\n\nAre your assumptions plausible?\nWhat does your estimate mean for real-world decisions?\nCan you generalize to other populations?\n\nTools for evaluation: - Sensitivity analyses - Negative control outcomes - Covariate balance checks - Expert consultation"
  },
  {
    "objectID": "advanced/01-02-causal-roadmap.html#a-simple-simulated-example",
    "href": "advanced/01-02-causal-roadmap.html#a-simple-simulated-example",
    "title": "Chapter 1.2: The Causal Roadmap",
    "section": "A Simple Simulated Example",
    "text": "A Simple Simulated Example\nWe’ll simulate a small dataset to show how steps 1-5 look in practice.\n\n\nCode\nset.seed(42)\nn &lt;- 1000\nage &lt;- rnorm(n, 75, 6)\ncvd_history &lt;- rbinom(n, 1, plogis(0.1 * (age - 70)))\nW &lt;- data.frame(age, cvd_history)\nA &lt;- rbinom(n, 1, plogis(-1 + 0.05 * (age - 70) + 1.5 * cvd_history))\nY &lt;- rbinom(n, 1, plogis(-2 + 0.4 * A + 0.1 * (age - 70) + 1 * cvd_history))\ndata &lt;- data.frame(W, A, Y)\n\n\nEstimate ATE using g-computation:\n\n\nCode\nmodel &lt;- glm(Y ~ A + age + cvd_history, family = binomial, data = data)\nnewdata1 &lt;- transform(data, A = 1)\nnewdata0 &lt;- transform(data, A = 0)\np1 &lt;- predict(model, newdata = newdata1, type = \"response\")\np0 &lt;- predict(model, newdata = newdata0, type = \"response\")\nmean(p1 - p0)\n\n\n[1] 0.1206902"
  },
  {
    "objectID": "advanced/01-02-causal-roadmap.html#summary",
    "href": "advanced/01-02-causal-roadmap.html#summary",
    "title": "Chapter 1.2: The Causal Roadmap",
    "section": "Summary",
    "text": "Summary\nThe Causal Roadmap gives a rigorous framework for designing and evaluating real-world effect estimates. Before choosing a statistical method, start with the science: - Define your causal question - Specify your model and assumptions - Decide how to express the effect - Choose a transparent, defensible method\nIn the next chapter, we’ll explore different estimation strategies in detail."
  },
  {
    "objectID": "advanced/03-03-longitudinal-case-study.html",
    "href": "advanced/03-03-longitudinal-case-study.html",
    "title": "Chapter 3.3: Case Study – Real-World Application in Longitudinal Analysis",
    "section": "",
    "text": "This chapter walks through a complete applied example of longitudinal causal inference using real-world data concepts.\nThe goal is to help you translate the earlier theoretical chapters into a practical analysis workflow.\nWe use the motivating real-world question:\n\nWhat is the 3-year cardiovascular risk difference if all eligible patients initiating osteoporosis therapy remained on denosumab vs. zoledronic acid under full adherence?\n\nAlthough we cannot use the proprietary data from the actual studies, this chapter recreates a plausible longitudinal structure and walks you through the entire pipeline:\n\nConstructing the longitudinal dataset\n\nDefining the causal question, intervention, and estimand\n\nIdentifying longitudinal confounders and censoring\n\nUsing SuperLearner for nuisance function estimation\n\nApplying TMLE or LMTP for longitudinal causal effects\n\nInterpreting the results in a regulatory and clinical context\n\nThis chapter integrates lessons from Chapter 1 (causal roadmap) and Chapter 2 (estimation)."
  },
  {
    "objectID": "advanced/03-03-longitudinal-case-study.html#case-study-real-world-longitudinal-causal-inference-using-targeted-learning",
    "href": "advanced/03-03-longitudinal-case-study.html#case-study-real-world-longitudinal-causal-inference-using-targeted-learning",
    "title": "Chapter 3.3: Case Study – Real-World Application in Longitudinal Analysis",
    "section": "",
    "text": "This chapter walks through a complete applied example of longitudinal causal inference using real-world data concepts.\nThe goal is to help you translate the earlier theoretical chapters into a practical analysis workflow.\nWe use the motivating real-world question:\n\nWhat is the 3-year cardiovascular risk difference if all eligible patients initiating osteoporosis therapy remained on denosumab vs. zoledronic acid under full adherence?\n\nAlthough we cannot use the proprietary data from the actual studies, this chapter recreates a plausible longitudinal structure and walks you through the entire pipeline:\n\nConstructing the longitudinal dataset\n\nDefining the causal question, intervention, and estimand\n\nIdentifying longitudinal confounders and censoring\n\nUsing SuperLearner for nuisance function estimation\n\nApplying TMLE or LMTP for longitudinal causal effects\n\nInterpreting the results in a regulatory and clinical context\n\nThis chapter integrates lessons from Chapter 1 (causal roadmap) and Chapter 2 (estimation)."
  }
]