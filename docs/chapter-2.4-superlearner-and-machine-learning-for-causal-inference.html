<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Chapter 2.4: SuperLearner and Machine Learning for Causal Inference | Resources</title>
  <meta name="description" content="Causal learnin handbook, including interactive targeted learning workshop materials and linkouts to annotated bibliographies and more advanced examples." />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Chapter 2.4: SuperLearner and Machine Learning for Causal Inference | Resources" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Causal learnin handbook, including interactive targeted learning workshop materials and linkouts to annotated bibliographies and more advanced examples." />
  <meta name="github-repo" content="https://github.com/amertens/causal-learning-handbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Chapter 2.4: SuperLearner and Machine Learning for Causal Inference | Resources" />
  
  <meta name="twitter:description" content="Causal learnin handbook, including interactive targeted learning workshop materials and linkouts to annotated bibliographies and more advanced examples." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="chapter-x-common-pitfalls-and-how-to-avoid-them.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Causal Learning Handbook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome!</a></li>
<li class="chapter" data-level="2" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><i class="fa fa-check"></i><b>2</b> Chapter 2.4: SuperLearner and Machine Learning for Causal Inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#why-use-superlearner-in-causal-inference"><i class="fa fa-check"></i><b>2.1</b> 1. Why Use SuperLearner in Causal Inference?</a></li>
<li class="chapter" data-level="2.2" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#conceptual-overview-of-superlearner"><i class="fa fa-check"></i><b>2.2</b> 2. Conceptual Overview of SuperLearner</a></li>
<li class="chapter" data-level="2.3" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#a-minimal-working-example"><i class="fa fa-check"></i><b>2.3</b> 3. A Minimal Working Example</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#simulated-data"><i class="fa fa-check"></i><b>2.3.1</b> 3.1 Simulated data</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#using-the-superlearner-package"><i class="fa fa-check"></i><b>2.4</b> 4. Using the <code>SuperLearner</code> Package</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#basic-call"><i class="fa fa-check"></i><b>2.4.1</b> 4.1 Basic call</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#choosing-a-loss-function-mse-vs-log-likelihood-vs-auc"><i class="fa fa-check"></i><b>2.5</b> 5. Choosing a Loss Function: MSE vs Log-Likelihood vs AUC</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#mean-squared-error-mse"><i class="fa fa-check"></i><b>2.5.1</b> 5.1 Mean Squared Error (MSE)</a></li>
<li class="chapter" data-level="2.5.2" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#negative-log-likelihood-binomial-deviance"><i class="fa fa-check"></i><b>2.5.2</b> 5.2 Negative Log-Likelihood (Binomial deviance)</a></li>
<li class="chapter" data-level="2.5.3" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#auc-rank-loss"><i class="fa fa-check"></i><b>2.5.3</b> 5.3 AUC / Rank Loss</a></li>
<li class="chapter" data-level="2.5.4" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#which-loss-should-i-choose"><i class="fa fa-check"></i><b>2.5.4</b> 5.4 Which loss should I choose?</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#interpreting-cv-risk-and-coefficient-weights"><i class="fa fa-check"></i><b>2.6</b> 6. Interpreting CV-Risk and Coefficient Weights</a></li>
<li class="chapter" data-level="2.7" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#customizing-the-library-and-tuning-learners"><i class="fa fa-check"></i><b>2.7</b> 7. Customizing the Library and Tuning Learners</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#adding-tuned-versions-of-a-learner-e.g.-random-forest"><i class="fa fa-check"></i><b>2.7.1</b> 7.1 Adding tuned versions of a learner (e.g., Random Forest)</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cross-validated-superlearner-cv.superlearner"><i class="fa fa-check"></i><b>2.8</b> 8. Cross-Validated SuperLearner (<code>CV.SuperLearner</code>)</a></li>
<li class="chapter" data-level="2.9" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#integrating-superlearner-into-causal-inference"><i class="fa fa-check"></i><b>2.9</b> 9. Integrating SuperLearner into Causal Inference</a></li>
<li class="chapter" data-level="2.10" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#practical-tips-for-using-superlearner"><i class="fa fa-check"></i><b>2.10</b> 10. Practical Tips for Using SuperLearner</a></li>
<li class="chapter" data-level="2.11" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#summary"><i class="fa fa-check"></i><b>2.11</b> 11. Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><i class="fa fa-check"></i><b>3</b> Chapter X: Common Pitfalls and How to Avoid Them</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#pitfall-confusing-association-with-causation"><i class="fa fa-check"></i><b>3.1</b> 1. Pitfall: Confusing Association With Causation</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#the-regression-coefficient-problem"><i class="fa fa-check"></i><b>3.1.1</b> The regression coefficient problem</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#pitfall-adjusting-for-post-treatment-variables"><i class="fa fa-check"></i><b>3.2</b> 2. Pitfall: Adjusting for Post-Treatment Variables</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#the-mediatorcollider-trap"><i class="fa fa-check"></i><b>3.2.1</b> The mediator/collider trap</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#pitfall-violated-positivity-or-limited-overlap"><i class="fa fa-check"></i><b>3.3</b> 3. Pitfall: Violated Positivity or Limited Overlap</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#treatment-not-comparable-across-covariate-strata"><i class="fa fa-check"></i><b>3.3.1</b> Treatment not comparable across covariate strata</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#pitfall-misspecified-outcome-or-propensity-models"><i class="fa fa-check"></i><b>3.4</b> 4. Pitfall: Misspecified Outcome or Propensity Models</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#relying-on-simple-models-in-complex-settings"><i class="fa fa-check"></i><b>3.4.1</b> Relying on simple models in complex settings</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#pitfall-blindly-trusting-machine-learning"><i class="fa fa-check"></i><b>3.5</b> 5. Pitfall: Blindly Trusting Machine Learning</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#flexible-models-do-not-guarantee-valid-inference"><i class="fa fa-check"></i><b>3.5.1</b> Flexible models do not guarantee valid inference</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#pitfall-ignoring-censoring-and-informative-dropout"><i class="fa fa-check"></i><b>3.6</b> 6. Pitfall: Ignoring Censoring and Informative Dropout</a></li>
<li class="chapter" data-level="3.7" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#pitfall-over-interpreting-heterogeneous-treatment-effects"><i class="fa fa-check"></i><b>3.7</b> 7. Pitfall: Over-interpreting Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#subgroup-and-hte-analyses-are-fragile"><i class="fa fa-check"></i><b>3.7.1</b> Subgroup and HTE analyses are fragile</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#pitfall-using-the-wrong-estimand"><i class="fa fa-check"></i><b>3.8</b> 8. Pitfall: Using the Wrong Estimand</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#odds-ratios-and-hazard-ratios-are-commonly-misinterpreted"><i class="fa fa-check"></i><b>3.8.1</b> Odds ratios and hazard ratios are commonly misinterpreted</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#pitfall-not-performing-diagnostics"><i class="fa fa-check"></i><b>3.9</b> 9. Pitfall: Not Performing Diagnostics</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#the-model-ran-does-not-imply-validity"><i class="fa fa-check"></i><b>3.9.1</b> “The model ran” does not imply validity</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#pitfall-neglecting-sensitivity-analyses"><i class="fa fa-check"></i><b>3.10</b> 10. Pitfall: Neglecting Sensitivity Analyses</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#unmeasured-confounding-is-not-solved-by-optimism"><i class="fa fa-check"></i><b>3.10.1</b> Unmeasured confounding is not solved by optimism</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#pitfall-poor-alignment-between-randomized-and-real-world-data-in-hybrid-designs"><i class="fa fa-check"></i><b>3.11</b> 11. Pitfall: Poor Alignment Between Randomized and Real-World Data in Hybrid Designs</a></li>
<li class="chapter" data-level="3.12" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#pitfall-reporting-without-context-or-interpretation"><i class="fa fa-check"></i><b>3.12</b> 12. Pitfall: Reporting Without Context or Interpretation</a></li>
<li class="chapter" data-level="3.13" data-path="chapter-x-common-pitfalls-and-how-to-avoid-them.html"><a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html#summary-1"><i class="fa fa-check"></i><b>3.13</b> 13. Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="annotated-bibliography-modern-causal-inference-and-targeted-learning.html"><a href="annotated-bibliography-modern-causal-inference-and-targeted-learning.html"><i class="fa fa-check"></i><b>4</b> Annotated Bibliography: Modern Causal Inference and Targeted Learning</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="annotated-bibliography-modern-causal-inference-and-targeted-learning.html"><a href="annotated-bibliography-modern-causal-inference-and-targeted-learning.html#to-add"><i class="fa fa-check"></i><b>4.0.1</b> to add:</a></li>
<li class="chapter" data-level="4.1" data-path="annotated-bibliography-modern-causal-inference-and-targeted-learning.html"><a href="annotated-bibliography-modern-causal-inference-and-targeted-learning.html#causal-inference-roadmap-and-target-trial-emulation"><i class="fa fa-check"></i><b>4.1</b> 1. Causal Inference Roadmap and Target Trial Emulation</a></li>
<li class="chapter" data-level="4.2" data-path="annotated-bibliography-modern-causal-inference-and-targeted-learning.html"><a href="annotated-bibliography-modern-causal-inference-and-targeted-learning.html#estimand-specification-in-clinical-studies"><i class="fa fa-check"></i><b>4.2</b> 2. Estimand Specification in Clinical Studies</a></li>
<li class="chapter" data-level="4.3" data-path="annotated-bibliography-modern-causal-inference-and-targeted-learning.html"><a href="annotated-bibliography-modern-causal-inference-and-targeted-learning.html#super-learner-ensemble-learning"><i class="fa fa-check"></i><b>4.3</b> 3. Super Learner Ensemble Learning</a></li>
<li class="chapter" data-level="4.4" data-path="annotated-bibliography-modern-causal-inference-and-targeted-learning.html"><a href="annotated-bibliography-modern-causal-inference-and-targeted-learning.html#targeted-maximum-likelihood-estimation-tmle"><i class="fa fa-check"></i><b>4.4</b> 4. Targeted Maximum Likelihood Estimation (TMLE)</a></li>
<li class="chapter" data-level="4.5" data-path="annotated-bibliography-modern-causal-inference-and-targeted-learning.html"><a href="annotated-bibliography-modern-causal-inference-and-targeted-learning.html#time-dependent-confounding-and-intercurrent-events"><i class="fa fa-check"></i><b>4.5</b> 5. Time-Dependent Confounding and Intercurrent Events</a></li>
<li class="chapter" data-level="4.6" data-path="annotated-bibliography-modern-causal-inference-and-targeted-learning.html"><a href="annotated-bibliography-modern-causal-inference-and-targeted-learning.html#dynamic-treatment-regimes-and-stochastic-interventions"><i class="fa fa-check"></i><b>4.6</b> 6. Dynamic Treatment Regimes and Stochastic Interventions</a></li>
<li class="chapter" data-level="4.7" data-path="annotated-bibliography-modern-causal-inference-and-targeted-learning.html"><a href="annotated-bibliography-modern-causal-inference-and-targeted-learning.html#longitudinal-tmle-and-related-methods"><i class="fa fa-check"></i><b>4.7</b> 7. Longitudinal TMLE and Related Methods</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="annotated-bibliography-modern-causal-inference-and-targeted-learning.html"><a href="annotated-bibliography-modern-causal-inference-and-targeted-learning.html#suggested-reading-order"><i class="fa fa-check"></i><b>4.7.1</b> Suggested Reading Order</a></li>
<li class="chapter" data-level="4.7.2" data-path="annotated-bibliography-modern-causal-inference-and-targeted-learning.html"><a href="annotated-bibliography-modern-causal-inference-and-targeted-learning.html#reference-files-link-pdfs"><i class="fa fa-check"></i><b>4.7.2</b> Reference Files (link pdfs)</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Resources</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter-2.4-superlearner-and-machine-learning-for-causal-inference" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Chapter 2.4: SuperLearner and Machine Learning for Causal Inference<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#chapter-2.4-superlearner-and-machine-learning-for-causal-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Flexible prediction to strengthen causal effect estimation</em></p>
<p>Modern causal inference relies on estimating <strong>nuisance functions</strong> (outcome regressions and treatment / censoring mechanisms) that are as accurate as possible. If these models are mis-specified, even sophisticated causal estimators can be biased.</p>
<p>Rather than gambling on a single model (e.g., logistic regression), we can <strong>stack</strong> many candidate learners and let the data decide how to combine them. This is what <strong>SuperLearner</strong> does.</p>
<p>In this chapter you will learn:</p>
<ul>
<li>The intuition behind SuperLearner (SL) and stacking<br />
</li>
<li>How cross-validation is used to avoid overfitting<br />
</li>
<li>How to build and interpret SuperLearner models in R<br />
</li>
<li>When and why to choose different <strong>loss functions</strong> (MSE, log-likelihood, AUC)<br />
</li>
<li>How to customize SL libraries and tune algorithms<br />
</li>
<li>How SL integrates with TMLE and other causal estimators</li>
</ul>
<p>This chapter leans heavily on the excellent visual tutorial by Katherine Hoffman and the SuperLearner demo by David Benkeser (both provided as PDFs), and recasts them in a causal-inference focused Quarto format.</p>
<hr />
<div id="why-use-superlearner-in-causal-inference" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> 1. Why Use SuperLearner in Causal Inference?<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#why-use-superlearner-in-causal-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Causal estimators such as g-computation, IPTW, AIPW, and TMLE rely on estimating:</p>
<ul>
<li><p>The <strong>outcome regression</strong>:<br />
<span class="math inline">\(Q(W, A) = E[Y \mid W, A]\)</span></p></li>
<li><p>The <strong>treatment (or censoring) mechanism</strong>:<br />
<span class="math inline">\(g(W) = P(A = 1 \mid W)\)</span></p></li>
</ul>
<p>In traditional practice, both are often modeled with simple GLMs. This is dangerous when:</p>
<ul>
<li>Relationships are nonlinear</li>
<li>Interactions are present</li>
<li>There are many covariates</li>
<li>We are unsure about which variables to include or in what functional form</li>
</ul>
<p>SuperLearner helps by:</p>
<ul>
<li>Combining multiple algorithms (GLM, random forests, LASSO, boosted trees, etc.)</li>
<li>Using <strong>K-fold cross-validation</strong> to evaluate and weight each algorithm</li>
<li>Producing an ensemble predictor with theoretical guarantees (an “oracle inequality”): asymptotically, SL performs nearly as well as the best algorithm in the library</li>
</ul>
<p>In causal inference, we rarely care about prediction for its own sake, but good prediction of nuisance functions leads to <strong>better causal effect estimation</strong>.</p>
<hr />
</div>
<div id="conceptual-overview-of-superlearner" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> 2. Conceptual Overview of SuperLearner<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#conceptual-overview-of-superlearner" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>At a high level, SuperLearner does the following:</p>
<ol style="list-style-type: decimal">
<li>Pick a <strong>set of candidate learners</strong> (the library).</li>
<li>Split the data into <strong>K folds</strong>.</li>
<li>For each learner:
<ul>
<li>Fit on K-1 folds (training data),</li>
<li>Predict on the held-out fold (validation data).</li>
</ul></li>
<li>Collect <strong>cross-validated predictions</strong> for every observation and every learner.</li>
<li>Fit a <strong>metalearner</strong> (often a linear regression) that finds the optimal weighted combination of the learners’ predictions to minimize a chosen <strong>loss function</strong> (e.g., mean squared error, negative log-likelihood).</li>
<li>Refit each base learner on the full dataset.</li>
<li>Use the metalearner and the refit base learners to form the final ensemble and obtain predictions for new data.</li>
</ol>
<p>This is exactly the workflow illustrated in the “VISUAL GUIDE TO SUPERLEARNING” figure in the KHstats tutorial.</p>
<hr />
</div>
<div id="a-minimal-working-example" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> 3. A Minimal Working Example<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#a-minimal-working-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’ll start with a simple prediction problem, then connect it back to causal inference later.</p>
<div id="simulated-data" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> 3.1 Simulated data<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#simulated-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-2" tabindex="-1"></a><span class="fu">library</span>(SuperLearner)</span>
<span id="cb1-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb1-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb1-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-7" tabindex="-1"></a>obs <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb1-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-8" tabindex="-1"></a>  <span class="at">id =</span> <span class="dv">1</span><span class="sc">:</span>n,</span>
<span id="cb1-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-9" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">rnorm</span>(n),</span>
<span id="cb1-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-10" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">rbinom</span>(n, <span class="dv">1</span>, <span class="fu">plogis</span>(<span class="dv">10</span> <span class="sc">*</span> x1)),</span>
<span id="cb1-11"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-11" tabindex="-1"></a>  <span class="at">x3 =</span> <span class="fu">rbinom</span>(n, <span class="dv">1</span>, <span class="fu">plogis</span>(x1 <span class="sc">*</span> x2 <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> x2)),</span>
<span id="cb1-12"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-12" tabindex="-1"></a>  <span class="at">x4 =</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> x1 <span class="sc">*</span> x2, <span class="at">sd =</span> <span class="fl">0.5</span> <span class="sc">*</span> x3),</span>
<span id="cb1-13"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-13" tabindex="-1"></a>  <span class="at">y  =</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x2 <span class="sc">*</span> x3 <span class="sc">+</span> <span class="fu">sin</span>(x4) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.2</span>)</span>
<span id="cb1-14"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-14" tabindex="-1"></a>)</span>
<span id="cb1-15"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb1-16" tabindex="-1"></a><span class="fu">glimpse</span>(obs)</span></code></pre></div>
<pre><code>## Rows: 2,000
## Columns: 6
## $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, …
## $ x1 &lt;dbl&gt; 2.287247161, -1.196771682, -0.694292510, -0.412292951, -0.970673341, -0.947279945, 0.748139340, -0.116955226, 0.1526576…
## $ x2 &lt;int&gt; 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,…
## $ x3 &lt;int&gt; 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,…
## $ x4 &lt;dbl&gt; 1.50283924, 0.04260947, 0.00000000, 0.00000000, 0.00000000, 1.07555690, 0.90481789, 0.03292026, 0.15265763, 3.04174063,…
## $ y  &lt;dbl&gt; 5.03669124, -1.13306036, -0.38284042, -0.28671230, -0.87539307, -0.23969890, 3.82052213, -0.25644487, 1.43066246, 3.979…</code></pre>
<p>The outcome <code>y</code> is a nonlinear function of the covariates, with interactions and a sine term. GLMs will struggle here.</p>
<hr />
</div>
</div>
<div id="using-the-superlearner-package" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> 4. Using the <code>SuperLearner</code> Package<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#using-the-superlearner-package" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="basic-call" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> 4.1 Basic call<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#basic-call" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’ll start with a small library for illustration.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb3-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-3" tabindex="-1"></a>X <span class="ot">&lt;-</span> obs <span class="sc">%&gt;%</span> <span class="fu">select</span>(x1<span class="sc">:</span>x4) <span class="sc">%&gt;%</span> <span class="fu">as.data.frame</span>()</span>
<span id="cb3-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-4" tabindex="-1"></a>Y <span class="ot">&lt;-</span> obs<span class="sc">$</span>y</span>
<span id="cb3-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-6" tabindex="-1"></a>SL.lib <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;SL.glm&quot;</span>,      <span class="co"># simple GLM</span></span>
<span id="cb3-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-7" tabindex="-1"></a>            <span class="st">&quot;SL.mean&quot;</span>,     <span class="co"># intercept-only</span></span>
<span id="cb3-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-8" tabindex="-1"></a>            <span class="st">&quot;SL.earth&quot;</span>,    <span class="co"># multivariate adaptive regression splines (MARS)</span></span>
<span id="cb3-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-9" tabindex="-1"></a>            <span class="st">&quot;SL.ranger&quot;</span>)   <span class="co"># random forest</span></span>
<span id="cb3-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-10" tabindex="-1"></a></span>
<span id="cb3-11"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-11" tabindex="-1"></a>sl_fit <span class="ot">&lt;-</span> <span class="fu">SuperLearner</span>(</span>
<span id="cb3-12"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-12" tabindex="-1"></a>  <span class="at">Y =</span> Y,</span>
<span id="cb3-13"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-13" tabindex="-1"></a>  <span class="at">X =</span> X,</span>
<span id="cb3-14"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-14" tabindex="-1"></a>  <span class="at">newX =</span> <span class="cn">NULL</span>,</span>
<span id="cb3-15"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-15" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">gaussian</span>(),</span>
<span id="cb3-16"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-16" tabindex="-1"></a>  <span class="at">SL.library =</span> SL.lib,</span>
<span id="cb3-17"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-17" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;method.NNLS&quot;</span>,  <span class="co"># non-negative least squares metalearner</span></span>
<span id="cb3-18"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-18" tabindex="-1"></a>  <span class="at">cvControl =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">10</span><span class="dt">L</span>)</span>
<span id="cb3-19"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-19" tabindex="-1"></a>)</span>
<span id="cb3-20"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-20" tabindex="-1"></a></span>
<span id="cb3-21"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb3-21" tabindex="-1"></a>sl_fit</span></code></pre></div>
<pre><code>## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = NULL, family = gaussian(), SL.library = SL.lib, method = &quot;method.NNLS&quot;, cvControl = list(V = 10L)) 
## 
## 
## 
##                     Risk      Coef
## SL.glm_All    0.15038334 0.0000000
## SL.mean_All   4.52716718 0.0000000
## SL.earth_All  0.04408952 0.8593302
## SL.ranger_All 0.05735981 0.1406698</code></pre>
<p>Key outputs:</p>
<ul>
<li><code>Risk</code>: cross-validated risk (e.g., MSE) for each learner</li>
<li><code>Coef</code>: weight given to each learner in the ensemble</li>
</ul>
<p>The learner with the smallest CV-risk often gets the largest weight, but SL can combine learners.</p>
<p>We can access the <strong>ensemble predictions</strong>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb5-1" tabindex="-1"></a><span class="fu">head</span>(sl_fit<span class="sc">$</span>SL.predict)</span></code></pre></div>
<pre><code>##          [,1]
## 1  5.36960217
## 2 -1.15651994
## 3 -0.66557910
## 4 -0.39653932
## 5 -0.94879355
## 6 -0.04451723</code></pre>
<p>and predictions from individual learners:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb7-1" tabindex="-1"></a><span class="fu">head</span>(sl_fit<span class="sc">$</span>library.predict)</span></code></pre></div>
<pre><code>##   SL.glm_All SL.mean_All SL.earth_All SL.ranger_All
## 1  4.9119003    1.145784   5.44169210     4.9292158
## 2 -0.9619611    1.145784  -1.14585410    -1.2216759
## 3 -0.8907976    1.145784  -0.67326080    -0.6186528
## 4 -0.6300543    1.145784  -0.40211488    -0.3624791
## 5 -1.1463457    1.145784  -0.94947428    -0.9446351
## 6 -0.1673960    1.145784  -0.01723667    -0.2111701</code></pre>
<hr />
</div>
</div>
<div id="choosing-a-loss-function-mse-vs-log-likelihood-vs-auc" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> 5. Choosing a Loss Function: MSE vs Log-Likelihood vs AUC<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#choosing-a-loss-function-mse-vs-log-likelihood-vs-auc" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SuperLearner allows different <strong>loss functions</strong>, which define what we mean by “best” prediction.</p>
<div id="mean-squared-error-mse" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> 5.1 Mean Squared Error (MSE)<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#mean-squared-error-mse" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Default for <code>family = gaussian()</code></li>
<li>Appropriate for <strong>continuous outcomes</strong> when we care about squared error:
<span class="math display">\[ L(y, \hat{y}) = (y - \hat{y})^2 \]</span></li>
<li>Good when we want well-calibrated mean predictions</li>
</ul>
<p>Example (already used above): <code>method = "method.NNLS"</code> with <code>family = gaussian()</code></p>
</div>
<div id="negative-log-likelihood-binomial-deviance" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> 5.2 Negative Log-Likelihood (Binomial deviance)<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#negative-log-likelihood-binomial-deviance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Natural choice for <strong>binary outcomes</strong> when we care about probability calibration:
<span class="math display">\[ L(y, \hat{p}) = -[y \log(\hat{p}) + (1-y) \log(1-\hat{p})] \]</span></li>
<li>Strongly penalizes confident but wrong predictions</li>
<li>Recommended for:
<ul>
<li>Outcome models (<code>Y</code> binary)</li>
<li>Treatment models (<code>A</code> binary) in causal inference</li>
</ul></li>
</ul>
<p>Use <code>method = "method.NNloglik"</code> with <code>family = binomial()</code>.</p>
<p>Example:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb9-1" tabindex="-1"></a><span class="co"># suppose Y is binary</span></span>
<span id="cb9-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb9-2" tabindex="-1"></a>Y_bin <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n, <span class="dv">1</span>, <span class="fu">plogis</span>(X<span class="sc">$</span>x1 <span class="sc">+</span> X<span class="sc">$</span>x2))</span>
<span id="cb9-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb9-3" tabindex="-1"></a></span>
<span id="cb9-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb9-4" tabindex="-1"></a>sl_loglik <span class="ot">&lt;-</span> <span class="fu">SuperLearner</span>(</span>
<span id="cb9-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb9-5" tabindex="-1"></a>  <span class="at">Y =</span> Y_bin,</span>
<span id="cb9-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb9-6" tabindex="-1"></a>  <span class="at">X =</span> X,</span>
<span id="cb9-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb9-7" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">binomial</span>(),</span>
<span id="cb9-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb9-8" tabindex="-1"></a>  <span class="at">SL.library =</span> <span class="fu">c</span>(<span class="st">&quot;SL.glm&quot;</span>, <span class="st">&quot;SL.mean&quot;</span>, <span class="st">&quot;SL.ranger&quot;</span>),</span>
<span id="cb9-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb9-9" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;method.NNloglik&quot;</span></span>
<span id="cb9-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb9-10" tabindex="-1"></a>)</span>
<span id="cb9-11"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb9-11" tabindex="-1"></a></span>
<span id="cb9-12"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb9-12" tabindex="-1"></a>sl_loglik</span></code></pre></div>
<pre><code>## 
## Call:  
## SuperLearner(Y = Y_bin, X = X, family = binomial(), SL.library = c(&quot;SL.glm&quot;, &quot;SL.mean&quot;, &quot;SL.ranger&quot;), method = &quot;method.NNloglik&quot;) 
## 
## 
## 
##                    Risk      Coef
## SL.glm_All    0.5286216 0.8914129
## SL.mean_All   0.6818619 0.0000000
## SL.ranger_All 0.5537130 0.1085871</code></pre>
</div>
<div id="auc-rank-loss" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> 5.3 AUC / Rank Loss<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#auc-rank-loss" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>For classification problems where the <strong>ranking</strong> of probabilities matters more than calibration</li>
<li>Common when choosing a threshold later (e.g., risk stratification)</li>
<li>SuperLearner implementation: <code>method = "method.AUC"</code></li>
<li>Particularly useful when interested in discriminatory ability (e.g., disease risk scores)</li>
</ul>
<p>Example:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb11-1" tabindex="-1"></a><span class="fu">library</span>(cvAUC)   <span class="co"># required by method.AUC</span></span>
<span id="cb11-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb11-3" tabindex="-1"></a>sl_auc <span class="ot">&lt;-</span> <span class="fu">SuperLearner</span>(</span>
<span id="cb11-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb11-4" tabindex="-1"></a>  <span class="at">Y =</span> Y_bin,</span>
<span id="cb11-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb11-5" tabindex="-1"></a>  <span class="at">X =</span> X,</span>
<span id="cb11-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb11-6" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">binomial</span>(),</span>
<span id="cb11-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb11-7" tabindex="-1"></a>  <span class="at">SL.library =</span> <span class="fu">c</span>(<span class="st">&quot;SL.glm&quot;</span>, <span class="st">&quot;SL.mean&quot;</span>, <span class="st">&quot;SL.ranger&quot;</span>),</span>
<span id="cb11-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb11-8" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;method.AUC&quot;</span></span>
<span id="cb11-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb11-9" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Warning in method$computeCoef(Z = Z, Y = Y, libraryNames = libraryNames, : optim didn&#39;t converge when estimating the super learner
## coefficients, reason (see ?optim): 52 optim message: ERROR: ABNORMAL_TERMINATION_IN_LNSRCH</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb13-1" tabindex="-1"></a>sl_auc</span></code></pre></div>
<pre><code>## 
## Call:  
## SuperLearner(Y = Y_bin, X = X, family = binomial(), SL.library = c(&quot;SL.glm&quot;, &quot;SL.mean&quot;, &quot;SL.ranger&quot;), method = &quot;method.AUC&quot;) 
## 
## 
##                    Risk         Coef
## SL.glm_All    0.1938743 0.9988292700
## SL.mean_All   0.5268196 0.0006672332
## SL.ranger_All 0.2104779 0.0005034968</code></pre>
</div>
<div id="which-loss-should-i-choose" class="section level3 hasAnchor" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> 5.4 Which loss should I choose?<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#which-loss-should-i-choose" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>For outcome models in TMLE/AIPW:</strong>
<ul>
<li>If binary → log-likelihood (binomial deviance)<br />
</li>
<li>If continuous → MSE or other appropriate distribution-based loss<br />
</li>
</ul></li>
<li><strong>For treatment / censoring models in causal inference:</strong>
<ul>
<li>Typically log-likelihood (because we want accurate estimates of <code>P(A | W)</code>)</li>
</ul></li>
<li><strong>For pure classification (no causal estimation):</strong>
<ul>
<li>Consider AUC loss (<code>method.AUC</code>) if ranking is the priority</li>
</ul></li>
</ul>
<hr />
</div>
</div>
<div id="interpreting-cv-risk-and-coefficient-weights" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> 6. Interpreting CV-Risk and Coefficient Weights<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#interpreting-cv-risk-and-coefficient-weights" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb15-1" tabindex="-1"></a>sl_fit<span class="sc">$</span>cvRisk</span></code></pre></div>
<pre><code>##    SL.glm_All   SL.mean_All  SL.earth_All SL.ranger_All 
##    0.15038334    4.52716718    0.04408952    0.05735981</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb17-1" tabindex="-1"></a>sl_fit<span class="sc">$</span>coef</span></code></pre></div>
<pre><code>##    SL.glm_All   SL.mean_All  SL.earth_All SL.ranger_All 
##     0.0000000     0.0000000     0.8593302     0.1406698</code></pre>
<ul>
<li><code>cvRisk</code> shows cross-validated risk for each algorithm</li>
<li><code>coef</code> gives the ensemble weights (metalearner solution)</li>
</ul>
<p>An algorithm might have:</p>
<ul>
<li>Low risk → high weight<br />
</li>
<li>High risk → weight near zero (effectively excluded)</li>
</ul>
<p>This matches the demonstration in the Benkeser notes where GLM dominates mean-only models when predicting MI.</p>
<hr />
</div>
<div id="customizing-the-library-and-tuning-learners" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> 7. Customizing the Library and Tuning Learners<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#customizing-the-library-and-tuning-learners" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="adding-tuned-versions-of-a-learner-e.g.-random-forest" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> 7.1 Adding tuned versions of a learner (e.g., Random Forest)<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#adding-tuned-versions-of-a-learner-e.g.-random-forest" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can define bounded random forest variants with different hyperparameters.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb19-1" tabindex="-1"></a>SL.ranger_mtry3 <span class="ot">&lt;-</span> <span class="cf">function</span>(..., <span class="at">mtry =</span> <span class="dv">3</span>){</span>
<span id="cb19-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb19-2" tabindex="-1"></a>  <span class="fu">SL.ranger</span>(..., <span class="at">mtry =</span> mtry)</span>
<span id="cb19-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb19-3" tabindex="-1"></a>}</span>
<span id="cb19-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb19-4" tabindex="-1"></a></span>
<span id="cb19-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb19-5" tabindex="-1"></a>SL.ranger_mtry4 <span class="ot">&lt;-</span> <span class="cf">function</span>(..., <span class="at">mtry =</span> <span class="dv">4</span>){</span>
<span id="cb19-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb19-6" tabindex="-1"></a>  <span class="fu">SL.ranger</span>(..., <span class="at">mtry =</span> mtry)</span>
<span id="cb19-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb19-7" tabindex="-1"></a>}</span>
<span id="cb19-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb19-8" tabindex="-1"></a></span>
<span id="cb19-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb19-9" tabindex="-1"></a>SL.lib_tuned <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;SL.glm&quot;</span>,</span>
<span id="cb19-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb19-10" tabindex="-1"></a>                  <span class="st">&quot;SL.earth&quot;</span>,</span>
<span id="cb19-11"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb19-11" tabindex="-1"></a>                  <span class="st">&quot;SL.ranger_mtry3&quot;</span>,</span>
<span id="cb19-12"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb19-12" tabindex="-1"></a>                  <span class="st">&quot;SL.ranger_mtry4&quot;</span>)</span></code></pre></div>
<p>Then run:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb20-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb20-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb20-2" tabindex="-1"></a>sl_tuned <span class="ot">&lt;-</span> <span class="fu">SuperLearner</span>(</span>
<span id="cb20-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb20-3" tabindex="-1"></a>  <span class="at">Y =</span> Y,</span>
<span id="cb20-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb20-4" tabindex="-1"></a>  <span class="at">X =</span> X,</span>
<span id="cb20-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb20-5" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">gaussian</span>(),</span>
<span id="cb20-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb20-6" tabindex="-1"></a>  <span class="at">SL.library =</span> SL.lib_tuned,</span>
<span id="cb20-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb20-7" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;method.NNLS&quot;</span></span>
<span id="cb20-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb20-8" tabindex="-1"></a>)</span>
<span id="cb20-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb20-9" tabindex="-1"></a></span>
<span id="cb20-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb20-10" tabindex="-1"></a>sl_tuned<span class="sc">$</span>cvRisk</span></code></pre></div>
<pre><code>##          SL.glm_All        SL.earth_All SL.ranger_mtry3_All SL.ranger_mtry4_All 
##          0.15062184          0.04401740          0.05044671          0.05242569</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb22-1" tabindex="-1"></a>sl_tuned<span class="sc">$</span>coef</span></code></pre></div>
<pre><code>##          SL.glm_All        SL.earth_All SL.ranger_mtry3_All SL.ranger_mtry4_All 
##           0.0000000           0.7318382           0.2681618           0.0000000</code></pre>
<p>SuperLearner automatically picks which tuned version (or combination) works best.</p>
<hr />
</div>
</div>
<div id="cross-validated-superlearner-cv.superlearner" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> 8. Cross-Validated SuperLearner (<code>CV.SuperLearner</code>)<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cross-validated-superlearner-cv.superlearner" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><code>CV.SuperLearner</code> adds an <strong>outer layer</strong> of cross-validation to evaluate SL versus its components objectively.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb24-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb24-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb24-2" tabindex="-1"></a>cv_sl <span class="ot">&lt;-</span> <span class="fu">CV.SuperLearner</span>(</span>
<span id="cb24-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb24-3" tabindex="-1"></a>  <span class="at">Y =</span> Y,</span>
<span id="cb24-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb24-4" tabindex="-1"></a>  <span class="at">X =</span> X,</span>
<span id="cb24-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb24-5" tabindex="-1"></a>  <span class="at">V =</span> <span class="dv">5</span>,</span>
<span id="cb24-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb24-6" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">gaussian</span>(),</span>
<span id="cb24-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb24-7" tabindex="-1"></a>  <span class="at">SL.library =</span> SL.lib</span>
<span id="cb24-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb24-8" tabindex="-1"></a>)</span>
<span id="cb24-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb24-9" tabindex="-1"></a></span>
<span id="cb24-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb24-10" tabindex="-1"></a>cv_sl</span></code></pre></div>
<pre><code>## 
## Call:  CV.SuperLearner(Y = Y, X = X, V = 5, family = gaussian(), SL.library = SL.lib) 
## 
## 
## Cross-validated predictions from the SuperLearner:  SL.predict 
## 
## Cross-validated predictions from the discrete super learner (cross-validation selector):  discreteSL.predict 
## 
## Which library algorithm was the discrete super learner:  whichDiscreteSL 
## 
## Cross-validated prediction for all algorithms in the library:  library.predict</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb26-1" tabindex="-1"></a><span class="fu">plot</span>(cv_sl)</span></code></pre></div>
<p><img src="causal-learning-handbook_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>The plot shows:</p>
<ul>
<li>Cross-validated risks and confidence intervals for each learner</li>
<li>Performance of the discrete and continuous SuperLearner</li>
</ul>
<p>This step is particularly helpful when you want to justify using SL rather than a single, simpler algorithm.</p>
<hr />
</div>
<div id="integrating-superlearner-into-causal-inference" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> 9. Integrating SuperLearner into Causal Inference<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#integrating-superlearner-into-causal-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far we focused on prediction. How does this relate to causal inference?</p>
<p>For a point-treatment ATE, a TMLE analysis might look like:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb27-1" tabindex="-1"></a><span class="co"># Example skeleton (you will flesh this out later with your own data)</span></span>
<span id="cb27-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb27-2" tabindex="-1"></a><span class="fu">library</span>(tmle)</span>
<span id="cb27-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb27-3" tabindex="-1"></a></span>
<span id="cb27-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb27-4" tabindex="-1"></a>tmle_fit <span class="ot">&lt;-</span> <span class="fu">tmle</span>(</span>
<span id="cb27-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb27-5" tabindex="-1"></a>  <span class="at">Y =</span> Y_bin,          <span class="co"># binary outcome</span></span>
<span id="cb27-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb27-6" tabindex="-1"></a>  <span class="at">A =</span> A,              <span class="co"># treatment</span></span>
<span id="cb27-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb27-7" tabindex="-1"></a>  <span class="at">W =</span> X,              <span class="co"># covariates</span></span>
<span id="cb27-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb27-8" tabindex="-1"></a>  <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>,</span>
<span id="cb27-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb27-9" tabindex="-1"></a>  <span class="at">Q.SL.library =</span> <span class="fu">c</span>(<span class="st">&quot;SL.glm&quot;</span>, <span class="st">&quot;SL.ranger&quot;</span>, <span class="st">&quot;SL.earth&quot;</span>),</span>
<span id="cb27-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb27-10" tabindex="-1"></a>  <span class="at">g.SL.library =</span> <span class="fu">c</span>(<span class="st">&quot;SL.glm&quot;</span>, <span class="st">&quot;SL.ranger&quot;</span>, <span class="st">&quot;SL.mean&quot;</span>)</span>
<span id="cb27-11"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb27-11" tabindex="-1"></a>)</span>
<span id="cb27-12"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb27-12" tabindex="-1"></a></span>
<span id="cb27-13"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb27-13" tabindex="-1"></a>tmle_fit<span class="sc">$</span>estimates<span class="sc">$</span>ATE</span></code></pre></div>
<p>Here:</p>
<ul>
<li><code>Q.SL.library</code> is used to estimate outcome regression <code>E[Y|A,W]</code></li>
<li><code>g.SL.library</code> is used to estimate propensity scores <code>P(A|W)</code></li>
<li>TMLE combines these with targeting to produce an efficient, doubly robust estimate</li>
</ul>
<p>Key advantages:</p>
<ul>
<li>You no longer need to guess the “right” model for Y or A<br />
</li>
<li>You can include many flexible learners without overfitting (thanks to SL + CV)<br />
</li>
<li>Your causal inference relies less on arbitrary parametric modeling choices</li>
</ul>
<hr />
</div>
<div id="practical-tips-for-using-superlearner" class="section level2 hasAnchor" number="2.10">
<h2><span class="header-section-number">2.10</span> 10. Practical Tips for Using SuperLearner<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#practical-tips-for-using-superlearner" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><strong>Start with a modest but diverse library</strong>
<ul>
<li>GLM (<code>SL.glm</code>)<br />
</li>
<li>Random forest (<code>SL.ranger</code> or <code>SL.randomForest</code>)<br />
</li>
<li>MARS (<code>SL.earth</code>)<br />
</li>
<li>Penalized regression (<code>SL.glmnet</code>)</li>
</ul></li>
<li><strong>Pick loss functions that match your problem</strong>
<ul>
<li>Binary → log-likelihood (for calibration) or AUC (for ranking)<br />
</li>
<li>Continuous → MSE</li>
</ul></li>
<li><strong>Watch computation time</strong>
<ul>
<li>SL is more expensive than a single GLM, especially with many learners and CV folds.</li>
</ul></li>
<li><strong>Use SL primarily on nuisance functions</strong>
<ul>
<li>Don’t use SL to directly estimate the causal effect; instead, use SL to estimate <code>Q</code> and <code>g</code> and feed these into TMLE, AIPW, etc.</li>
</ul></li>
<li><strong>Inspect SL outputs</strong>
<ul>
<li>Which learners are getting weight?<br />
</li>
<li>Are any learners consistently poor performers?<br />
</li>
<li>Do you need to adjust your library?</li>
</ul></li>
</ol>
<hr />
</div>
<div id="summary" class="section level2 hasAnchor" number="2.11">
<h2><span class="header-section-number">2.11</span> 11. Summary<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter you learned</p>
<ul>
<li>How SuperLearner combines multiple algorithms using cross-validation and a metalearner<br />
</li>
<li>The role of loss functions (MSE, log-likelihood, AUC) and when to choose each<br />
</li>
<li>How to implement SuperLearner in R, inspect CV-risk and weights, and customize the library<br />
</li>
<li>How SuperLearner supports reliable causal inference by improving nuisance function estimation and integrating seamlessly with TMLE and other estimators</li>
</ul>
<p>Next, we move to <strong>Module 3</strong>, where we tackle longitudinal data, dynamic treatment regimes, and modified treatment policies, often using SuperLearner as a core building block.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter-x-common-pitfalls-and-how-to-avoid-them.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
