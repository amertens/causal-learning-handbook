<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Chapter 2.4: SuperLearner and Machine Learning for Causal Inference | Chapter 2.4: SuperLearner and Machine Learning for Causal Inference</title>
  <meta name="description" content="Causal learnin handbook, including interactive targeted learning workshop materials and linkouts to annotated bibliographies and more advanced examples." />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Chapter 2.4: SuperLearner and Machine Learning for Causal Inference | Chapter 2.4: SuperLearner and Machine Learning for Causal Inference" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Causal learnin handbook, including interactive targeted learning workshop materials and linkouts to annotated bibliographies and more advanced examples." />
  <meta name="github-repo" content="https://github.com/amertens/causal-learning-handbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Chapter 2.4: SuperLearner and Machine Learning for Causal Inference | Chapter 2.4: SuperLearner and Machine Learning for Causal Inference" />
  
  <meta name="twitter:description" content="Causal learnin handbook, including interactive targeted learning workshop materials and linkouts to annotated bibliographies and more advanced examples." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="tutorial-on-targeted-learning-and-the-causal-roadmap.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Causal Learning Handbook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><i class="fa fa-check"></i><b>1</b> Tutorial on Targeted Learning and the Causal Roadmap</a>
<ul>
<li class="chapter" data-level="1.1" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#why-venture-down-a-new-path"><i class="fa fa-check"></i><b>1.2</b> Why venture down a new path?</a></li>
<li class="chapter" data-level="1.3" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#notation"><i class="fa fa-check"></i><b>1.3</b> Notation</a></li>
<li class="chapter" data-level="1.4" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#motivation"><i class="fa fa-check"></i><b>1.4</b> Motivation</a></li>
<li class="chapter" data-level="1.5" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#the-causal-roadmap"><i class="fa fa-check"></i><b>1.5</b> The Causal Roadmap</a></li>
<li class="chapter" data-level="1.6" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#step-0-state-the-question"><i class="fa fa-check"></i><b>1.6</b> Step 0: State the question</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#target-trial-emulation"><i class="fa fa-check"></i><b>1.6.1</b> Target Trial Emulation</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#step-1-define-the-causal-model"><i class="fa fa-check"></i><b>1.7</b> Step 1: Define the causal model</a></li>
<li class="chapter" data-level="1.8" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#step-2-define-the-causal-parameter-of-interest"><i class="fa fa-check"></i><b>1.8</b> Step 2: Define the causal parameter of interest</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#estimand-specification-ich-e9r1-framework"><i class="fa fa-check"></i><b>1.8.1</b> Estimand Specification (ICH E9[R1] Framework)</a></li>
<li class="chapter" data-level="1.8.2" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#treatment-policy-versus-hypothetical-estimands"><i class="fa fa-check"></i><b>1.8.2</b> Treatment-Policy versus Hypothetical Estimands</a></li>
<li class="chapter" data-level="1.8.3" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#intercurrent-events"><i class="fa fa-check"></i><b>1.8.3</b> Intercurrent Events</a></li>
<li class="chapter" data-level="1.8.4" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#time-to-event-outcomes-and-risk-based-estimands"><i class="fa fa-check"></i><b>1.8.4</b> Time-to-Event Outcomes and Risk-Based Estimands</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#step-3-link-to-observed-data"><i class="fa fa-check"></i><b>1.9</b> Step 3: Link to observed data</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#observed-data-censoring-rules"><i class="fa fa-check"></i><b>1.9.1</b> Observed-Data Censoring Rules</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#step-4-assess-identifiablity"><i class="fa fa-check"></i><b>1.10</b> Step 4: Assess Identifiablity</a></li>
<li class="chapter" data-level="1.11" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#step-5-choose-and-apply-the-estimator"><i class="fa fa-check"></i><b>1.11</b> Step 5: Choose and apply the estimator</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#simple-substitution-estimator-g-computation"><i class="fa fa-check"></i><b>1.11.1</b> Simple substitution estimator (g-computation)</a></li>
<li class="chapter" data-level="1.11.2" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#iptw--inverse-probability-of-treatment-weighting-estimator"><i class="fa fa-check"></i><b>1.11.2</b> IPTW- Inverse Probability of Treatment Weighting estimator</a></li>
<li class="chapter" data-level="1.11.3" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#tmle--targeted-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>1.11.3</b> TMLE- targeted maximum likelihood estimation</a></li>
<li class="chapter" data-level="1.11.4" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#estimator-properties"><i class="fa fa-check"></i><b>1.11.4</b> Estimator Properties</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#step-6-statistical-uncertainty"><i class="fa fa-check"></i><b>1.12</b> Step 6: Statistical Uncertainty</a></li>
<li class="chapter" data-level="1.13" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#step-7-interpret-findings"><i class="fa fa-check"></i><b>1.13</b> Step 7: Interpret findings</a></li>
<li class="chapter" data-level="1.14" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#summary-and-discussion"><i class="fa fa-check"></i><b>1.14</b> Summary and Discussion</a></li>
<li class="chapter" data-level="1.15" data-path="tutorial-on-targeted-learning-and-the-causal-roadmap.html"><a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html#caution-use-your-tools-well."><i class="fa fa-check"></i><b>1.15</b> Caution: Use your tools well.</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><i class="fa fa-check"></i><b>2</b> Chapter 2.4: SuperLearner and Machine Learning for Causal Inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#why-use-superlearner-in-causal-inference"><i class="fa fa-check"></i><b>2.1</b> 1. Why Use SuperLearner in Causal Inference?</a></li>
<li class="chapter" data-level="2.2" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#conceptual-overview-of-superlearner"><i class="fa fa-check"></i><b>2.2</b> 2. Conceptual Overview of SuperLearner</a></li>
<li class="chapter" data-level="2.3" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#a-minimal-working-example"><i class="fa fa-check"></i><b>2.3</b> 3. A Minimal Working Example</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#simulated-data"><i class="fa fa-check"></i><b>2.3.1</b> 3.1 Simulated data</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#using-the-superlearner-package"><i class="fa fa-check"></i><b>2.4</b> 4. Using the <code>SuperLearner</code> Package</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#basic-call"><i class="fa fa-check"></i><b>2.4.1</b> 4.1 Basic call</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#choosing-a-loss-function-mse-vs-log-likelihood-vs-auc"><i class="fa fa-check"></i><b>2.5</b> 5. Choosing a Loss Function: MSE vs Log-Likelihood vs AUC</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#mean-squared-error-mse"><i class="fa fa-check"></i><b>2.5.1</b> 5.1 Mean Squared Error (MSE)</a></li>
<li class="chapter" data-level="2.5.2" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#negative-log-likelihood-binomial-deviance"><i class="fa fa-check"></i><b>2.5.2</b> 5.2 Negative Log-Likelihood (Binomial deviance)</a></li>
<li class="chapter" data-level="2.5.3" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#auc-rank-loss"><i class="fa fa-check"></i><b>2.5.3</b> 5.3 AUC / Rank Loss</a></li>
<li class="chapter" data-level="2.5.4" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#which-loss-should-i-choose"><i class="fa fa-check"></i><b>2.5.4</b> 5.4 Which loss should I choose?</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#interpreting-cv-risk-and-coefficient-weights"><i class="fa fa-check"></i><b>2.6</b> 6. Interpreting CV-Risk and Coefficient Weights</a></li>
<li class="chapter" data-level="2.7" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#customizing-the-library-and-tuning-learners"><i class="fa fa-check"></i><b>2.7</b> 7. Customizing the Library and Tuning Learners</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#adding-tuned-versions-of-a-learner-e.g.-random-forest"><i class="fa fa-check"></i><b>2.7.1</b> 7.1 Adding tuned versions of a learner (e.g., Random Forest)</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cross-validated-superlearner-cv.superlearner"><i class="fa fa-check"></i><b>2.8</b> 8. Cross-Validated SuperLearner (<code>CV.SuperLearner</code>)</a></li>
<li class="chapter" data-level="2.9" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#integrating-superlearner-into-causal-inference"><i class="fa fa-check"></i><b>2.9</b> 9. Integrating SuperLearner into Causal Inference</a></li>
<li class="chapter" data-level="2.10" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#practical-tips-for-using-superlearner"><i class="fa fa-check"></i><b>2.10</b> 10. Practical Tips for Using SuperLearner</a></li>
<li class="chapter" data-level="2.11" data-path="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#summary"><i class="fa fa-check"></i><b>2.11</b> 11. Summary</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Chapter 2.4: SuperLearner and Machine Learning for Causal Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter-2.4-superlearner-and-machine-learning-for-causal-inference" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Chapter 2.4: SuperLearner and Machine Learning for Causal Inference<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#chapter-2.4-superlearner-and-machine-learning-for-causal-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Flexible prediction to strengthen causal effect estimation</em></p>
<p>Modern causal inference relies on estimating <strong>nuisance functions</strong> (outcome regressions and treatment / censoring mechanisms) that are as accurate as possible. If these models are mis-specified, even sophisticated causal estimators can be biased.</p>
<p>Rather than gambling on a single model (e.g., logistic regression), we can <strong>stack</strong> many candidate learners and let the data decide how to combine them. This is what <strong>SuperLearner</strong> does.</p>
<p>In this chapter you will learn:</p>
<ul>
<li>The intuition behind SuperLearner (SL) and stacking<br />
</li>
<li>How cross-validation is used to avoid overfitting<br />
</li>
<li>How to build and interpret SuperLearner models in R<br />
</li>
<li>When and why to choose different <strong>loss functions</strong> (MSE, log-likelihood, AUC)<br />
</li>
<li>How to customize SL libraries and tune algorithms<br />
</li>
<li>How SL integrates with TMLE and other causal estimators</li>
</ul>
<p>This chapter leans heavily on the excellent visual tutorial by Katherine Hoffman and the SuperLearner demo by David Benkeser (both provided as PDFs), and recasts them in a causal-inference focused Quarto format.</p>
<hr />
<div id="why-use-superlearner-in-causal-inference" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> 1. Why Use SuperLearner in Causal Inference?<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#why-use-superlearner-in-causal-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Causal estimators such as g-computation, IPTW, AIPW, and TMLE rely on estimating:</p>
<ul>
<li><p>The <strong>outcome regression</strong>:<br />
<span class="math inline">\(Q(W, A) = E[Y \mid W, A]\)</span></p></li>
<li><p>The <strong>treatment (or censoring) mechanism</strong>:<br />
<span class="math inline">\(g(W) = P(A = 1 \mid W)\)</span></p></li>
</ul>
<p>In traditional practice, both are often modeled with simple GLMs. This is dangerous when:</p>
<ul>
<li>Relationships are nonlinear</li>
<li>Interactions are present</li>
<li>There are many covariates</li>
<li>We are unsure about which variables to include or in what functional form</li>
</ul>
<p>SuperLearner helps by:</p>
<ul>
<li>Combining multiple algorithms (GLM, random forests, LASSO, boosted trees, etc.)</li>
<li>Using <strong>K-fold cross-validation</strong> to evaluate and weight each algorithm</li>
<li>Producing an ensemble predictor with theoretical guarantees (an “oracle inequality”): asymptotically, SL performs nearly as well as the best algorithm in the library</li>
</ul>
<p>In causal inference, we rarely care about prediction for its own sake, but good prediction of nuisance functions leads to <strong>better causal effect estimation</strong>.</p>
<hr />
</div>
<div id="conceptual-overview-of-superlearner" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> 2. Conceptual Overview of SuperLearner<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#conceptual-overview-of-superlearner" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>At a high level, SuperLearner does the following:</p>
<ol style="list-style-type: decimal">
<li>Pick a <strong>set of candidate learners</strong> (the library).</li>
<li>Split the data into <strong>K folds</strong>.</li>
<li>For each learner:
<ul>
<li>Fit on K-1 folds (training data),</li>
<li>Predict on the held-out fold (validation data).</li>
</ul></li>
<li>Collect <strong>cross-validated predictions</strong> for every observation and every learner.</li>
<li>Fit a <strong>metalearner</strong> (often a linear regression) that finds the optimal weighted combination of the learners’ predictions to minimize a chosen <strong>loss function</strong> (e.g., mean squared error, negative log-likelihood).</li>
<li>Refit each base learner on the full dataset.</li>
<li>Use the metalearner and the refit base learners to form the final ensemble and obtain predictions for new data.</li>
</ol>
<p>This is exactly the workflow illustrated in the “VISUAL GUIDE TO SUPERLEARNING” figure in the KHstats tutorial.</p>
<hr />
</div>
<div id="a-minimal-working-example" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> 3. A Minimal Working Example<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#a-minimal-working-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’ll start with a simple prediction problem, then connect it back to causal inference later.</p>
<div id="simulated-data" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> 3.1 Simulated data<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#simulated-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb67-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-2" tabindex="-1"></a><span class="fu">library</span>(SuperLearner)</span>
<span id="cb67-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-3" tabindex="-1"></a></span>
<span id="cb67-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb67-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb67-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-6" tabindex="-1"></a></span>
<span id="cb67-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-7" tabindex="-1"></a>obs <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb67-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-8" tabindex="-1"></a>  <span class="at">id =</span> <span class="dv">1</span><span class="sc">:</span>n,</span>
<span id="cb67-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-9" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">rnorm</span>(n),</span>
<span id="cb67-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-10" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">rbinom</span>(n, <span class="dv">1</span>, <span class="fu">plogis</span>(<span class="dv">10</span> <span class="sc">*</span> x1)),</span>
<span id="cb67-11"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-11" tabindex="-1"></a>  <span class="at">x3 =</span> <span class="fu">rbinom</span>(n, <span class="dv">1</span>, <span class="fu">plogis</span>(x1 <span class="sc">*</span> x2 <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> x2)),</span>
<span id="cb67-12"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-12" tabindex="-1"></a>  <span class="at">x4 =</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> x1 <span class="sc">*</span> x2, <span class="at">sd =</span> <span class="fl">0.5</span> <span class="sc">*</span> x3),</span>
<span id="cb67-13"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-13" tabindex="-1"></a>  <span class="at">y  =</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x2 <span class="sc">*</span> x3 <span class="sc">+</span> <span class="fu">sin</span>(x4) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.2</span>)</span>
<span id="cb67-14"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-14" tabindex="-1"></a>)</span>
<span id="cb67-15"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-15" tabindex="-1"></a></span>
<span id="cb67-16"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb67-16" tabindex="-1"></a><span class="fu">glimpse</span>(obs)</span></code></pre></div>
<pre><code>## Rows: 2,000
## Columns: 6
## $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, …
## $ x1 &lt;dbl&gt; 2.287247161, -1.196771682, -0.694292510, -0.412292951, -0.970673341…
## $ x2 &lt;int&gt; 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1…
## $ x3 &lt;int&gt; 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1…
## $ x4 &lt;dbl&gt; 1.50283924, 0.04260947, 0.00000000, 0.00000000, 0.00000000, 1.07555…
## $ y  &lt;dbl&gt; 5.03669124, -1.13306036, -0.38284042, -0.28671230, -0.87539307, -0.…</code></pre>
<p>The outcome <code>y</code> is a nonlinear function of the covariates, with interactions and a sine term. GLMs will struggle here.</p>
<hr />
</div>
</div>
<div id="using-the-superlearner-package" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> 4. Using the <code>SuperLearner</code> Package<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#using-the-superlearner-package" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="basic-call" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> 4.1 Basic call<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#basic-call" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’ll start with a small library for illustration.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb69-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-2" tabindex="-1"></a></span>
<span id="cb69-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-3" tabindex="-1"></a>X <span class="ot">&lt;-</span> obs <span class="sc">%&gt;%</span> <span class="fu">select</span>(x1<span class="sc">:</span>x4) <span class="sc">%&gt;%</span> <span class="fu">as.data.frame</span>()</span>
<span id="cb69-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-4" tabindex="-1"></a>Y <span class="ot">&lt;-</span> obs<span class="sc">$</span>y</span>
<span id="cb69-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-5" tabindex="-1"></a></span>
<span id="cb69-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-6" tabindex="-1"></a>SL.lib <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;SL.glm&quot;</span>,      <span class="co"># simple GLM</span></span>
<span id="cb69-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-7" tabindex="-1"></a>            <span class="st">&quot;SL.mean&quot;</span>,     <span class="co"># intercept-only</span></span>
<span id="cb69-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-8" tabindex="-1"></a>            <span class="st">&quot;SL.earth&quot;</span>,    <span class="co"># multivariate adaptive regression splines (MARS)</span></span>
<span id="cb69-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-9" tabindex="-1"></a>            <span class="st">&quot;SL.ranger&quot;</span>)   <span class="co"># random forest</span></span>
<span id="cb69-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-10" tabindex="-1"></a></span>
<span id="cb69-11"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-11" tabindex="-1"></a>sl_fit <span class="ot">&lt;-</span> <span class="fu">SuperLearner</span>(</span>
<span id="cb69-12"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-12" tabindex="-1"></a>  <span class="at">Y =</span> Y,</span>
<span id="cb69-13"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-13" tabindex="-1"></a>  <span class="at">X =</span> X,</span>
<span id="cb69-14"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-14" tabindex="-1"></a>  <span class="at">newX =</span> <span class="cn">NULL</span>,</span>
<span id="cb69-15"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-15" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">gaussian</span>(),</span>
<span id="cb69-16"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-16" tabindex="-1"></a>  <span class="at">SL.library =</span> SL.lib,</span>
<span id="cb69-17"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-17" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;method.NNLS&quot;</span>,  <span class="co"># non-negative least squares metalearner</span></span>
<span id="cb69-18"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-18" tabindex="-1"></a>  <span class="at">cvControl =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">10</span><span class="dt">L</span>)</span>
<span id="cb69-19"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb69-19" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Loading required namespace: earth</code></pre>
<pre><code>## Loading required namespace: ranger</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb72-1" tabindex="-1"></a>sl_fit</span></code></pre></div>
<pre><code>## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = NULL, family = gaussian(), SL.library = SL.lib,  
##     method = &quot;method.NNLS&quot;, cvControl = list(V = 10L)) 
## 
## 
##                     Risk      Coef
## SL.glm_All    0.15038334 0.0000000
## SL.mean_All   4.52716718 0.0000000
## SL.earth_All  0.04408952 0.8593302
## SL.ranger_All 0.05735981 0.1406698</code></pre>
<p>Key outputs:</p>
<ul>
<li><code>Risk</code>: cross-validated risk (e.g., MSE) for each learner</li>
<li><code>Coef</code>: weight given to each learner in the ensemble</li>
</ul>
<p>The learner with the smallest CV-risk often gets the largest weight, but SL can combine learners.</p>
<p>We can access the <strong>ensemble predictions</strong>:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb74-1" tabindex="-1"></a><span class="fu">head</span>(sl_fit<span class="sc">$</span>SL.predict)</span></code></pre></div>
<pre><code>##          [,1]
## 1  5.36960217
## 2 -1.15651994
## 3 -0.66557910
## 4 -0.39653932
## 5 -0.94879355
## 6 -0.04451723</code></pre>
<p>and predictions from individual learners:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb76-1" tabindex="-1"></a><span class="fu">head</span>(sl_fit<span class="sc">$</span>library.predict)</span></code></pre></div>
<pre><code>##   SL.glm_All SL.mean_All SL.earth_All SL.ranger_All
## 1  4.9119003    1.145784   5.44169210     4.9292158
## 2 -0.9619611    1.145784  -1.14585410    -1.2216759
## 3 -0.8907976    1.145784  -0.67326080    -0.6186528
## 4 -0.6300543    1.145784  -0.40211488    -0.3624791
## 5 -1.1463457    1.145784  -0.94947428    -0.9446351
## 6 -0.1673960    1.145784  -0.01723667    -0.2111701</code></pre>
<hr />
</div>
</div>
<div id="choosing-a-loss-function-mse-vs-log-likelihood-vs-auc" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> 5. Choosing a Loss Function: MSE vs Log-Likelihood vs AUC<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#choosing-a-loss-function-mse-vs-log-likelihood-vs-auc" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SuperLearner allows different <strong>loss functions</strong>, which define what we mean by “best” prediction.</p>
<div id="mean-squared-error-mse" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> 5.1 Mean Squared Error (MSE)<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#mean-squared-error-mse" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Default for <code>family = gaussian()</code></li>
<li>Appropriate for <strong>continuous outcomes</strong> when we care about squared error:
<span class="math display">\[ L(y, \hat{y}) = (y - \hat{y})^2 \]</span></li>
<li>Good when we want well-calibrated mean predictions</li>
</ul>
<p>Example (already used above): <code>method = "method.NNLS"</code> with <code>family = gaussian()</code></p>
</div>
<div id="negative-log-likelihood-binomial-deviance" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> 5.2 Negative Log-Likelihood (Binomial deviance)<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#negative-log-likelihood-binomial-deviance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Natural choice for <strong>binary outcomes</strong> when we care about probability calibration:
<span class="math display">\[ L(y, \hat{p}) = -[y \log(\hat{p}) + (1-y) \log(1-\hat{p})] \]</span></li>
<li>Strongly penalizes confident but wrong predictions</li>
<li>Recommended for:
<ul>
<li>Outcome models (<code>Y</code> binary)</li>
<li>Treatment models (<code>A</code> binary) in causal inference</li>
</ul></li>
</ul>
<p>Use <code>method = "method.NNloglik"</code> with <code>family = binomial()</code>.</p>
<p>Example:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb78-1" tabindex="-1"></a><span class="co"># suppose Y is binary</span></span>
<span id="cb78-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb78-2" tabindex="-1"></a>Y_bin <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n, <span class="dv">1</span>, <span class="fu">plogis</span>(X<span class="sc">$</span>x1 <span class="sc">+</span> X<span class="sc">$</span>x2))</span>
<span id="cb78-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb78-3" tabindex="-1"></a></span>
<span id="cb78-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb78-4" tabindex="-1"></a>sl_loglik <span class="ot">&lt;-</span> <span class="fu">SuperLearner</span>(</span>
<span id="cb78-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb78-5" tabindex="-1"></a>  <span class="at">Y =</span> Y_bin,</span>
<span id="cb78-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb78-6" tabindex="-1"></a>  <span class="at">X =</span> X,</span>
<span id="cb78-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb78-7" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">binomial</span>(),</span>
<span id="cb78-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb78-8" tabindex="-1"></a>  <span class="at">SL.library =</span> <span class="fu">c</span>(<span class="st">&quot;SL.glm&quot;</span>, <span class="st">&quot;SL.mean&quot;</span>, <span class="st">&quot;SL.ranger&quot;</span>),</span>
<span id="cb78-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb78-9" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;method.NNloglik&quot;</span></span>
<span id="cb78-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb78-10" tabindex="-1"></a>)</span>
<span id="cb78-11"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb78-11" tabindex="-1"></a></span>
<span id="cb78-12"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb78-12" tabindex="-1"></a>sl_loglik</span></code></pre></div>
<pre><code>## 
## Call:  
## SuperLearner(Y = Y_bin, X = X, family = binomial(), SL.library = c(&quot;SL.glm&quot;,  
##     &quot;SL.mean&quot;, &quot;SL.ranger&quot;), method = &quot;method.NNloglik&quot;) 
## 
## 
##                    Risk      Coef
## SL.glm_All    0.5286216 0.8914129
## SL.mean_All   0.6818619 0.0000000
## SL.ranger_All 0.5537130 0.1085871</code></pre>
</div>
<div id="auc-rank-loss" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> 5.3 AUC / Rank Loss<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#auc-rank-loss" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>For classification problems where the <strong>ranking</strong> of probabilities matters more than calibration</li>
<li>Common when choosing a threshold later (e.g., risk stratification)</li>
<li>SuperLearner implementation: <code>method = "method.AUC"</code></li>
<li>Particularly useful when interested in discriminatory ability (e.g., disease risk scores)</li>
</ul>
<p>Example:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb80-1" tabindex="-1"></a><span class="fu">library</span>(cvAUC)   <span class="co"># required by method.AUC</span></span>
<span id="cb80-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb80-2" tabindex="-1"></a></span>
<span id="cb80-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb80-3" tabindex="-1"></a>sl_auc <span class="ot">&lt;-</span> <span class="fu">SuperLearner</span>(</span>
<span id="cb80-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb80-4" tabindex="-1"></a>  <span class="at">Y =</span> Y_bin,</span>
<span id="cb80-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb80-5" tabindex="-1"></a>  <span class="at">X =</span> X,</span>
<span id="cb80-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb80-6" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">binomial</span>(),</span>
<span id="cb80-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb80-7" tabindex="-1"></a>  <span class="at">SL.library =</span> <span class="fu">c</span>(<span class="st">&quot;SL.glm&quot;</span>, <span class="st">&quot;SL.mean&quot;</span>, <span class="st">&quot;SL.ranger&quot;</span>),</span>
<span id="cb80-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb80-8" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;method.AUC&quot;</span></span>
<span id="cb80-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb80-9" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Warning in method$computeCoef(Z = Z, Y = Y, libraryNames = libraryNames, :
## optim didn&#39;t converge when estimating the super learner coefficients, reason
## (see ?optim): 52 optim message: ERROR: ABNORMAL_TERMINATION_IN_LNSRCH</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb82-1" tabindex="-1"></a>sl_auc</span></code></pre></div>
<pre><code>## 
## Call:  
## SuperLearner(Y = Y_bin, X = X, family = binomial(), SL.library = c(&quot;SL.glm&quot;,  
##     &quot;SL.mean&quot;, &quot;SL.ranger&quot;), method = &quot;method.AUC&quot;) 
## 
## 
##                    Risk         Coef
## SL.glm_All    0.1938743 0.9988292700
## SL.mean_All   0.5268196 0.0006672332
## SL.ranger_All 0.2104779 0.0005034968</code></pre>
</div>
<div id="which-loss-should-i-choose" class="section level3 hasAnchor" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> 5.4 Which loss should I choose?<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#which-loss-should-i-choose" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>For outcome models in TMLE/AIPW:</strong>
<ul>
<li>If binary → log-likelihood (binomial deviance)<br />
</li>
<li>If continuous → MSE or other appropriate distribution-based loss<br />
</li>
</ul></li>
<li><strong>For treatment / censoring models in causal inference:</strong>
<ul>
<li>Typically log-likelihood (because we want accurate estimates of <code>P(A | W)</code>)</li>
</ul></li>
<li><strong>For pure classification (no causal estimation):</strong>
<ul>
<li>Consider AUC loss (<code>method.AUC</code>) if ranking is the priority</li>
</ul></li>
</ul>
<hr />
</div>
</div>
<div id="interpreting-cv-risk-and-coefficient-weights" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> 6. Interpreting CV-Risk and Coefficient Weights<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#interpreting-cv-risk-and-coefficient-weights" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb84-1" tabindex="-1"></a>sl_fit<span class="sc">$</span>cvRisk</span></code></pre></div>
<pre><code>##    SL.glm_All   SL.mean_All  SL.earth_All SL.ranger_All 
##    0.15038334    4.52716718    0.04408952    0.05735981</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb86-1" tabindex="-1"></a>sl_fit<span class="sc">$</span>coef</span></code></pre></div>
<pre><code>##    SL.glm_All   SL.mean_All  SL.earth_All SL.ranger_All 
##     0.0000000     0.0000000     0.8593302     0.1406698</code></pre>
<ul>
<li><code>cvRisk</code> shows cross-validated risk for each algorithm</li>
<li><code>coef</code> gives the ensemble weights (metalearner solution)</li>
</ul>
<p>An algorithm might have:</p>
<ul>
<li>Low risk → high weight<br />
</li>
<li>High risk → weight near zero (effectively excluded)</li>
</ul>
<p>This matches the demonstration in the Benkeser notes where GLM dominates mean-only models when predicting MI.</p>
<hr />
</div>
<div id="customizing-the-library-and-tuning-learners" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> 7. Customizing the Library and Tuning Learners<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#customizing-the-library-and-tuning-learners" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="adding-tuned-versions-of-a-learner-e.g.-random-forest" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> 7.1 Adding tuned versions of a learner (e.g., Random Forest)<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#adding-tuned-versions-of-a-learner-e.g.-random-forest" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can define bounded random forest variants with different hyperparameters.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb88-1" tabindex="-1"></a>SL.ranger_mtry3 <span class="ot">&lt;-</span> <span class="cf">function</span>(..., <span class="at">mtry =</span> <span class="dv">3</span>){</span>
<span id="cb88-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb88-2" tabindex="-1"></a>  <span class="fu">SL.ranger</span>(..., <span class="at">mtry =</span> mtry)</span>
<span id="cb88-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb88-3" tabindex="-1"></a>}</span>
<span id="cb88-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb88-4" tabindex="-1"></a></span>
<span id="cb88-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb88-5" tabindex="-1"></a>SL.ranger_mtry4 <span class="ot">&lt;-</span> <span class="cf">function</span>(..., <span class="at">mtry =</span> <span class="dv">4</span>){</span>
<span id="cb88-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb88-6" tabindex="-1"></a>  <span class="fu">SL.ranger</span>(..., <span class="at">mtry =</span> mtry)</span>
<span id="cb88-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb88-7" tabindex="-1"></a>}</span>
<span id="cb88-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb88-8" tabindex="-1"></a></span>
<span id="cb88-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb88-9" tabindex="-1"></a>SL.lib_tuned <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;SL.glm&quot;</span>,</span>
<span id="cb88-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb88-10" tabindex="-1"></a>                  <span class="st">&quot;SL.earth&quot;</span>,</span>
<span id="cb88-11"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb88-11" tabindex="-1"></a>                  <span class="st">&quot;SL.ranger_mtry3&quot;</span>,</span>
<span id="cb88-12"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb88-12" tabindex="-1"></a>                  <span class="st">&quot;SL.ranger_mtry4&quot;</span>)</span></code></pre></div>
<p>Then run:</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb89-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb89-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb89-2" tabindex="-1"></a>sl_tuned <span class="ot">&lt;-</span> <span class="fu">SuperLearner</span>(</span>
<span id="cb89-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb89-3" tabindex="-1"></a>  <span class="at">Y =</span> Y,</span>
<span id="cb89-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb89-4" tabindex="-1"></a>  <span class="at">X =</span> X,</span>
<span id="cb89-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb89-5" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">gaussian</span>(),</span>
<span id="cb89-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb89-6" tabindex="-1"></a>  <span class="at">SL.library =</span> SL.lib_tuned,</span>
<span id="cb89-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb89-7" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;method.NNLS&quot;</span></span>
<span id="cb89-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb89-8" tabindex="-1"></a>)</span>
<span id="cb89-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb89-9" tabindex="-1"></a></span>
<span id="cb89-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb89-10" tabindex="-1"></a>sl_tuned<span class="sc">$</span>cvRisk</span></code></pre></div>
<pre><code>##          SL.glm_All        SL.earth_All SL.ranger_mtry3_All SL.ranger_mtry4_All 
##          0.15062184          0.04401740          0.05044671          0.05242569</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb91-1" tabindex="-1"></a>sl_tuned<span class="sc">$</span>coef</span></code></pre></div>
<pre><code>##          SL.glm_All        SL.earth_All SL.ranger_mtry3_All SL.ranger_mtry4_All 
##           0.0000000           0.7318382           0.2681618           0.0000000</code></pre>
<p>SuperLearner automatically picks which tuned version (or combination) works best.</p>
<hr />
</div>
</div>
<div id="cross-validated-superlearner-cv.superlearner" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> 8. Cross-Validated SuperLearner (<code>CV.SuperLearner</code>)<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cross-validated-superlearner-cv.superlearner" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><code>CV.SuperLearner</code> adds an <strong>outer layer</strong> of cross-validation to evaluate SL versus its components objectively.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb93-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb93-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb93-2" tabindex="-1"></a>cv_sl <span class="ot">&lt;-</span> <span class="fu">CV.SuperLearner</span>(</span>
<span id="cb93-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb93-3" tabindex="-1"></a>  <span class="at">Y =</span> Y,</span>
<span id="cb93-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb93-4" tabindex="-1"></a>  <span class="at">X =</span> X,</span>
<span id="cb93-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb93-5" tabindex="-1"></a>  <span class="at">V =</span> <span class="dv">5</span>,</span>
<span id="cb93-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb93-6" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">gaussian</span>(),</span>
<span id="cb93-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb93-7" tabindex="-1"></a>  <span class="at">SL.library =</span> SL.lib</span>
<span id="cb93-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb93-8" tabindex="-1"></a>)</span>
<span id="cb93-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb93-9" tabindex="-1"></a></span>
<span id="cb93-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb93-10" tabindex="-1"></a>cv_sl</span></code></pre></div>
<pre><code>## 
## Call:  
## CV.SuperLearner(Y = Y, X = X, V = 5, family = gaussian(), SL.library = SL.lib) 
## 
## 
## 
## Cross-validated predictions from the SuperLearner:  SL.predict 
## 
## Cross-validated predictions from the discrete super learner (cross-validation selector):  discreteSL.predict 
## 
## Which library algorithm was the discrete super learner:  whichDiscreteSL 
## 
## Cross-validated prediction for all algorithms in the library:  library.predict</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb95-1" tabindex="-1"></a><span class="fu">plot</span>(cv_sl)</span></code></pre></div>
<p><img src="causal-learning-handbook_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>The plot shows:</p>
<ul>
<li>Cross-validated risks and confidence intervals for each learner</li>
<li>Performance of the discrete and continuous SuperLearner</li>
</ul>
<p>This step is particularly helpful when you want to justify using SL rather than a single, simpler algorithm.</p>
<hr />
</div>
<div id="integrating-superlearner-into-causal-inference" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> 9. Integrating SuperLearner into Causal Inference<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#integrating-superlearner-into-causal-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far we focused on prediction. How does this relate to causal inference?</p>
<p>For a point-treatment ATE, a TMLE analysis might look like:</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb96-1" tabindex="-1"></a><span class="co"># Example skeleton (you will flesh this out later with your own data)</span></span>
<span id="cb96-2"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb96-2" tabindex="-1"></a><span class="fu">library</span>(tmle)</span>
<span id="cb96-3"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb96-3" tabindex="-1"></a></span>
<span id="cb96-4"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb96-4" tabindex="-1"></a>tmle_fit <span class="ot">&lt;-</span> <span class="fu">tmle</span>(</span>
<span id="cb96-5"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb96-5" tabindex="-1"></a>  <span class="at">Y =</span> Y_bin,          <span class="co"># binary outcome</span></span>
<span id="cb96-6"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb96-6" tabindex="-1"></a>  <span class="at">A =</span> A,              <span class="co"># treatment</span></span>
<span id="cb96-7"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb96-7" tabindex="-1"></a>  <span class="at">W =</span> X,              <span class="co"># covariates</span></span>
<span id="cb96-8"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb96-8" tabindex="-1"></a>  <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>,</span>
<span id="cb96-9"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb96-9" tabindex="-1"></a>  <span class="at">Q.SL.library =</span> <span class="fu">c</span>(<span class="st">&quot;SL.glm&quot;</span>, <span class="st">&quot;SL.ranger&quot;</span>, <span class="st">&quot;SL.earth&quot;</span>),</span>
<span id="cb96-10"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb96-10" tabindex="-1"></a>  <span class="at">g.SL.library =</span> <span class="fu">c</span>(<span class="st">&quot;SL.glm&quot;</span>, <span class="st">&quot;SL.ranger&quot;</span>, <span class="st">&quot;SL.mean&quot;</span>)</span>
<span id="cb96-11"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb96-11" tabindex="-1"></a>)</span>
<span id="cb96-12"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb96-12" tabindex="-1"></a></span>
<span id="cb96-13"><a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#cb96-13" tabindex="-1"></a>tmle_fit<span class="sc">$</span>estimates<span class="sc">$</span>ATE</span></code></pre></div>
<p>Here:</p>
<ul>
<li><code>Q.SL.library</code> is used to estimate outcome regression <code>E[Y|A,W]</code></li>
<li><code>g.SL.library</code> is used to estimate propensity scores <code>P(A|W)</code></li>
<li>TMLE combines these with targeting to produce an efficient, doubly robust estimate</li>
</ul>
<p>Key advantages:</p>
<ul>
<li>You no longer need to guess the “right” model for Y or A<br />
</li>
<li>You can include many flexible learners without overfitting (thanks to SL + CV)<br />
</li>
<li>Your causal inference relies less on arbitrary parametric modeling choices</li>
</ul>
<hr />
</div>
<div id="practical-tips-for-using-superlearner" class="section level2 hasAnchor" number="2.10">
<h2><span class="header-section-number">2.10</span> 10. Practical Tips for Using SuperLearner<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#practical-tips-for-using-superlearner" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><strong>Start with a modest but diverse library</strong>
<ul>
<li>GLM (<code>SL.glm</code>)<br />
</li>
<li>Random forest (<code>SL.ranger</code> or <code>SL.randomForest</code>)<br />
</li>
<li>MARS (<code>SL.earth</code>)<br />
</li>
<li>Penalized regression (<code>SL.glmnet</code>)</li>
</ul></li>
<li><strong>Pick loss functions that match your problem</strong>
<ul>
<li>Binary → log-likelihood (for calibration) or AUC (for ranking)<br />
</li>
<li>Continuous → MSE</li>
</ul></li>
<li><strong>Watch computation time</strong>
<ul>
<li>SL is more expensive than a single GLM, especially with many learners and CV folds.</li>
</ul></li>
<li><strong>Use SL primarily on nuisance functions</strong>
<ul>
<li>Don’t use SL to directly estimate the causal effect; instead, use SL to estimate <code>Q</code> and <code>g</code> and feed these into TMLE, AIPW, etc.</li>
</ul></li>
<li><strong>Inspect SL outputs</strong>
<ul>
<li>Which learners are getting weight?<br />
</li>
<li>Are any learners consistently poor performers?<br />
</li>
<li>Do you need to adjust your library?</li>
</ul></li>
</ol>
<hr />
</div>
<div id="summary" class="section level2 hasAnchor" number="2.11">
<h2><span class="header-section-number">2.11</span> 11. Summary<a href="chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter you learned</p>
<ul>
<li>How SuperLearner combines multiple algorithms using cross-validation and a metalearner<br />
</li>
<li>The role of loss functions (MSE, log-likelihood, AUC) and when to choose each<br />
</li>
<li>How to implement SuperLearner in R, inspect CV-risk and weights, and customize the library<br />
</li>
<li>How SuperLearner supports reliable causal inference by improving nuisance function estimation and integrating seamlessly with TMLE and other estimators</li>
</ul>
<p>Next, we move to <strong>Module 3</strong>, where we tackle longitudinal data, dynamic treatment regimes, and modified treatment policies, often using SuperLearner as a core building block.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tutorial-on-targeted-learning-and-the-causal-roadmap.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
