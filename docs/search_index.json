[["index.html", "Chapter 2.4: SuperLearner and Machine Learning for Causal Inference Chapter 1 Welcome!", " Chapter 2.4: SuperLearner and Machine Learning for Causal Inference Chapter 1 Welcome! This site introduces the Causal Roadmap, Targeted Maximum Likelihood Estimation (TMLE), and Super Learner for modern causal inference in epidemiology, with an emphasis on pharmacoepidemiology and clinical trial analysis. "],["chapter-2.4-superlearner-and-machine-learning-for-causal-inference.html", "Chapter 2 Chapter 2.4: SuperLearner and Machine Learning for Causal Inference 2.1 1. Why Use SuperLearner in Causal Inference? 2.2 2. Conceptual Overview of SuperLearner 2.3 3. A Minimal Working Example 2.4 4. Using the SuperLearner Package 2.5 5. Choosing a Loss Function: MSE vs Log-Likelihood vs AUC 2.6 6. Interpreting CV-Risk and Coefficient Weights 2.7 7. Customizing the Library and Tuning Learners 2.8 8. Cross-Validated SuperLearner (CV.SuperLearner) 2.9 9. Integrating SuperLearner into Causal Inference 2.10 10. Practical Tips for Using SuperLearner 2.11 11. Summary", " Chapter 2 Chapter 2.4: SuperLearner and Machine Learning for Causal Inference Flexible prediction to strengthen causal effect estimation Modern causal inference relies on estimating nuisance functions (outcome regressions and treatment / censoring mechanisms) that are as accurate as possible. If these models are mis-specified, even sophisticated causal estimators can be biased. Rather than gambling on a single model (e.g., logistic regression), we can stack many candidate learners and let the data decide how to combine them. This is what SuperLearner does. In this chapter you will learn: The intuition behind SuperLearner (SL) and stacking How cross-validation is used to avoid overfitting How to build and interpret SuperLearner models in R When and why to choose different loss functions (MSE, log-likelihood, AUC) How to customize SL libraries and tune algorithms How SL integrates with TMLE and other causal estimators This chapter leans heavily on the excellent visual tutorial by Katherine Hoffman and the SuperLearner demo by David Benkeser (both provided as PDFs), and recasts them in a causal-inference focused Quarto format. 2.1 1. Why Use SuperLearner in Causal Inference? Causal estimators such as g-computation, IPTW, AIPW, and TMLE rely on estimating: The outcome regression: \\(Q(W, A) = E[Y \\mid W, A]\\) The treatment (or censoring) mechanism: \\(g(W) = P(A = 1 \\mid W)\\) In traditional practice, both are often modeled with simple GLMs. This is dangerous when: Relationships are nonlinear Interactions are present There are many covariates We are unsure about which variables to include or in what functional form SuperLearner helps by: Combining multiple algorithms (GLM, random forests, LASSO, boosted trees, etc.) Using K-fold cross-validation to evaluate and weight each algorithm Producing an ensemble predictor with theoretical guarantees (an “oracle inequality”): asymptotically, SL performs nearly as well as the best algorithm in the library In causal inference, we rarely care about prediction for its own sake, but good prediction of nuisance functions leads to better causal effect estimation. 2.2 2. Conceptual Overview of SuperLearner At a high level, SuperLearner does the following: Pick a set of candidate learners (the library). Split the data into K folds. For each learner: Fit on K-1 folds (training data), Predict on the held-out fold (validation data). Collect cross-validated predictions for every observation and every learner. Fit a metalearner (often a linear regression) that finds the optimal weighted combination of the learners’ predictions to minimize a chosen loss function (e.g., mean squared error, negative log-likelihood). Refit each base learner on the full dataset. Use the metalearner and the refit base learners to form the final ensemble and obtain predictions for new data. This is exactly the workflow illustrated in the “VISUAL GUIDE TO SUPERLEARNING” figure in the KHstats tutorial. 2.3 3. A Minimal Working Example We’ll start with a simple prediction problem, then connect it back to causal inference later. 2.3.1 3.1 Simulated data library(tidyverse) library(SuperLearner) set.seed(7) n &lt;- 2000 obs &lt;- tibble( id = 1:n, x1 = rnorm(n), x2 = rbinom(n, 1, plogis(10 * x1)), x3 = rbinom(n, 1, plogis(x1 * x2 + 0.5 * x2)), x4 = rnorm(n, mean = x1 * x2, sd = 0.5 * x3), y = x1 + x2 + x2 * x3 + sin(x4) + rnorm(n, sd = 0.2) ) glimpse(obs) ## Rows: 2,000 ## Columns: 6 ## $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, … ## $ x1 &lt;dbl&gt; 2.287247161, -1.196771682, -0.694292510, -0.412292951, -0.970673341, -0.947279945, 0.748139340, -0.116955226, 0.1526576… ## $ x2 &lt;int&gt; 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,… ## $ x3 &lt;int&gt; 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,… ## $ x4 &lt;dbl&gt; 1.50283924, 0.04260947, 0.00000000, 0.00000000, 0.00000000, 1.07555690, 0.90481789, 0.03292026, 0.15265763, 3.04174063,… ## $ y &lt;dbl&gt; 5.03669124, -1.13306036, -0.38284042, -0.28671230, -0.87539307, -0.23969890, 3.82052213, -0.25644487, 1.43066246, 3.979… The outcome y is a nonlinear function of the covariates, with interactions and a sine term. GLMs will struggle here. 2.4 4. Using the SuperLearner Package 2.4.1 4.1 Basic call We’ll start with a small library for illustration. set.seed(1234) X &lt;- obs %&gt;% select(x1:x4) %&gt;% as.data.frame() Y &lt;- obs$y SL.lib &lt;- c(&quot;SL.glm&quot;, # simple GLM &quot;SL.mean&quot;, # intercept-only &quot;SL.earth&quot;, # multivariate adaptive regression splines (MARS) &quot;SL.ranger&quot;) # random forest sl_fit &lt;- SuperLearner( Y = Y, X = X, newX = NULL, family = gaussian(), SL.library = SL.lib, method = &quot;method.NNLS&quot;, # non-negative least squares metalearner cvControl = list(V = 10L) ) sl_fit ## ## Call: ## SuperLearner(Y = Y, X = X, newX = NULL, family = gaussian(), SL.library = SL.lib, method = &quot;method.NNLS&quot;, cvControl = list(V = 10L)) ## ## ## ## Risk Coef ## SL.glm_All 0.15038334 0.0000000 ## SL.mean_All 4.52716718 0.0000000 ## SL.earth_All 0.04408952 0.8593302 ## SL.ranger_All 0.05735981 0.1406698 Key outputs: Risk: cross-validated risk (e.g., MSE) for each learner Coef: weight given to each learner in the ensemble The learner with the smallest CV-risk often gets the largest weight, but SL can combine learners. We can access the ensemble predictions: head(sl_fit$SL.predict) ## [,1] ## 1 5.36960217 ## 2 -1.15651994 ## 3 -0.66557910 ## 4 -0.39653932 ## 5 -0.94879355 ## 6 -0.04451723 and predictions from individual learners: head(sl_fit$library.predict) ## SL.glm_All SL.mean_All SL.earth_All SL.ranger_All ## 1 4.9119003 1.145784 5.44169210 4.9292158 ## 2 -0.9619611 1.145784 -1.14585410 -1.2216759 ## 3 -0.8907976 1.145784 -0.67326080 -0.6186528 ## 4 -0.6300543 1.145784 -0.40211488 -0.3624791 ## 5 -1.1463457 1.145784 -0.94947428 -0.9446351 ## 6 -0.1673960 1.145784 -0.01723667 -0.2111701 2.5 5. Choosing a Loss Function: MSE vs Log-Likelihood vs AUC SuperLearner allows different loss functions, which define what we mean by “best” prediction. 2.5.1 5.1 Mean Squared Error (MSE) Default for family = gaussian() Appropriate for continuous outcomes when we care about squared error: \\[ L(y, \\hat{y}) = (y - \\hat{y})^2 \\] Good when we want well-calibrated mean predictions Example (already used above): method = \"method.NNLS\" with family = gaussian() 2.5.2 5.2 Negative Log-Likelihood (Binomial deviance) Natural choice for binary outcomes when we care about probability calibration: \\[ L(y, \\hat{p}) = -[y \\log(\\hat{p}) + (1-y) \\log(1-\\hat{p})] \\] Strongly penalizes confident but wrong predictions Recommended for: Outcome models (Y binary) Treatment models (A binary) in causal inference Use method = \"method.NNloglik\" with family = binomial(). Example: # suppose Y is binary Y_bin &lt;- rbinom(n, 1, plogis(X$x1 + X$x2)) sl_loglik &lt;- SuperLearner( Y = Y_bin, X = X, family = binomial(), SL.library = c(&quot;SL.glm&quot;, &quot;SL.mean&quot;, &quot;SL.ranger&quot;), method = &quot;method.NNloglik&quot; ) sl_loglik ## ## Call: ## SuperLearner(Y = Y_bin, X = X, family = binomial(), SL.library = c(&quot;SL.glm&quot;, &quot;SL.mean&quot;, &quot;SL.ranger&quot;), method = &quot;method.NNloglik&quot;) ## ## ## ## Risk Coef ## SL.glm_All 0.5286216 0.8914129 ## SL.mean_All 0.6818619 0.0000000 ## SL.ranger_All 0.5537130 0.1085871 2.5.3 5.3 AUC / Rank Loss For classification problems where the ranking of probabilities matters more than calibration Common when choosing a threshold later (e.g., risk stratification) SuperLearner implementation: method = \"method.AUC\" Particularly useful when interested in discriminatory ability (e.g., disease risk scores) Example: library(cvAUC) # required by method.AUC sl_auc &lt;- SuperLearner( Y = Y_bin, X = X, family = binomial(), SL.library = c(&quot;SL.glm&quot;, &quot;SL.mean&quot;, &quot;SL.ranger&quot;), method = &quot;method.AUC&quot; ) ## Warning in method$computeCoef(Z = Z, Y = Y, libraryNames = libraryNames, : optim didn&#39;t converge when estimating the super learner ## coefficients, reason (see ?optim): 52 optim message: ERROR: ABNORMAL_TERMINATION_IN_LNSRCH sl_auc ## ## Call: ## SuperLearner(Y = Y_bin, X = X, family = binomial(), SL.library = c(&quot;SL.glm&quot;, &quot;SL.mean&quot;, &quot;SL.ranger&quot;), method = &quot;method.AUC&quot;) ## ## ## Risk Coef ## SL.glm_All 0.1938743 0.9988292700 ## SL.mean_All 0.5268196 0.0006672332 ## SL.ranger_All 0.2104779 0.0005034968 2.5.4 5.4 Which loss should I choose? For outcome models in TMLE/AIPW: If binary → log-likelihood (binomial deviance) If continuous → MSE or other appropriate distribution-based loss For treatment / censoring models in causal inference: Typically log-likelihood (because we want accurate estimates of P(A | W)) For pure classification (no causal estimation): Consider AUC loss (method.AUC) if ranking is the priority 2.6 6. Interpreting CV-Risk and Coefficient Weights sl_fit$cvRisk ## SL.glm_All SL.mean_All SL.earth_All SL.ranger_All ## 0.15038334 4.52716718 0.04408952 0.05735981 sl_fit$coef ## SL.glm_All SL.mean_All SL.earth_All SL.ranger_All ## 0.0000000 0.0000000 0.8593302 0.1406698 cvRisk shows cross-validated risk for each algorithm coef gives the ensemble weights (metalearner solution) An algorithm might have: Low risk → high weight High risk → weight near zero (effectively excluded) This matches the demonstration in the Benkeser notes where GLM dominates mean-only models when predicting MI. 2.7 7. Customizing the Library and Tuning Learners 2.7.1 7.1 Adding tuned versions of a learner (e.g., Random Forest) We can define bounded random forest variants with different hyperparameters. SL.ranger_mtry3 &lt;- function(..., mtry = 3){ SL.ranger(..., mtry = mtry) } SL.ranger_mtry4 &lt;- function(..., mtry = 4){ SL.ranger(..., mtry = mtry) } SL.lib_tuned &lt;- c(&quot;SL.glm&quot;, &quot;SL.earth&quot;, &quot;SL.ranger_mtry3&quot;, &quot;SL.ranger_mtry4&quot;) Then run: set.seed(123) sl_tuned &lt;- SuperLearner( Y = Y, X = X, family = gaussian(), SL.library = SL.lib_tuned, method = &quot;method.NNLS&quot; ) sl_tuned$cvRisk ## SL.glm_All SL.earth_All SL.ranger_mtry3_All SL.ranger_mtry4_All ## 0.15062184 0.04401740 0.05044671 0.05242569 sl_tuned$coef ## SL.glm_All SL.earth_All SL.ranger_mtry3_All SL.ranger_mtry4_All ## 0.0000000 0.7318382 0.2681618 0.0000000 SuperLearner automatically picks which tuned version (or combination) works best. 2.8 8. Cross-Validated SuperLearner (CV.SuperLearner) CV.SuperLearner adds an outer layer of cross-validation to evaluate SL versus its components objectively. set.seed(123) cv_sl &lt;- CV.SuperLearner( Y = Y, X = X, V = 5, family = gaussian(), SL.library = SL.lib ) cv_sl ## ## Call: CV.SuperLearner(Y = Y, X = X, V = 5, family = gaussian(), SL.library = SL.lib) ## ## ## Cross-validated predictions from the SuperLearner: SL.predict ## ## Cross-validated predictions from the discrete super learner (cross-validation selector): discreteSL.predict ## ## Which library algorithm was the discrete super learner: whichDiscreteSL ## ## Cross-validated prediction for all algorithms in the library: library.predict plot(cv_sl) The plot shows: Cross-validated risks and confidence intervals for each learner Performance of the discrete and continuous SuperLearner This step is particularly helpful when you want to justify using SL rather than a single, simpler algorithm. 2.9 9. Integrating SuperLearner into Causal Inference So far we focused on prediction. How does this relate to causal inference? For a point-treatment ATE, a TMLE analysis might look like: # Example skeleton (you will flesh this out later with your own data) library(tmle) tmle_fit &lt;- tmle( Y = Y_bin, # binary outcome A = A, # treatment W = X, # covariates family = &quot;binomial&quot;, Q.SL.library = c(&quot;SL.glm&quot;, &quot;SL.ranger&quot;, &quot;SL.earth&quot;), g.SL.library = c(&quot;SL.glm&quot;, &quot;SL.ranger&quot;, &quot;SL.mean&quot;) ) tmle_fit$estimates$ATE Here: Q.SL.library is used to estimate outcome regression E[Y|A,W] g.SL.library is used to estimate propensity scores P(A|W) TMLE combines these with targeting to produce an efficient, doubly robust estimate Key advantages: You no longer need to guess the “right” model for Y or A You can include many flexible learners without overfitting (thanks to SL + CV) Your causal inference relies less on arbitrary parametric modeling choices 2.10 10. Practical Tips for Using SuperLearner Start with a modest but diverse library GLM (SL.glm) Random forest (SL.ranger or SL.randomForest) MARS (SL.earth) Penalized regression (SL.glmnet) Pick loss functions that match your problem Binary → log-likelihood (for calibration) or AUC (for ranking) Continuous → MSE Watch computation time SL is more expensive than a single GLM, especially with many learners and CV folds. Use SL primarily on nuisance functions Don’t use SL to directly estimate the causal effect; instead, use SL to estimate Q and g and feed these into TMLE, AIPW, etc. Inspect SL outputs Which learners are getting weight? Are any learners consistently poor performers? Do you need to adjust your library? 2.11 11. Summary In this chapter you learned How SuperLearner combines multiple algorithms using cross-validation and a metalearner The role of loss functions (MSE, log-likelihood, AUC) and when to choose each How to implement SuperLearner in R, inspect CV-risk and weights, and customize the library How SuperLearner supports reliable causal inference by improving nuisance function estimation and integrating seamlessly with TMLE and other estimators Next, we move to Module 3, where we tackle longitudinal data, dynamic treatment regimes, and modified treatment policies, often using SuperLearner as a core building block. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
