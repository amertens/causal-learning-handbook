{
  "hash": "8b1f159e6ad1ed1d705d2e7ea22f2425",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 1.3: From Causal Questions to Analysis — Identification, Estimands, and Statistical Models\"\nformat: html\n---\n\n\n\n\nIn this chapter, we connect the **causal estimand**—the quantity that answers our scientific question—to a **statistical estimand**, which is something we can estimate from observed data. This is the crucial middle step of the Causal Roadmap: translating what we *want* to know into what we *can* learn from the dataset at hand.\n\nWe will walk carefully through:\n- Identification: when causal effects are estimable from data\n- Statistical estimands: mapping causal parameters to observable quantities\n- Positivity, consistency, and exchangeability in practice\n- Why regression coefficients are *not* causal effects\n- How to compute identified estimands using real R code\n\n---\n\n# 1. What Does It Mean to *Identify* a Causal Effect?\n\n:::{.callout-important}\n## Identification: The Bridge from Theory to Data\n\nIn plain language, **identification** asks: \"Can we figure out the causal effect using only the data we actually collected?\" We define causal effects using potential outcomes -- what *would have happened* under each treatment -- but we never observe both for the same person. Identification is the logical argument that, under stated assumptions, the unobservable causal quantity equals something we *can* compute from real data. Without identification, no amount of statistical sophistication will recover a valid causal estimate.\n\nA causal estimand (e.g., ATE = E[Y(1) -- Y(0)]) is defined in terms of **potential outcomes**, which we cannot fully observe. Identification means:\n\n> **Can we express the causal estimand entirely in terms of the observed data distribution?**\n\nIf yes, the causal effect is **identified**.\nIf no, the causal effect is **not identifiable** without additional untestable assumptions or new data.\n:::\n\nThe three key assumptions required for identification are:\n\n:::{.callout-note}\n## Assumption 1: Consistency -- \"You Get What You Get\"\n\nThe observed outcome equals the counterfactual outcome under the treatment actually received:\n\n$$\nY = Y(A)\n$$\n\nIn practice, this means the treatment is **well-defined** and there is only one version of it. If someone took denosumab, their observed outcome equals their potential outcome under denosumab. If they took ZA, their observed outcome equals their potential outcome under ZA. Consistency can break down when \"treatment\" is vague (e.g., \"exercise more\") or when the drug formulation, dose, or route of administration varies across patients in ways that matter for the outcome.\n:::\n\n\n:::{.callout-warning}\n## Assumption 2: Exchangeability -- The Hardest Assumption to Defend\n\nFormally:\n\n$$\nY(a) \\perp A \\mid W\n$$\n\nThis is the assumption that **there are no unmeasured confounders**. After adjusting for the measured covariates $W$, the treated and untreated groups are comparable -- as if treatment had been randomly assigned within strata of $W$.\n\n- In a randomized trial, randomization guarantees exchangeability by design.\n- In observational data, we must **assume** it holds -- and this assumption is **untestable** from the data alone.\n\nThis is typically the most difficult assumption to defend in applied epidemiology. If there are unmeasured factors (e.g., frailty, physician preference, disease severity scores not in the data) that influence both treatment and outcome, identification fails. Always use a DAG to reason about which variables are sufficient to block all backdoor paths, and conduct sensitivity analyses (e.g., E-values) to assess how robust your conclusions are to potential unmeasured confounding.\n:::\n\n\n:::{.callout-caution}\n## Assumption 3: Positivity -- Watch for Practical Violations\n\nEveryone has a *positive probability* of receiving either treatment at each level of covariates:\n\n$$\n0 < P(A=a \\mid W=w) < 1\n$$\n\n**Practical positivity violations** are extremely common in real-world observational data and deserve careful attention:\n\n- **Structural violations**: Some covariate strata *never* receive a particular treatment by clinical convention (e.g., patients over age 90 may never be prescribed aggressive chemotherapy). No statistical method can fix this -- the causal effect is simply not defined in those strata.\n- **Random violations**: Some strata have very few patients receiving one treatment, leading to near-zero propensity scores. This causes extreme inverse-probability weights and unstable estimates.\n\nSigns to look for: propensity scores piling up near 0 or 1, very large IPTW weights, and g-computation predictions extrapolating far outside the support of the data. We will practice diagnosing this with R shortly.\n:::\n\n\n---\n\n# 2. The Identification Formula\n\n:::{.callout-tip}\n## The Key Insight: Replacing Counterfactuals with Observables\n\nThe identification formula is the payoff of the three assumptions above. It tells us that **we can replace unobservable counterfactuals with observable conditional expectations**. In other words, instead of needing to see what *would have happened* to each person under both treatments (impossible), we can compute the causal effect from quantities we actually observe in the data.\n\nIf the assumptions hold, the ATE becomes:\n\n$$\nE[Y(1) - Y(0)]\n=\nE_W\\!\\left[\nE[Y \\mid A=1, W]\n-\nE[Y \\mid A=0, W]\n\\right]\n$$\n\nThis formula tells us:\n\n- First, estimate the expected outcome conditional on treatment and covariates -- separately for $A=1$ and $A=0$.\n- Then, average the difference over the empirical distribution of $W$ in the study population.\n\nThis is the foundation for all standard causal inference estimators: **g-computation**, **IPTW**, **AIPW**, and **TMLE**.\n:::\n\n---\n\n# 3. The Statistical Estimand\n\n:::{.callout-important}\n## Causal Estimand vs. Statistical Estimand -- Know the Difference\n\nA **causal estimand** is the scientific quantity of interest, defined in the language of potential outcomes (e.g., \"What is the average difference in 3-year MI risk if everyone took denosumab vs. if everyone took ZA?\"). A **statistical estimand** is the specific function of the observed data distribution that equals the causal estimand *under the identification assumptions*. These are conceptually distinct: the causal estimand lives in the world of \"what if,\" while the statistical estimand lives in the world of \"what we can compute.\"\n\n**Example:**\n\n- **Causal estimand**: 3-year difference in MI risk if all patients received denosumab versus if all received ZA\n- **Statistical estimand**: Mean difference in **standardized predicted risks** from a model of $P(Y \\mid A, W)$, averaged over the covariate distribution\n\nRegression coefficients (e.g., log-odds ratios from a logistic regression) are **not** the statistical estimand for the ATE. They are conditional measures on the log-odds scale, not marginal risk differences. Conflating the two is one of the most common errors in applied epidemiology.\n:::\n\n---\n\n# 4. Example: Identification in a Simulated Osteoporosis Cohort\n\nWe walk through a reproducible R example to illustrate the identification step.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\n\nn <- 5000\nage <- rnorm(n, 75, 6)\ncvd <- rbinom(n, 1, plogis(0.1 * (age - 70)))\n\n# Treatment assignment with confounding\nA <- rbinom(n, 1, plogis(-1 + 0.07 * (age - 70) + 1.2 * cvd))\n\n# Potential outcomes generated under each treatment\nrisk <- plogis(-2 + 0.5*A + 0.08*(age - 70) + 0.9*cvd)\nY <- rbinom(n, 1, risk)\n\ndat <- data.frame(age, cvd, A, Y)\n```\n:::\n\n\n\n\n---\n\n# 5. Step-by-Step Identification Using G-Computation\n\n### 5.1 Fit an outcome regression\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- glm(Y ~ A + age + cvd, family = binomial, data = dat)\n```\n:::\n\n\n\n\n### 5.2 Predict counterfactual outcomes\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat1 <- transform(dat, A = 1)\ndat0 <- transform(dat, A = 0)\n\np1 <- predict(mod, newdata = dat1, type = \"response\")\np0 <- predict(mod, newdata = dat0, type = \"response\")\n```\n:::\n\n\n\n\n### 5.3 Compute the standardized ATE\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nate_gcomp <- mean(p1 - p0)\nate_gcomp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1343446\n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-tip}\n## Interpreting the G-Computation ATE\n\nThe number printed above is the **average treatment effect on the risk-difference scale**. It estimates how much the probability of MI would change, on average, if every patient in the study population were given denosumab ($A=1$) compared to a world where every patient received ZA ($A=0$). A positive value means denosumab is associated with *higher* MI risk after adjusting for age and CVD history; a negative value means *lower* risk. Because this is a standardized (marginal) estimate averaged over the real covariate distribution, it has a direct public-health interpretation -- unlike a conditional odds ratio from a logistic regression.\n:::\n\n---\n\n# 6. Diagnosing Exchangeability and Positivity\n\n## 6.1 Overlap check\n\n:::{.callout-warning}\n## What Positivity Violations Look Like in Practice\n\nThe propensity score plot below is your primary diagnostic for positivity. You are looking for **red flags** in the distribution of estimated propensity scores:\n\n- **Mass piling up near 0 or 1**: This means some patients have a near-certain probability of receiving (or not receiving) treatment given their covariates. These are practical positivity violations -- the data contain little or no information about what would happen if these patients received the other treatment.\n- **Poor overlap between treated and untreated groups**: If the propensity score distributions for $A=1$ and $A=0$ do not share substantial common support, any causal estimate in the non-overlapping region relies on extrapolation rather than data.\n\nWhen you see these patterns, consider restricting the analysis to the region of common support, trimming extreme weights, or acknowledging the limitations in your interpretation.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(density(predict(glm(A ~ age + cvd, family = binomial, data = dat), type = \"response\")),\n     main = \"Propensity Score Overlap\", xlab = \"Estimated Propensity Score\")\n```\n\n::: {.cell-output-display}\n![](01-03-identification-estimands_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nps <- predict(glm(A ~ age + cvd, family = binomial, data = dat), type = \"response\")\n```\n:::\n\n\n\n\n---\n\n# 7. Interpretability: Why the Roadmap Matters\n\n:::{.callout-note}\n## The Causal Roadmap Keeps Your Analysis Honest\n\nWithout explicit identification:\n\n- Analysts might report non-causal associations as if they were causal effects\n- Sensitivity analyses are impossible because the assumptions were never stated\n- Regulatory and public-health decisions become less defensible under scrutiny\n\nWith identification:\n\n- The causal effect has a clear, unambiguous interpretation tied to a well-defined intervention\n- Assumptions are transparent and can be interrogated by reviewers, clinicians, and regulators\n- Statistical methods are chosen *because they target the identified estimand*, not out of convenience\n:::\n\n---\n\n# 8. Summary\n\n:::{.callout-tip}\n## Chapter Takeaways\n\nIn this chapter, we connected the causal estimand to observable data using the identification step. You learned:\n\n- Causal effects are defined using **potential outcomes** -- quantities we cannot fully observe\n- Identification requires three assumptions: **consistency**, **exchangeability**, and **positivity**\n- When these assumptions hold, causal effects can be expressed entirely in terms of observable data via the **identification formula**\n- A **statistical estimand** is the observable-data function that equals the causal estimand under correct identification -- it is not the same as a regression coefficient\n- **G-computation** is a simple and transparent method for implementing the identification formula: fit an outcome model, predict under each treatment, and average\n\nNext, we turn to **estimation strategies** -- how to choose and implement methods like IPTW, TMLE, or outcome regression to actually estimate the identified effect.\n:::\n\n---\n\n## Software Implementation (R)\n\nThis example estimates the effect of a **modified treatment policy** (a stochastic intervention that shifts treatment probability) using the `lmtp` package. This connects directly to the identification results in this chapter: rather than asking \"what if everyone were treated?\", we ask \"what if each person's probability of treatment increased?\"\n\n- Simulate panel data with a single time point\n- Use `lmtp_tmle()` to estimate the effect of shifting the treatment mechanism\n- Falls back to `tmle` for a standard ATE if `lmtp` is unavailable\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nn <- 500\nW1 <- rnorm(n)\nW2 <- rbinom(n, 1, 0.5)\nA  <- rbinom(n, 1, plogis(-0.3 + 0.5 * W1))\nY  <- as.numeric(rbinom(n, 1, plogis(-1 + 0.8 * A + 0.6 * W1 + 0.3 * W2)))\n\nif (requireNamespace(\"lmtp\", quietly = TRUE)) {\n  library(lmtp)\n\n  dat <- data.frame(W1 = W1, W2 = W2, A = A, Y = Y)\n\n  ## Modified treatment policy: shift everyone to A = 1\n  result <- lmtp_tmle(\n    data = dat,\n    trt = \"A\",\n    outcome = \"Y\",\n    baseline = c(\"W1\", \"W2\"),\n    shift = function(data, trt) rep(1, nrow(data)),\n    outcome_type = \"binomial\",\n    learners_outcome = \"SL.glm\",\n    learners_trt = \"SL.glm\",\n    folds = 5\n  )\n  print(result)\n} else if (requireNamespace(\"tmle\", quietly = TRUE)) {\n  message(\"lmtp not available; falling back to tmle for a standard ATE.\")\n  library(tmle)\n  fit <- tmle(Y = Y, A = A, W = data.frame(W1, W2),\n              Q.SL.library = \"SL.glm\", g.SL.library = \"SL.glm\")\n  cat(\"ATE:\", round(fit$estimates$ATE$psi, 3), \"\\n\")\n  cat(\"95% CI:\", round(fit$estimates$ATE$CI, 3), \"\\n\")\n} else {\n  message(\"Install 'lmtp' or 'tmle':\n    install.packages('lmtp')\n    install.packages('tmle')\")\n}\n```\n:::\n",
    "supporting": [
      "01-03-identification-estimands_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}