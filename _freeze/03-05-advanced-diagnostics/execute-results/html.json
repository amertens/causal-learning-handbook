{
  "hash": "2e0bb50c31db93443e4eaeb8dcf85335",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 3.5: Advanced Diagnostics and Sensitivity Analyses\"\nformat: html\n---\n\n\n\n\n# Chapter 3.5  \n## Advanced Diagnostics and Sensitivity Analyses  \n*Ensuring credible causal conclusions in real-world longitudinal data*\n\n::: {.callout-note}\n## Learning objectives\n- Assess positivity and overlap using propensity score diagnostics\n- Evaluate nuisance model performance (outcome model and treatment model)\n- Perform weight diagnostics for IPTW and TMLE, including truncation sensitivity\n- Compute and interpret E-values for sensitivity to unmeasured confounding\n- Use negative control outcomes and exposures to detect residual bias\n- Apply TMLE-specific diagnostics: influence curve summaries and calibration checks\n:::\n\n::: {.callout-important}\n## Sources and scope\nThis chapter is educational. Causal conclusions depend on identification assumptions (e.g., consistency, exchangeability, positivity) and on diagnostic evidence that the data support the target estimand. When flexible machine learning is used for nuisance estimation, valid inference typically requires cross-fitting or a cross-validated TMLE variant, plus appropriate rate conditions.\n:::\n\n\n\n\n\n\n\n\n\n:::{.callout-important}\n## Why Diagnostics Are Non-Negotiable\n\nCausal inference is never just about estimating an effect — it is about **credibly defending** that effect. As an epidemiologist, you know that an estimate without diagnostics is like a clinical trial without a protocol: technically possible, but not credible. Diagnostics and sensitivity analyses are essential components of the causal roadmap because:\n\n- Identifying assumptions are never fully testable\n- Real-world data contain missingness, selection, unmeasured confounding, and misclassification\n- Positivity and model misspecification can quietly undermine estimation\n- Regulatory-grade RWE requires transparency and robustness checks\n\nThis chapter walks through:\n\n1. Diagnostics for identification assumptions\n2. Positivity and overlap assessment\n3. Diagnostics for nuisance models (Q and g)\n4. Weight diagnostics\n5. Sensitivity analyses for unmeasured confounding\n6. Negative controls\n7. TMLE-specific diagnostics\n8. Longitudinal diagnostics (LMTP / longitudinal TMLE)\n:::\n\n---\n\n:::{.callout-note}\n## Diagnostics for Identification Assumptions\n\nThe core identification assumptions are:\n\n- **Consistency**: the observed outcome under the treatment actually received equals the potential outcome under that treatment\n- **Exchangeability (No unmeasured confounding)**: conditional on measured covariates, treatment groups are comparable\n- **Positivity**: every covariate stratum has a non-zero probability of each treatment level\n- **Correct nuisance model specification**: the models for treatment and outcome are well-specified enough to avoid bias\n\nWhile not directly testable, their **empirical implications** can be evaluated. Think of these as the load-bearing walls of your analysis -- if any one fails, the whole structure is compromised.\n:::\n\n### 1.1 Data structure and quality checks\n\nBefore causal modeling:\n\n- Verify time ordering\n- Confirm treatment and outcome timestamps\n- Inspect missingness patterns\n- Look for coding shifts (ICD-9 to ICD-10)\n- Examine distributions and implausible values  \n\n:::{.callout-tip}\n## DAG Review: Your Causal Roadmap on Paper\n\nA DAG (Directed Acyclic Graph) is not just a statistical formality -- it is your most powerful communication tool. When you sit down with clinical collaborators, a DAG makes implicit assumptions explicit. It clarifies:\n\n- **Adjustment sets**: which variables must be conditioned on to block confounding paths\n- **Potential unmeasured confounding**: where you lack data on important causes\n- **Variables that should *not* be adjusted for**: mediators (which would block the causal path you want to estimate) and colliders (which would open bias paths)\n\nDraw the DAG before writing any code. Share it with your clinical team. If they disagree with an arrow, that disagreement is a scientific conversation worth having before you run a single model.\n:::\n\n---\n\n:::{.callout-warning}\n## Positivity Diagnostics: When Your Data Cannot Support Your Question\n\nPositivity requires:\n\n> Every combination of covariates has a non-zero probability of receiving each treatment.\n\n**Why this matters so much**: Positivity violations are dangerous because they are silent. Your code will still run, your model will still produce a number, but that number may be driven by a handful of observations with extreme weights rather than by the actual data. In epidemiologic terms, you are extrapolating beyond the support of your data -- making claims about what would happen to people for whom you have essentially no evidence.\n\nViolations cause unstable weights and unreliable estimates.\n:::\n\n### 2.1 Propensity score overlap\n\n```r\nps <- predict(glm(A ~ W1 + W2, family = binomial), type = \"response\")\n\nlibrary(ggplot2)\nggplot(tibble(ps = ps, A = A), aes(x = ps, fill = factor(A))) +\n  geom_density(alpha = 0.4)\n```\n\nIndicators of concern:\n\n- Mass near 0 or 1\n- Disjoint distributions  \n\n:::{.callout-caution}\n## Clever Covariate Range (TMLE)\n\nThe clever covariate (H) is the quantity TMLE uses to update its initial outcome model. When propensity scores are near 0 or 1, the clever covariate explodes in magnitude -- and a single observation can hijack your entire estimate. Always inspect its range before trusting your TMLE output.\n\n```r\nH <- A/ps - (1-A)/(1-ps)\nsummary(H)\n```\n\nExtreme values imply near-positivity violations. If you see values in the hundreds or thousands, your estimate is likely unstable and you should investigate the propensity score distribution.\n:::\n\n:::{.callout-tip}\n## Remedies for Positivity Violations\n\nWhen you detect positivity problems, do not simply ignore them. You have several practical options:\n\n- **Restrict to the overlap population**: limit your analysis to the covariate region where both treatment groups have adequate representation. This changes your target population, but the estimate you get is actually supported by data.\n- **Truncate weights**: cap extreme weights at a percentile (e.g., 1st and 99th). This introduces a small amount of bias but can dramatically reduce variance.\n- **Use stochastic interventions instead of static ones**: rather than asking “what if everyone were treated?”, ask “what if the probability of treatment shifted by some amount?” This naturally respects the data support.\n- **Simplify the intervention**: coarser treatment definitions may have better overlap.\n:::\n\n---\n\n:::{.callout-note}\n## Diagnostics for Nuisance Functions (Q and g)\n\nIn causal inference, \"nuisance functions\" are the models you fit not because you care about their parameters, but because you need them to estimate the causal effect. **Q** is the outcome model (predicting Y given treatment and covariates) and **g** is the treatment model (predicting treatment probability given covariates, i.e., the propensity score). Accurate nuisance models are crucial for TMLE, AIPW, and LMTP. If these models are badly misspecified, your causal estimate inherits that bias -- even with doubly robust methods, you need at least one of Q or g to be well-specified.\n:::\n\n### 3.1 Predictive accuracy\n\n- AUC for binary outcomes  \n- MSE/R² for continuous  \n- Cross-validated risk from SuperLearner  \n\n### 3.2 Calibration plots\n\n```r\ndat %>% \n  mutate(pred = Qhat) %>% \n  ggplot(aes(x = pred, y = Y)) +\n    geom_point(alpha = 0.3) +\n    geom_smooth()\n```\n\n### 3.3 Overfitting assessment\n\nCompare:\n\n- Training loss  \n- Cross-validated loss  \n\nLarge discrepancy → overfitting.\n\n### 3.4 Variable-importance sanity check\n\nEnsure top predictors are *clinically plausible*.\n\n---\n\n:::{.callout-warning}\n## Weight Diagnostics: Are a Few Observations Driving Your Results?\n\nInverse probability weights are used in IPTW, MSMs, and censoring models. Extreme weights are a red flag that a small number of unusual observations are exerting outsized influence on your effect estimate. Always inspect weights before reporting results.\n\n### 4.1 Weight summaries\n\n```r\nsummary(weights)\nquantile(weights, probs = c(0.01, 0.99))\n```\n\nRed flags:\n\n- Mean far from 1 (for stabilized weights, the mean should be approximately 1)\n- Very heavy tail\n- Huge max weights (a single weight of 500 means that one person \"counts\" as 500 in your pseudo-population)\n\n### 4.2 Visual check\n\n```r\nggplot(tibble(w = weights), aes(x = w)) +\n  geom_histogram()\n```\n\n### 4.3 Truncation\n\n```r\nlower <- quantile(weights, 0.01)\nupper <- quantile(weights, 0.99)\nw_trunc <- pmin(pmax(weights, lower), upper)\n```\n\nReport results both with and without truncation. If they differ substantially, your findings are fragile.\n:::\n\n---\n\n## 5. Sensitivity Analyses for Unmeasured Confounding\n\n:::{.callout-note}\n## E-values: How Strong Would Unmeasured Confounding Need to Be?\n\nAn **E-value** answers a simple but powerful question: *What is the minimum strength of association that an unmeasured confounder would need to have with both the treatment and the outcome -- above and beyond the measured covariates -- to fully explain away the observed effect?*\n\nIn plain language: if you find an E-value of 3.5, it means an unmeasured confounder would need to be associated with at least a 3.5-fold increase in both the likelihood of treatment *and* the likelihood of the outcome to reduce your observed effect to the null. The larger the E-value, the more robust your finding is to unmeasured confounding. A small E-value (close to 1) means even weak unmeasured confounding could account for your result.\n\nE-values do not prove the absence of confounding -- they quantify how extreme an unmeasured confounder would need to be. This helps reviewers and decision-makers calibrate their confidence in the finding.\n:::\n\n### 5.2 Quantitative Bias Analysis (QBA)\n\nSimulates impact of:\n\n- Unmeasured confounder prevalence  \n- Unmeasured confounder associations  \n\nR packages: **episensr**, **causalsens**\n\n\n### 5.4 Sensitivity using stochastic interventions\n\nLMTP can quantify robustness of static intervention effects.\n\n---\n\n:::{.callout-important}\n## Negative Controls: One of the Most Powerful Tools for Detecting Unmeasured Confounding\n\nNegative controls are arguably the single most informative diagnostic you can run for unmeasured confounding. The logic is elegant: pick an outcome that you *know* the treatment should not affect, but that shares the same confounding structure as your primary outcome. Then estimate the \"effect\" of treatment on that negative control outcome. If you find an association, something is wrong -- and that something is almost certainly unmeasured confounding or some other systematic bias.\n\n**Negative control outcomes (NCOs):**\n\n- Causally unrelated to treatment\n- Share confounding structures with the real outcome\n\nIf TMLE of treatment on the NCO produces a non-null result, confounding likely remains in your primary analysis.\n\nExample:\n\n```r\ntmle_nco <- tmle(\n  Y = dat$negative_event,\n  A = dat$treatment,\n  W = dat[, confounders]\n)\n```\n\n**Negative control exposures** work similarly: pick an exposure that should not affect your outcome but shares the same confounding structure as the real treatment. Both types serve as empirical \"smoke tests\" for bias.\n:::\n\n---\n\n:::{.callout-caution}\n## TMLE-Specific Diagnostics: Things That Can Go Wrong in the Targeting Step\n\nTMLE adds a \"targeting step\" that updates the initial outcome model to optimize bias-variance tradeoff for your specific causal parameter. This step is powerful but can go wrong in subtle ways. Watch for these issues:\n\n### 7.1 Clever covariate behavior\n\nExtreme clever covariate values lead to unstable targeting. If the clever covariate has values in the hundreds or thousands, the fluctuation parameter (epsilon) is being estimated from near-singular data. Revisit your propensity score model.\n\n### 7.2 Targeting step convergence\n\nCheck for warnings in logistic fluctuation:\n\n```\nglm.fit: algorithm did not converge\n```\n\nThis warning means the targeting step failed to find a stable update. Common causes include near-positivity violations or a very poor initial fit.\n\n### 7.3 Influence-curve distribution\n\n```r\nIC <- tmle_fit$ic\nmean(IC); var(IC)\n```\n\nThe mean of the influence curve should be approximately zero (this is the efficient influence function equation that TMLE solves). Heavy tails in the IC distribution suggest that Wald-type confidence intervals may have poor coverage -- consider bootstrap-based inference instead.\n:::\n\n---\n\n:::{.callout-warning}\n## Longitudinal Diagnostics: Where Problems Compound Over Time\n\nIn longitudinal settings (LMTP, longitudinal TMLE), every challenge from the cross-sectional case is amplified. Positivity must hold at *every* timepoint. Weights are *multiplied* across time, so moderate violations at each step can produce extreme cumulative weights. This section is critical for anyone working with repeated-measures or time-varying treatment data.\n\n### 8.1 Sequential positivity\n\nCheck treatment probabilities at each timepoint. A treatment probability of 0.05 at one time may be acceptable, but if you have 10 timepoints with similar probabilities, the cumulative probability can become vanishingly small.\n\n### 8.2 Cumulative weights\n\n```r\ncumw <- apply(weight_matrix, 1, prod)\nhist(cumw)\n```\n\nExtreme cumulative weights signal instability. Even if weights look reasonable at each individual time step, their product can be enormous.\n\n### 8.3 Truncation across time\n\nTruncate weights at each time step or truncate cumulative weights. Both approaches involve tradeoffs -- per-step truncation is more conservative but can compound across time; cumulative truncation directly addresses the problem but is harder to interpret.\n\n### 8.4 Time-varying confounding sanity checks\n\nEnsure intermediate variables are not inappropriate colliders. In longitudinal settings, a variable can be both a confounder (of future treatment-outcome relationships) and a mediator (of past treatment effects). The DAG becomes essential for sorting out what should and should not be conditioned on at each time step.\n:::\n\n---\n\n:::{.callout-tip}\n## Recommended Diagnostics Workflow: A Practical Checklist\n\nUse this as a step-by-step checklist for every causal analysis. Print it out, tape it to your monitor, or include it in your analysis plan. Skipping steps here is how \"surprising\" results end up in print.\n\n### Before estimation\n\n- [ ] Confirm time-ordering of treatment, covariates, and outcome\n- [ ] Draw a DAG and review it with clinical collaborators\n- [ ] Check missingness patterns (MCAR/MAR/MNAR)\n- [ ] Summarize covariate distributions by treatment group (Table 1)\n\n### During estimation\n\n- [ ] Check propensity score overlap (density plots by treatment group)\n- [ ] Evaluate weight distributions (summaries, histograms, max values)\n- [ ] Inspect Q and g predictions (calibration, AUC, cross-validated risk)\n- [ ] Check TMLE targeting step convergence and clever covariate range\n\n### After estimation\n\n- [ ] Run sensitivity analyses (E-values, quantitative bias analysis)\n- [ ] Run negative control analyses (NCOs and/or NCEs)\n- [ ] Compare across estimators (IPTW, TMLE, AIPW) -- agreement increases credibility\n- [ ] Perform robustness checks (population restriction, alternative confounder sets, different SuperLearner libraries)\n:::\n\n---\n\n:::{.callout-important}\n## Summary: Diagnostics Are the Backbone of Credible Causal Inference\n\nTo produce defensible causal evidence, diagnostics must be:\n\n- **Systematic**: follow the workflow above for every analysis, not just when results look surprising\n- **Transparent**: report diagnostics in your paper or supplement, not just in your internal notes\n- **Documented in the analysis plan**: pre-specify which diagnostics you will run and how you will respond to problems\n- **Interpreted with domain expertise**: a propensity score distribution or E-value only becomes meaningful when contextualized by clinical knowledge\n\nWith these tools, analysts can judge the **credibility**, **robustness**, and **transportability** of causal findings -- essential for regulatory, clinical, and scientific decision-making. An estimate without diagnostics is not a causal estimate; it is a hope.\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.4.2 (2024-10-31 ucrt)\n#> Platform: x86_64-w64-mingw32/x64\n#> Running under: Windows 11 x64 (build 26200)\n#> \n#> Matrix products: default\n#> \n#> \n#> locale:\n#> [1] LC_COLLATE=English_United States.utf8 \n#> [2] LC_CTYPE=English_United States.utf8   \n#> [3] LC_MONETARY=English_United States.utf8\n#> [4] LC_NUMERIC=C                          \n#> [5] LC_TIME=English_United States.utf8    \n#> \n#> time zone: America/Los_Angeles\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.2.0     cli_3.6.5        \n#>  [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n#>  [9] rmarkdown_2.29    knitr_1.49        jsonlite_2.0.0    xfun_0.49        \n#> [13] digest_0.6.37     rlang_1.1.6       evaluate_1.0.5\n```\n:::\n\n\n\n\n---\n\n## Sources and further reading\n\n- VanderWeele TJ, Ding P (2017). Sensitivity analysis in observational research: introducing the E-value. *Ann Intern Med* 167(4):268-274.\n- Lipsitch M, Tchetgen Tchetgen E, Cohen T (2010). Negative controls: a tool for detecting confounding and bias in observational studies. *Epidemiology* 21(3):383-388.\n- Petersen ML, Porter KE, Gruber S, et al. (2012). Diagnosing and responding to violations in the positivity assumption. *Stat Methods Med Res* 21(1):31-54.\n- Gruber S, van der Laan MJ (2010). An application of collaborative targeted maximum likelihood estimation in causal inference and genomics. *Int J Biostat* 6(1):Article 18.\n- van der Laan MJ, Rose S (2011). *Targeted Learning*. Springer. Chapter 10 (Diagnostics).\n- Hernan MA, Robins JM (2020). *Causal Inference: What If*. Chapters 11-12. [Free online](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)\n- `EValue` R package: [CRAN](https://cran.r-project.org/package=EValue)\n- `drtmle` R package: [CRAN](https://cran.r-project.org/package=drtmle)\n\n---\n\n## Software Implementation (R)\n\nThis example demonstrates key **diagnostic checks** for a TMLE analysis: positivity assessment, clever covariate summaries, weight truncation sensitivity, and E-value computation.\n\n- Simulate a dataset with a near-positivity violation in one confounder stratum\n- Run TMLE, then inspect propensity score overlap and clever covariate distribution\n- Show truncation sensitivity: compare ATE estimates across different truncation levels\n- Compute an E-value for robustness to unmeasured confounding\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1)\nn <- 800\nW1 <- rnorm(n)\nW2 <- rbinom(n, 1, 0.3)\n## Near-positivity violation: W2 = 1 strongly predicts A = 1\nA  <- rbinom(n, 1, plogis(-0.5 + 0.4 * W1 + 2.5 * W2))\nY  <- 0.5 * A + 0.8 * W1 - 0.3 * W2 + rnorm(n, sd = 0.5)\n\nW  <- data.frame(W1 = W1, W2 = W2)\n\n## ── 1. Propensity score overlap ──\nps_mod <- glm(A ~ W1 + W2, family = binomial)\nps <- predict(ps_mod, type = \"response\")\ncat(\"Propensity score summary:\\n\")\nprint(summary(ps))\ncat(\"\\nMin ps:\", round(min(ps), 4),\n    \"  Max ps:\", round(max(ps), 4), \"\\n\")\n\n## ── 2. TMLE with diagnostics ──\nif (requireNamespace(\"tmle\", quietly = TRUE)) {\n  library(tmle)\n  fit <- tmle(Y = Y, A = A, W = W,\n              Q.SL.library = \"SL.glm\", g.SL.library = \"SL.glm\")\n\n  ## Clever covariate summary\n  H1 <- A / ps\n  H0 <- (1 - A) / (1 - ps)\n  cat(\"\\nClever covariate (treated) — max:\", round(max(H1), 1),\n      \" 99th pctile:\", round(quantile(H1, 0.99), 1), \"\\n\")\n\n  ## Influence curve check\n  cat(\"EIC mean:\", round(mean(fit$estimates$ATE$IC), 6),\n      \" (should ≈ 0)\\n\")\n\n  ## ── 3. Truncation sensitivity ──\n  cat(\"\\n── Truncation sensitivity ──\\n\")\n  for (g_bound in c(0.01, 0.025, 0.05, 0.10)) {\n    fit_trunc <- tmle(Y = Y, A = A, W = W,\n                      Q.SL.library = \"SL.glm\",\n                      g.SL.library = \"SL.glm\",\n                      gbound = g_bound)\n    cat(sprintf(\"  g-bound = %.3f → ATE = %.3f  (SE = %.3f)\\n\",\n                g_bound,\n                fit_trunc$estimates$ATE$psi,\n                sqrt(fit_trunc$estimates$ATE$var.psi)))\n  }\n} else {\n  message(\"Install the 'tmle' package:  install.packages('tmle')\")\n}\n\n## ── 4. E-value (no package needed) ──\n## For a risk ratio of 1.5, the E-value formula is:\n##   E = RR + sqrt(RR * (RR - 1))\nRR <- 1.5\nE_val <- RR + sqrt(RR * (RR - 1))\ncat(\"\\n── E-value ──\\n\")\ncat(\"Point estimate RR:\", RR, \"\\n\")\ncat(\"E-value:\", round(E_val, 2),\n    \"\\n(An unmeasured confounder would need RR ≥\",\n    round(E_val, 2), \"with both A and Y to explain away the effect)\\n\")\n```\n:::\n",
    "supporting": [
      "03-05-advanced-diagnostics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}