{
  "hash": "55350331cb728c2a513a860f014953d9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"An Illustrated Guide to Longitudinal TMLE\"\nsubtitle: \"Extending the KH Stats approach to time-varying treatments and confounders\"\nformat: html\n---\n\n\n\n\n# An Illustrated Guide to Longitudinal TMLE\n*How L-TMLE removes time-dependent confounding, step by step*\n\nThis chapter extends the pedagogical approach of Kat Hoffman's [Illustrated Guide to TMLE](https://www.khstats.com/blog/tmle/tutorial) to the **longitudinal setting**. If you have read the KH Stats guides and understand point-treatment TMLE, you are ready for this chapter.\n\nWe will build up from the familiar 4-step TMLE algorithm to its longitudinal counterpart, using a simple two-time-point example so you can see exactly what changes — and why.\n\n---\n\n# 1. Quick Recap: Point-Treatment TMLE in 4 Steps\n\nIn the [KH Stats illustrated guide](https://www.khstats.com/blog/tmle/tutorial-pt2), TMLE for a binary treatment and outcome proceeds in four steps:\n\n:::{.callout-note}\n## Step 1: Estimate the Expected Outcome\nFit a model for $E[Y \\mid A, W]$ — the expected outcome given treatment and confounders. Use this to predict outcomes under treatment ($A = 1$) and under control ($A = 0$) for every observation.\n\n$$\\hat{Q}(A, W) = \\hat{E}[Y \\mid A, W]$$\n$$\\hat{Q}(1, W) \\quad \\text{and} \\quad \\hat{Q}(0, W)$$\n:::\n\n:::{.callout-tip}\n## Step 2: Estimate the Propensity Score\nFit a model for $P(A = 1 \\mid W)$ — the probability of treatment given confounders.\n\n$$\\hat{g}(W) = \\hat{P}(A = 1 \\mid W)$$\n:::\n\n:::{.callout-important}\n## Step 3: Compute the Clever Covariate\nCombine the propensity score into a covariate that \"cleverly\" bridges the outcome model and the treatment model:\n\n$$H(A, W) = \\frac{I(A=1)}{\\hat{g}(W)} - \\frac{I(A=0)}{1 - \\hat{g}(W)}$$\n:::\n\n:::{.callout-warning}\n## Step 4: Target, Then Estimate\nRun a logistic regression of $Y$ on $H(A, W)$ with $\\text{logit}(\\hat{Q}(A, W))$ as an offset. The coefficient $\\hat{\\epsilon}$ is the **fluctuation parameter**. Update predictions:\n\n$$\\hat{Q}^*(A, W) = \\text{expit}\\big(\\text{logit}(\\hat{Q}(A, W)) + \\hat{\\epsilon} \\cdot H(A, W)\\big)$$\n\nThe TMLE estimate of the ATE is:\n\n$$\\hat{\\psi} = \\frac{1}{n}\\sum_i \\hat{Q}^*(1, W_i) - \\frac{1}{n}\\sum_i \\hat{Q}^*(0, W_i)$$\n:::\n\nThis works beautifully for a single treatment decision at one point in time. But what happens when treatment occurs **repeatedly over time**, and confounders at each time are affected by prior treatment?\n\n---\n\n# 2. The New Problem: Time-Varying Treatment and Confounding\n\n## A simple scenario\n\nConsider a patient followed over **two time periods**. At each period, they either take a medication ($A_t = 1$) or not ($A_t = 0$). We measure their health status ($L_t$) at each period. We observe a final outcome $Y$.\n\nThe data for one patient looks like:\n\n```\nBaseline → Treatment₁ → Health₁ → Treatment₂ → Health₂ → Outcome\n   W          A₁          L₁          A₂          L₂         Y\n```\n\n## Why is this harder?\n\nThe critical complication is a **feedback loop**:\n\n```\n                    ┌──────────────────────────────┐\n                    │                              │\n                    ▼                              │\n   W ──→ A₁ ──→ L₁ ──→ A₂ ──→ Y                  │\n         │              ▲                          │\n         │              │                          │\n         └──────────────┘                          │\n         (A₁ affects L₁, which affects A₂)        │\n```\n\n- $L_1$ is a **confounder** for the $A_2 \\to Y$ relationship (it affects both $A_2$ and $Y$)\n- $L_1$ is also **affected by prior treatment** $A_1$\n\nThis means $L_1$ is simultaneously:\n\n- A confounder we need to adjust for\n- A mediator on the pathway from $A_1$ to $Y$\n\n:::{.callout-caution}\n## The Standard Regression Trap\nIf you **condition on** $L_1$ in a regression (the standard approach), you block part of the causal effect of $A_1$ that operates through $L_1$. You also potentially open collider bias paths.\n\nIf you **don't condition on** $L_1$, the effect of $A_2$ is confounded.\n\n**You cannot win with standard regression.** This is the fundamental problem that g-methods (including L-TMLE) solve.\n:::\n\n## A concrete example\n\nImagine studying whether continuous statin use prevents heart attack over 2 years:\n\n- **$W$:** Baseline cholesterol, age, smoking\n- **$A_1$:** Statin use in year 1\n- **$L_1$:** Cholesterol level at end of year 1 (affected by whether they took statins)\n- **$A_2$:** Statin use in year 2 (depends on their year-1 cholesterol)\n- **$Y$:** Heart attack by end of year 2\n\nA doctor is more likely to prescribe statins in year 2 if year-1 cholesterol is high ($L_1$ affects $A_2$). But high cholesterol also directly increases heart attack risk ($L_1$ affects $Y$). And year-1 statin use lowers year-1 cholesterol ($A_1$ affects $L_1$).\n\nConditioning on year-1 cholesterol in a regression blocks the indirect effect of year-1 statin use. Not conditioning on it leaves the year-2 effect confounded. **L-TMLE resolves this by working backwards through time.**\n\n---\n\n# 3. The Key Insight: Working Backwards\n\nPoint-treatment TMLE estimates $E[Y \\mid A, W]$ in a single step. Longitudinal TMLE uses **iterated conditional expectations** — a sequence of regressions that build on each other, working from the final outcome backwards to baseline.\n\n## The G-computation formula for longitudinal data\n\nFor the simple two-time-point case, the causal effect of \"always treat\" ($\\bar{a} = (1, 1)$) is:\n\n$$\nE[Y(\\bar{a})] = E_W\\bigg[ E_{L_1}\\Big[ E\\big[Y \\mid A_2 = 1, L_1, A_1 = 1, W\\big] \\;\\Big|\\; A_1 = 1, W \\Big] \\bigg]\n$$\n\nReading from inside out:\n\n1. **Inner expectation:** Model the outcome given the full history, then evaluate under the intervention $A_2 = 1$\n2. **Middle expectation:** Average over $L_1$ (the intermediate confounder) under the intervention $A_1 = 1$\n3. **Outer expectation:** Average over baseline covariates $W$\n\nThis is a **nested sequence** of conditional expectations. L-TMLE estimates each one, starting from the inside (the final outcome) and working outward (backwards in time).\n\n## Why backwards?\n\n:::{.callout-tip}\n## The Backwards Intuition\nThink of it like planning a road trip:\n\n- **Forwards:** \"From home, where might I end up?\" (requires considering all possible routes — combinatorial explosion)\n- **Backwards:** \"To reach my destination, where must I be at each prior step?\" (only one path to trace back)\n\nL-TMLE starts at the **destination** (the final outcome $Y$) and asks: given where we are at each earlier time, what outcome would we expect? Each step peels off one layer of time-varying confounding.\n:::\n\n---\n\n# 4. L-TMLE Step by Step (Two Time Points)\n\nWe now walk through the full L-TMLE algorithm for the \"always treat\" regime $\\bar{a} = (1, 1)$ with two time points. Each step is color-coded to match the point-treatment TMLE steps, so you can see the parallels.\n\n## Setup: Simulate data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'stringr' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.6.0\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(2026)\nn <- 2000\n\n# --- Baseline ---\nW <- rnorm(n, mean = 0, sd = 1)   # baseline confounder\n\n# --- Time 1 ---\n# Treatment at time 1 (depends on W)\ng1_true <- plogis(0.5 + 0.8 * W)\nA1 <- rbinom(n, 1, g1_true)\n\n# Time-varying confounder (affected by A1 and W)\nL1 <- rnorm(n, mean = 0.5 * W + 0.7 * A1, sd = 1)\n\n# --- Time 2 ---\n# Treatment at time 2 (depends on L1 and A1)\ng2_true <- plogis(-0.3 + 0.6 * L1 + 0.4 * A1)\nA2 <- rbinom(n, 1, g2_true)\n\n# --- Outcome ---\n# Y depends on full history\nY <- rbinom(n, 1, plogis(-2 + 0.5 * A1 + 0.6 * A2 + 0.3 * L1 +\n                           0.4 * W + 0.2 * A1 * A2))\n\ndat <- tibble(W, A1, L1, A2, Y)\n\ncat(\"Observed data:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nObserved data:\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n        W    A1      L1    A2     Y\n    <dbl> <int>   <dbl> <int> <int>\n1  0.521      1  0.0322     0     0\n2 -1.08       0 -0.301      0     0\n3  0.139      1  2.54       1     1\n4 -0.0847     1  0.337      1     0\n5 -0.667      1  0.422      0     1\n6 -2.52       0 -0.316      1     0\n```\n\n\n:::\n:::\n\n\n\n\n### True effect of \"always treat\"\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate the true counterfactual under (A1=1, A2=1)\nset.seed(2026)\nW_cf <- W  # same baseline\n\n# Under A1 = 1\nL1_cf <- rnorm(n, mean = 0.5 * W_cf + 0.7 * 1, sd = 1)\n\n# Under A2 = 1\nY_cf_11 <- plogis(-2 + 0.5 * 1 + 0.6 * 1 + 0.3 * L1_cf +\n                     0.4 * W_cf + 0.2 * 1 * 1)\n\n# Under (A1=0, A2=0)\nL1_cf_00 <- rnorm(n, mean = 0.5 * W_cf + 0.7 * 0, sd = 1)\nY_cf_00 <- plogis(-2 + 0.5 * 0 + 0.6 * 0 + 0.3 * L1_cf_00 +\n                     0.4 * W_cf + 0.2 * 0 * 0)\n\ntrue_EY_11 <- mean(Y_cf_11)\ntrue_EY_00 <- mean(Y_cf_00)\ntrue_ATE <- true_EY_11 - true_EY_00\n\ncat(\"True E[Y(1,1)]:\", round(true_EY_11, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue E[Y(1,1)]: 0.3945 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True E[Y(0,0)]:\", round(true_EY_00, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue E[Y(0,0)]: 0.1328 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True ATE:       \", round(true_ATE, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue ATE:        0.2617 \n```\n\n\n:::\n:::\n\n\n\n\n---\n\nNow we estimate $E[Y(1,1)]$ — the expected outcome if everyone always received treatment — using L-TMLE.\n\n## Phase A: Estimate the Treatment Mechanisms (the g's)\n\n:::{.callout-tip}\n## L-TMLE Step 1: Estimate Treatment at EACH Time Point\n\nIn point-treatment TMLE, we estimated one propensity score $g(W)$. In L-TMLE, we estimate a **treatment mechanism at every time point**, conditional on the history up to that point.\n\n**Time 1:**\n$$\\hat{g}_1(W) = \\hat{P}(A_1 = 1 \\mid W)$$\n\n**Time 2:**\n$$\\hat{g}_2(A_1, L_1, W) = \\hat{P}(A_2 = 1 \\mid A_1, L_1, W)$$\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit treatment models at each time point\ng1_mod <- glm(A1 ~ W, family = binomial, data = dat)\ng2_mod <- glm(A2 ~ L1 + A1 + W, family = binomial, data = dat)\n\ng1_hat <- predict(g1_mod, type = \"response\")\ng2_hat <- predict(g2_mod, type = \"response\")\n\n# Bound away from 0 and 1\ng1_hat <- pmax(0.01, pmin(0.99, g1_hat))\ng2_hat <- pmax(0.01, pmin(0.99, g2_hat))\n```\n:::\n\n\n\n\n### The cumulative treatment probability\n\nFor the \"always treat\" regime, the probability that a patient follows the full regime is the **product** of time-specific probabilities:\n\n$$\\hat{g}_{\\bar{1}}(W, L_1) = \\hat{g}_1(W) \\times \\hat{g}_2(A_1=1, L_1, W)$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cumulative probability of following the \"always treat\" regime\n# g2 needs to be evaluated at A1=1 (the intervention value)\ng2_under_a1 <- predict(g2_mod,\n                        newdata = dat %>% mutate(A1 = 1),\n                        type = \"response\")\ng2_under_a1 <- pmax(0.01, pmin(0.99, g2_under_a1))\n\ng_cumulative <- g1_hat * g2_under_a1\ncat(\"Cumulative g range:\", round(range(g_cumulative), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCumulative g range: 0.017 0.845 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Cumulative g mean: \", round(mean(g_cumulative), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCumulative g mean:  0.36 \n```\n\n\n:::\n:::\n\n\n\n\n---\n\n## Phase B: Sequential Outcome Regression — Working Backwards\n\nThis is where L-TMLE diverges from point-treatment TMLE. Instead of one outcome model, we build a **sequence** of outcome regressions, starting at the final outcome and moving backwards.\n\n:::{.callout-note}\n## L-TMLE Step 2a: Start at the End — Model the Final Outcome\n\nJust like point-treatment TMLE, start by modeling $E[Y \\mid A_2, L_1, A_1, W]$. This is the expected outcome given the complete observed history.\n\n$$\\hat{Q}_2(A_2, L_1, A_1, W) = \\hat{E}[Y \\mid A_2, L_1, A_1, W]$$\n\nThen predict under the intervention $A_2 = 1$:\n\n$$\\hat{Q}_2(1, L_1, A_1, W)$$\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 2a: Outcome regression at the final time point\nQ2_mod <- glm(Y ~ A2 + L1 + A1 + W + A1:A2, family = binomial, data = dat)\n\n# Predict under intervention A2 = 1, keeping everything else observed\nQ2_A2is1 <- predict(Q2_mod,\n                     newdata = dat %>% mutate(A2 = 1),\n                     type = \"response\")\n\ncat(\"Initial Q2(A2=1) range:\", round(range(Q2_A2is1), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInitial Q2(A2=1) range: 0.0343 0.7825 \n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-note}\n## L-TMLE Step 2b: Move Backwards — Create a \"Pseudo-Outcome\"\n\nHere is the critical step that handles time-dependent confounding. We take the predictions $\\hat{Q}_2(1, L_1, A_1, W)$ — which are functions of $L_1$, $A_1$, and $W$ — and treat them as a **new outcome** to be modeled at time 1.\n\nWe fit a regression of $\\hat{Q}_2(1, L_1, A_1, W)$ on $(A_1, W)$, **averaging over $L_1$** in the process:\n\n$$\\hat{Q}_1(A_1, W) = \\hat{E}\\big[\\hat{Q}_2(1, L_1, A_1, W) \\;\\big|\\; A_1, W\\big]$$\n\nThen predict under the intervention $A_1 = 1$:\n\n$$\\hat{Q}_1(1, W)$$\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 2b: Backwards regression — model Q2 predictions as a function of (A1, W)\n# This \"integrates out\" L1 from the estimation\n\nQ1_mod <- glm(Q2_A2is1 ~ A1 + W, family = quasibinomial, data = dat)\n\n# Predict under intervention A1 = 1\nQ1_A1is1 <- predict(Q1_mod,\n                     newdata = dat %>% mutate(A1 = 1),\n                     type = \"response\")\n\ncat(\"Initial Q1(A1=1) range:\", round(range(Q1_A1is1), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInitial Q1(A1=1) range: 0.1412 0.7377 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Initial G-comp estimate E[Y(1,1)]:\", round(mean(Q1_A1is1), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInitial G-comp estimate E[Y(1,1)]: 0.3899 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True E[Y(1,1)]:                   \", round(true_EY_11, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue E[Y(1,1)]:                    0.3945 \n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-caution}\n## What Just Happened?\n\nThis two-step backwards regression is the **longitudinal G-computation** formula in action:\n\n1. **Step 2a** estimated $E[Y \\mid A_2=1, L_1, A_1, W]$ — what we expect the outcome to be if $A_2 = 1$, for each specific value of $L_1$\n\n2. **Step 2b** estimated $E_{L_1}[E[Y \\mid A_2=1, L_1, A_1=1, W] \\mid A_1=1, W]$ — the expected outcome under both $A_1=1$ and $A_2=1$, **averaging over $L_1$ as it would be under $A_1 = 1$**\n\nBy regressing the time-2 predictions on $(A_1, W)$ **without conditioning on $L_1$**, we allowed $L_1$ to vary as it naturally would under the intervention $A_1 = 1$. This is how the backwards regression avoids the standard regression trap — it never conditions on the time-varying confounder while simultaneously modeling it as affected by treatment.\n\n**This is longitudinal G-computation. But like point-treatment G-computation, it depends on both models being correctly specified. The targeting steps below make it doubly robust.**\n:::\n\n---\n\n## Phase C: Targeting — Making It Doubly Robust\n\nNow we have initial estimates $\\hat{Q}_2$ and $\\hat{Q}_1$ from the sequential regressions. These estimates are optimized to predict well, but they are not yet \"targeted\" at our specific estimand $E[Y(1,1)]$.\n\nThe targeting steps update these initial estimates using information from the treatment mechanism, just like in point-treatment TMLE — but now we do it **at each time step, working backwards**.\n\n:::{.callout-important}\n## L-TMLE Step 3a: Target $\\hat{Q}_2$ at Time 2\n\nConstruct the clever covariate at time 2. For the \"always treat\" regime, among observations with $A_1 = 1$ and $A_2 = 1$ (those who followed the regime), this is:\n\n$$H_2 = \\frac{I(A_1 = 1, A_2 = 1)}{\\hat{g}_1(W) \\times \\hat{g}_2(1, L_1, W)}$$\n\nThis is the inverse of the **cumulative** probability of following the full treatment regime through time 2.\n\nFit the fluctuation model:\n\n$$\\text{logit}(Y) = \\text{logit}(\\hat{Q}_2(A_2, L_1, A_1, W)) + \\epsilon_2 \\cdot H_2$$\n\nUpdate: $\\hat{Q}_2^*(A_2, L_1, A_1, W) = \\text{expit}(\\text{logit}(\\hat{Q}_2) + \\hat{\\epsilon}_2 \\cdot H_2)$\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit <- function(p) log(p / (1 - p))\n\n# Clever covariate at time 2\n# Only patients who followed regime at BOTH times contribute\nH2 <- (dat$A1 == 1 & dat$A2 == 1) / (g1_hat * g2_hat)\n\n# Prediction from Q2 at OBSERVED (A1, A2)\nQA_init_t2 <- predict(Q2_mod, type = \"response\")\n\n# Fluctuation model at time 2\nfluc2 <- glm(dat$Y ~ -1 + offset(logit(QA_init_t2)) + H2,\n             family = binomial)\neps2 <- coef(fluc2)\n\n# Update Q2 predictions under A2=1\nQ2_star <- plogis(logit(Q2_A2is1) + eps2 / (g1_hat * g2_under_a1))\n# Note: for the targeted predictions under intervention, H2 = 1/g_cumulative\n\ncat(\"Epsilon at time 2:\", round(eps2, 5), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpsilon at time 2: 0.01659 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Updated Q2*(A2=1) range:\", round(range(Q2_star), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUpdated Q2*(A2=1) range: 0.0767 0.7858 \n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-important}\n## L-TMLE Step 3b: Target $\\hat{Q}_1$ at Time 1\n\nNow we repeat the targeting at time 1, but using the **updated** $\\hat{Q}_2^*$ as the outcome.\n\nRe-fit the backwards regression using $\\hat{Q}_2^*$ (the targeted predictions from step 3a):\n\n$$\\hat{Q}_1^{\\text{updated}}(A_1, W) = \\hat{E}\\big[\\hat{Q}_2^*(1, L_1, A_1, W) \\;\\big|\\; A_1, W\\big]$$\n\nThen target with the clever covariate at time 1:\n\n$$H_1 = \\frac{I(A_1 = 1)}{\\hat{g}_1(W)}$$\n\nFit: $\\text{logit}(\\hat{Q}_2^*) = \\text{logit}(\\hat{Q}_1^{\\text{updated}}) + \\epsilon_1 \\cdot H_1$\n\nUpdate: $\\hat{Q}_1^*(1, W) = \\text{expit}(\\text{logit}(\\hat{Q}_1^{\\text{updated}}) + \\hat{\\epsilon}_1 \\cdot H_1)$\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Re-fit backwards regression using targeted Q2*\nQ1_mod_updated <- glm(Q2_star ~ A1 + W, family = quasibinomial, data = dat)\n\nQ1_updated <- predict(Q1_mod_updated, type = \"response\")\nQ1_under_a1 <- predict(Q1_mod_updated,\n                        newdata = dat %>% mutate(A1 = 1),\n                        type = \"response\")\n\n# Clever covariate at time 1\nH1 <- (dat$A1 == 1) / g1_hat\n\n# Fluctuation model at time 1\nfluc1 <- glm(Q2_star ~ -1 + offset(logit(Q1_updated)) + H1,\n             family = quasibinomial)\neps1 <- coef(fluc1)\n\n# Final targeted predictions under A1 = 1\nQ1_star <- plogis(logit(Q1_under_a1) + eps1 / g1_hat)\n\ncat(\"Epsilon at time 1:\", round(eps1, 5), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpsilon at time 1: -0.00084 \n```\n\n\n:::\n:::\n\n\n\n\n---\n\n## Phase D: The Final L-TMLE Estimate\n\n:::{.callout-warning}\n## L-TMLE Step 4: Plug In and Estimate\n\nThe L-TMLE estimate of $E[Y(1,1)]$ is the sample mean of the final targeted predictions:\n\n$$\\hat{\\psi}_{\\text{LTMLE}} = \\frac{1}{n} \\sum_{i=1}^{n} \\hat{Q}_1^*(1, W_i)$$\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nltmle_estimate <- mean(Q1_star)\n\ncat(\"========================================\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n========================================\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"L-TMLE estimate E[Y(1,1)]:\", round(ltmle_estimate, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nL-TMLE estimate E[Y(1,1)]: 0.4011 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True E[Y(1,1)]:           \", round(true_EY_11, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue E[Y(1,1)]:            0.3945 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Naive G-comp (no targeting):\", round(mean(Q1_A1is1), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNaive G-comp (no targeting): 0.3899 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"========================================\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n========================================\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n# 5. How Does Targeting Remove Time-Dependent Confounding?\n\nThis is the most important section. Let us build intuition for **why** the backwards targeting procedure works.\n\n## The problem, restated\n\nIn our statin example:\n\n- Year-1 statins ($A_1$) lower cholesterol ($L_1$)\n- Lower cholesterol ($L_1$) makes year-2 statins ($A_2$) less likely\n- Lower cholesterol ($L_1$) also directly lowers heart attack risk ($Y$)\n\nA naive regression that conditions on $L_1$ blocks the indirect effect $A_1 \\to L_1 \\to Y$ and introduces collider bias. But not adjusting for $L_1$ confounds the $A_2 \\to Y$ effect.\n\n## How L-TMLE handles this\n\n:::{.callout-tip}\n## The Backwards Regression Solves the \"Condition or Not?\" Dilemma\n\n**At time 2:** We fit $E[Y \\mid A_2, L_1, A_1, W]$ and DO condition on $L_1$. This correctly adjusts for the confounding of $A_2$ by $L_1$.\n\n**At time 1:** We regress the time-2 predictions on $(A_1, W)$ and do NOT condition on $L_1$. Instead, we let $L_1$ vary freely. This preserves the indirect effect $A_1 \\to L_1 \\to Y$.\n\nBy working backwards, we condition on $L_1$ only where it is needed (as a confounder of $A_2$) and integrate it out where conditioning would be harmful (as a mediator of $A_1$).\n:::\n\n## What does targeting add?\n\nThe sequential regression (G-computation) above handles the time-ordering correctly, but it relies on **all** outcome models being correctly specified. The targeting steps add robustness:\n\n:::{.callout-important}\n## Why Targeting Matters\n\n**At time 2:** The fluctuation step using $H_2 = \\frac{I(A_1=1, A_2=1)}{g_1 \\cdot g_2}$ corrects the outcome model using information from the treatment models. Even if $\\hat{Q}_2$ is somewhat misspecified, the clever covariate steers the predictions toward the correct value for our target estimand.\n\n**At time 1:** The fluctuation step using $H_1 = \\frac{I(A_1=1)}{g_1}$ further corrects $\\hat{Q}_1$ using the time-1 treatment model.\n\nTogether, these targeting steps achieve **sequential double robustness**: the final estimate is consistent if, at each time point, either the outcome model OR the treatment model is correctly specified.\n:::\n\n## The double robustness property\n\nFor point-treatment TMLE, double robustness means: consistent if $\\hat{Q}$ OR $\\hat{g}$ is correct.\n\nFor L-TMLE, it extends to: consistent if **at each time step**, either the $\\hat{Q}_t$ or $\\hat{g}_t$ is correct. Specifically:\n\n| If correct at time 2 | If correct at time 1 | L-TMLE is... |\n|---|---|---|\n| $Q_2$ only | $Q_1$ only | Consistent |\n| $g_2$ only | $g_1$ only | Consistent |\n| $Q_2$ only | $g_1$ only | Consistent |\n| $g_2$ only | $Q_1$ only | Consistent |\n| Neither | Anything | Not guaranteed |\n\nThis is a much stronger property than simply requiring one global model to be correct.\n\n---\n\n# 6. Visualizing the Difference\n\n## Compare estimators\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- Naive: condition on everything ---\nnaive_mod <- glm(Y ~ A1 + A2 + L1 + W, family = binomial, data = dat)\nnaive_pred <- predict(naive_mod,\n                       newdata = dat %>% mutate(A1 = 1, A2 = 1),\n                       type = \"response\")\nnaive_est <- mean(naive_pred)\n\n# --- Standard IPTW with cumulative weights ---\n# Weight = 1 / P(observed treatment sequence)\niptw_w <- ifelse(dat$A1 == 1 & dat$A2 == 1,\n                 1 / (g1_hat * g2_hat),\n                 0)\n# Horvitz-Thompson\niptw_est <- sum(iptw_w * dat$Y) / sum(iptw_w)\n\n# --- Compare ---\ncomparison <- tibble(\n  Method = c(\"True E[Y(1,1)]\",\n             \"Naive regression (conditions on L1)\",\n             \"Sequential G-computation (no targeting)\",\n             \"Longitudinal IPTW\",\n             \"L-TMLE\"),\n  Estimate = round(c(true_EY_11, naive_est, mean(Q1_A1is1), iptw_est, ltmle_estimate), 4)\n)\ncomparison\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 2\n  Method                                  Estimate\n  <chr>                                      <dbl>\n1 True E[Y(1,1)]                             0.394\n2 Naive regression (conditions on L1)        0.366\n3 Sequential G-computation (no targeting)    0.390\n4 Longitudinal IPTW                          0.399\n5 L-TMLE                                     0.401\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(comparison, aes(x = Estimate,\n                       y = factor(Method, levels = rev(comparison$Method)))) +\n  geom_point(size = 3, color = c(\"red\", \"#4477AA\", \"#228833\", \"#EE6677\", \"#AA3377\")) +\n  geom_vline(xintercept = true_EY_11, linetype = \"dashed\", color = \"red\", linewidth = 0.5) +\n  annotate(\"text\", x = true_EY_11, y = 5.4, label = \"Truth\", color = \"red\", size = 3) +\n  labs(x = \"Estimated E[Y(1,1)]\", y = \"\", title = \"How Well Does Each Estimator Recover the Truth?\") +\n  theme_minimal() +\n  theme(axis.text.y = element_text(size = 10))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Use of `comparison$Method` is discouraged.\nℹ Use `Method` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](03-09-illustrated-ltmle_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n---\n\n# 7. The Full Picture: Point-Treatment vs. Longitudinal TMLE\n\n```\nPOINT-TREATMENT TMLE                    LONGITUDINAL TMLE (2 time points)\n\n┌─────────────────────┐               ┌─────────────────────────────────┐\n│ Step 1: Fit Q(A,W)  │               │ Step 2a: Fit Q₂(A₂,L₁,A₁,W)   │\n│ = E[Y | A, W]       │               │ = E[Y | A₂, L₁, A₁, W]        │\n└──────────┬──────────┘               └──────────────┬──────────────────┘\n           │                                         │\n           │                          ┌──────────────▼──────────────────┐\n           │                          │ Step 3a: Target Q₂ using H₂    │\n           │                          │ (clever cov with cumulative g)  │\n           │                          └──────────────┬──────────────────┘\n           │                                         │\n           │                          ┌──────────────▼──────────────────┐\n           │                          │ Step 2b: Fit Q₁(A₁,W)          │\n           │                          │ = E[Q₂* | A₁, W]               │\n           │                          │ (uses TARGETED Q₂ as outcome)   │\n           │                          └──────────────┬──────────────────┘\n           │                                         │\n┌──────────▼──────────┐               ┌──────────────▼──────────────────┐\n│ Step 2: Fit g(W)    │               │ Step 1: Fit g₁(W) and          │\n│ = P(A=1|W)          │               │         g₂(L₁,A₁,W)           │\n└──────────┬──────────┘               └──────────────┬──────────────────┘\n           │                                         │\n┌──────────▼──────────┐               ┌──────────────▼──────────────────┐\n│ Step 3: Clever cov  │               │ Step 3b: Target Q₁ using H₁    │\n│ H = I(A=1)/g        │               │ (clever cov with g₁ only)       │\n└──────────┬──────────┘               └──────────────┬──────────────────┘\n           │                                         │\n┌──────────▼──────────┐               ┌──────────────▼──────────────────┐\n│ Step 4: Target Q    │               │ Step 4: Average Q₁*            │\n│ and estimate ATE    │               │ = E[Y(1,1)] via L-TMLE          │\n└─────────────────────┘               └─────────────────────────────────┘\n```\n\nThe parallel structure is clear: L-TMLE repeats the \"model → target\" cycle at each time point, working backwards from the outcome.\n\n---\n\n# 8. Scaling Up: More Than Two Time Points\n\nWith $T$ time points, L-TMLE follows the same pattern:\n\n1. **Estimate treatment mechanisms** $\\hat{g}_t$ for $t = 1, \\ldots, T$\n2. **Start at time $T$:** Fit $\\hat{Q}_T = E[Y \\mid \\bar{A}_T, \\bar{L}_T, W]$\n3. **Target $\\hat{Q}_T$** using $H_T = I(\\bar{A}_T = \\bar{a}_T) / \\prod_{s=1}^{T} \\hat{g}_s$\n4. **Move to time $T-1$:** Fit $\\hat{Q}_{T-1} = E[\\hat{Q}_T^* \\mid \\bar{A}_{T-1}, \\bar{L}_{T-1}, W]$\n5. **Target $\\hat{Q}_{T-1}$** using $H_{T-1} = I(\\bar{A}_{T-1} = \\bar{a}_{T-1}) / \\prod_{s=1}^{T-1} \\hat{g}_s$\n6. **Repeat** until $\\hat{Q}_1^*$ is obtained\n7. **Estimate:** $\\hat{\\psi} = \\frac{1}{n}\\sum_i \\hat{Q}_1^*(1, W_i)$\n\nEach backwards step:\n\n- Conditions on the time-varying confounder $L_t$ (to deconfound $A_{t+1}$)\n- Then integrates $L_t$ out in the next regression (to preserve $A_t$'s mediated effect)\n- Targets the estimate using the cumulative treatment probability through time $t$\n\n:::{.callout-caution}\n## Practical Warning: Cumulative Products\nIn longitudinal IPTW, the weights are products of $T$ terms: $w = \\prod_{t=1}^{T} 1/g_t$. With many time points, these products can become extremely large or small, making IPTW unstable.\n\nL-TMLE avoids this by incorporating the treatment information through **targeting steps** rather than **multiplicative weights**. The clever covariates still involve the cumulative product, but they enter through a logistic fluctuation model that is far more numerically stable than multiplying raw weights.\n\nThis is a major practical advantage of L-TMLE over longitudinal IPTW.\n:::\n\n---\n\n# 9. Using the `ltmle` Package\n\nThe `ltmle` R package automates the entire procedure above. Here is how you would use it for our two-time-point example:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ltmle)\n\n# Data must be in temporal order: W, A1, L1, A2, Y\ndat_ltmle <- data.frame(\n  W  = dat$W,\n  A1 = dat$A1,\n  L1 = dat$L1,\n  A2 = dat$A2,\n  Y  = dat$Y\n)\n\n# Specify the \"always treat\" intervention\nresult <- ltmle(\n  data = dat_ltmle,\n  Anodes = c(\"A1\", \"A2\"),           # treatment nodes\n  Lnodes = c(\"L1\"),                  # time-varying covariate nodes\n  Ynodes = \"Y\",                      # outcome\n  abar = c(1, 1),                    # intervention: always treat\n  SL.library = c(\"SL.glm\", \"SL.mean\")  # Super Learner library\n)\n\nsummary(result)\n```\n:::\n\n\n\n\nThe package handles all the backwards regressions, clever covariates, fluctuation steps, and influence-curve inference automatically.\n\nFor more flexible interventions (stochastic, dynamic), the `lmtp` package extends this framework:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmtp)\n\nresult_lmtp <- lmtp_tmle(\n  data = dat,\n  trt = c(\"A1\", \"A2\"),\n  outcome = \"Y\",\n  baseline = \"W\",\n  time_vary = list(\"L1\"),\n  shift = function(data, trt) rep(1, nrow(data)),  # always treat\n  outcome_type = \"binomial\",\n  learners_outcome = c(\"SL.glm\", \"SL.mean\"),\n  learners_trt = c(\"SL.glm\", \"SL.mean\"),\n  folds = 5\n)\n\nresult_lmtp\n```\n:::\n\n\n\n\n---\n\n# 10. Summary: What Makes L-TMLE Special\n\n:::{.callout-note}\n## The Three Key Ideas\n\n**1. Backwards sequential regression** handles time-varying confounding correctly by conditioning on $L_t$ where it is a confounder and integrating it out where it is a mediator. No standard regression can do both.\n\n**2. Targeting at each time step** makes the estimator doubly robust at every stage. Even if some outcome models are misspecified, the treatment mechanism models can compensate (and vice versa).\n\n**3. Working on the logit scale** ensures all predictions stay bounded between 0 and 1, and avoids the extreme-weight instability of longitudinal IPTW.\n:::\n\n| Feature | Standard regression | Longitudinal IPTW | Longitudinal G-comp | L-TMLE |\n|---------|--------------------|--------------------|---------------------|--------|\n| Handles time-varying confounding | No | Yes | Yes | Yes |\n| Avoids extreme weights | Yes | No | Yes | Yes |\n| Doubly robust | No | No | No | Yes |\n| Works with machine learning | Partially | Partially | Partially | Yes |\n| Valid inference | Standard SE | Sandwich SE | Bootstrap | Influence curve |\n| Respects bounds | Depends | N/A | Not guaranteed | Yes |\n\n---\n\n# Key Takeaways\n\n1. **Time-varying confounders affected by prior treatment** create a problem that no standard regression can solve. You need g-methods.\n\n2. **L-TMLE extends point-treatment TMLE** by iterating the \"model → target\" cycle backwards through time. If you understand the KH Stats 4-step TMLE, L-TMLE is the same idea repeated at each time point.\n\n3. **The backwards regression** handles the \"condition or not?\" dilemma: condition on $L_t$ to deconfound $A_{t+1}$, then integrate it out to preserve $A_t$'s mediated effect.\n\n4. **Targeting at each step** makes L-TMLE doubly robust at every time point, not just globally.\n\n5. **L-TMLE avoids the extreme weight problem** of longitudinal IPTW by incorporating treatment information through logistic fluctuation models rather than multiplicative weights.\n\n6. **Packages `ltmle` and `lmtp`** automate the entire procedure, including Super Learner integration and influence-curve inference.\n\n---\n\n*This guide was inspired by [Kat Hoffman's Illustrated Guide to TMLE](https://www.khstats.com/blog/tmle/tutorial) and her [visual guides for causal inference](https://github.com/kathoffman/causal-inference-visual-guides). The backwards sequential regression framework for L-TMLE is described in [Petersen, Schwab, Gruber, Blaser, Schomaker, and van der Laan (2014)](https://pmc.ncbi.nlm.nih.gov/articles/PMC4405134/) and [Schnitzer et al.](https://putnamds.com/LTMLE_Schnitzer.pdf)*\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26200)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3 forcats_1.0.0   stringr_1.6.0   dplyr_1.1.4    \n [5] purrr_1.0.2     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n [9] ggplot2_3.5.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6      jsonlite_2.0.0    compiler_4.4.2    tidyselect_1.2.1 \n [5] scales_1.3.0      yaml_2.3.10       fastmap_1.2.0     R6_2.6.1         \n [9] labeling_0.4.3    generics_0.1.3    knitr_1.49        htmlwidgets_1.6.4\n[13] munsell_0.5.1     pillar_1.9.0      tzdb_0.4.0        rlang_1.1.6      \n[17] utf8_1.2.4        stringi_1.8.7     xfun_0.49         timechange_0.3.0 \n[21] cli_3.6.5         withr_3.0.2       magrittr_2.0.3    digest_0.6.37    \n[25] grid_4.4.2        rstudioapi_0.17.1 hms_1.1.3         lifecycle_1.0.4  \n[29] vctrs_0.6.5       evaluate_1.0.5    glue_1.8.0        farver_2.1.2     \n[33] fansi_1.0.6       colorspace_2.1-1  rmarkdown_2.29    tools_4.4.2      \n[37] pkgconfig_2.0.3   htmltools_0.5.8.1\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n## Software Implementation (R)\n\nThis example uses the `ltmle` package for a minimal, well-commented **longitudinal TMLE** that mirrors the visual walkthrough in this chapter: backwards sequential regression with targeting at each time step.\n\n- Simulate 2-time-point data with treatment–confounder feedback\n- Use `ltmle()` with `abar` for static \"always treat\" vs. \"never treat\"\n- Print the targeted estimate with influence-curve-based confidence interval\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nn <- 400\nW     <- rnorm(n)\nA_1   <- rbinom(n, 1, plogis(0.3 * W))\nL_1   <- rnorm(n, mean = 0.5 * A_1 + 0.4 * W, sd = 0.8)\nA_2   <- rbinom(n, 1, plogis(-0.2 + 0.3 * L_1 + 0.2 * W))\nY     <- rbinom(n, 1, plogis(-1 + 0.4 * A_1 + 0.4 * A_2 + 0.3 * L_1 + 0.3 * W))\n\ndat <- data.frame(W = W, A_1 = A_1, L_1 = L_1, A_2 = A_2, Y = Y)\n\nif (requireNamespace(\"ltmle\", quietly = TRUE)) {\n  library(ltmle)\n\n  result <- ltmle(\n    data = dat,\n    Anodes  = c(\"A_1\", \"A_2\"),\n    Lnodes  = \"L_1\",\n    Ynodes  = \"Y\",\n    abar = list(treatment = c(1, 1), control = c(0, 0)),\n    SL.library = \"glm\"\n  )\n\n  cat(\"── Longitudinal TMLE (always treat vs never treat) ──\\n\")\n  print(summary(result))\n\n  cat(\"\\nThis mirrors the backwards-regression + targeting\\n\")\n  cat(\"walkthrough illustrated earlier in this chapter.\\n\")\n} else {\n  message(\"Install the 'ltmle' package:  install.packages('ltmle')\")\n}\n```\n:::\n",
    "supporting": [
      "03-09-illustrated-ltmle_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}