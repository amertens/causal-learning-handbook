{
  "hash": "de92946454661e56d8dd710148ceb03b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 3.7: Imperfect Adherence, Time-Varying Confounding, and Longitudinal TMLE\"\nsubtitle: \"An illustrated guide to why conditioning on adherence biases causal effects, and how longitudinal TMLE and LMTP address this\"\nformat: html\n---\n\n\n\n\n# Chapter 3.7: Imperfect Adherence in Longitudinal Data\n*Why conditioning on adherence biases causal effects, and how longitudinal TMLE and LMTP address this*\n\nIn randomized trials, the intent-to-treat (ITT) analysis compares groups as randomized, regardless of whether patients actually took their medication. But in pharmacoepidemiology, we often want to know:\n\n> What would have happened if patients had actually followed a particular treatment strategy?\n\nThis chapter addresses **imperfect adherence** --- a central challenge in longitudinal causal inference. We show:\n\n- Why standard regression conditioning on time-varying adherence introduces bias\n- How the causal roadmap handles time-varying treatments and confounders\n- How to define realistic adherence-based estimands\n- How to implement longitudinal TMLE and LMTP to estimate effects under hypothetical adherence patterns\n\nThe motivating example: **antiretroviral therapy (ART) adherence and HIV virologic suppression**.\n\n---\n\n# 1. Clinical Motivation\n\n## The Setting\n\nA public health agency wants to evaluate antiretroviral therapy strategies for people living with HIV. The key question:\n\n> **What is the probability of virologic suppression (HIV RNA < 200 copies/mL) at 12 months if all patients maintained high adherence to their ART regimen, compared to the observed adherence pattern?**\n\n### Why this question is hard\n\nIn practice, ART adherence varies over time due to:\n\n- **Side effects** (nausea, fatigue) that reduce adherence\n- **Viral load response** --- patients who achieve suppression may become less diligent\n- **CD4 count changes** --- worsening health may change both adherence and outcomes\n- **Mental health** and substance use --- confounders that affect both adherence and outcomes\n\nCritically, adherence at each time point is influenced by time-varying health status, and time-varying health status is itself influenced by past adherence. This creates **time-varying confounding affected by prior treatment** --- a situation where standard methods fail.\n\n### Target population\nAdults living with HIV initiating a new ART regimen\n\n### Treatment strategies\n- **Strategy 1:** Maintain high adherence ($\\geq$ 95% of doses) at every monthly interval\n- **Strategy 0:** Natural (observed) adherence course\n\n### Outcome\nVirologic suppression (HIV RNA < 200 copies/mL) at 12 months\n\n### Intercurrent events\n- Loss to follow-up (informative censoring)\n- Treatment switching\n- Death\n\n### The decision\nShould adherence support programs be intensified? How much improvement in suppression could be expected from perfect adherence?\n\n---\n\n# 2. The Problem with Conditioning on Adherence\n\nBefore introducing the correct methods, let us understand **why standard approaches fail**.\n\n## Why not just adjust for adherence in a regression?\n\nConsider a simple model:\n$$\n\\text{logit}\\big(P(Y = 1)\\big) = \\beta_0 + \\beta_1 \\cdot \\text{Cumulative Adherence} + \\beta_2 \\cdot \\text{Baseline CD4}\n$$\n\nThis conditions on adherence. But why is that a problem?\n\n:::{.callout-caution}\n## The Conditioning-on-Adherence Trap\n\nThe problem is that adherence is **affected by prior health status**, which also affects the outcome. Adherence is simultaneously:\n\n- A **mediator** (treatment $\\to$ adherence $\\to$ outcome)\n- A **collider** when conditioned on (because both health status and unmeasured factors affect it)\n\nConditioning on adherence opens **backdoor paths** through time-varying confounders, introducing selection bias. This is a fundamental result from causal inference theory.\n\n**Think of it this way:** If you compare patients who adhered well vs. poorly, you are not comparing like-with-like. Patients who adhered poorly may have done so *because* they were sicker --- and that sickness, not the low adherence, may explain worse outcomes. Conditioning on adherence mixes up the reason for adherence with the effect of adherence.\n:::\n\n## Visual intuition\n\n:::{.callout-warning}\n## The Collider Problem in a Diagram\n\n```\n    CD4(t-1) -----> Adherence(t) -----> CD4(t) -----> Outcome\n       |                ^                  |\n       |                |                  |\n       +---------> Adherence(t) <----------+\n                     (collider when conditioned on)\n```\n\nWhen you condition on adherence at time $t$, you induce an association between CD4 at time $t-1$ and unmeasured factors that affect adherence --- biasing the effect estimate.\n\n**Why does this matter?** Once you \"hold adherence fixed\" in a regression, you create a spurious link between everything that caused adherence. This is collider bias, and it distorts the causal effect estimate in unpredictable ways --- sometimes making the treatment look more effective, sometimes less.\n:::\n\n## A common mistake: GEE models\n\nGeneralized Estimating Equations (GEE) are popular for longitudinal data, but a GEE model that adjusts for time-varying covariates **and** conditions on time-varying treatment (adherence) will produce biased estimates for causal effects when time-varying confounders are affected by prior treatment.\n\n**The solution:** Use g-methods (g-computation, IPTW, or TMLE) that properly handle the time-ordering of treatment, confounders, and outcome.\n\n---\n\n# 3. Causal Roadmap for Longitudinal Data\n\n:::{.callout-note}\n## Step 1: Define the Causal Question\n\n**Plain language:** What would the 12-month virologic suppression rate be if every patient maintained high adherence, compared to the naturally observed adherence pattern?\n\n**Counterfactual estimand:**\n\nLet $\\bar{A} = (A_1, A_2, \\ldots, A_T)$ denote the adherence history. Define two regimes:\n\n- **Always high adherence:** $d_1: A_t = 1$ for all $t$ (deterministic static regime)\n- **Natural course:** $d_0:$ treatment follows the observed distribution\n\nThe estimand is:\n\n$$\n\\psi = E\\big[Y(\\bar{a} = \\bar{1})\\big] - E\\big[Y^{\\text{obs}}\\big]\n$$\n\nwhere $Y(\\bar{a} = \\bar{1})$ is the potential outcome under the always-high-adherence regime.\n\nWe can also define **stochastic interventions** that shift the probability of high adherence (e.g., increase it by 20%) rather than forcing it to 100% --- more realistic and less prone to positivity violations.\n:::\n\n:::{.callout-note}\n## Step 2: Specify the Causal Model\n\nThe longitudinal data structure for patient $i$ at time $t$:\n\n$$\nO_i = \\big(W_i, \\; L_{i,1}, A_{i,1}, C_{i,1}, \\; L_{i,2}, A_{i,2}, C_{i,2}, \\; \\ldots, \\; L_{i,T}, A_{i,T}, C_{i,T}, \\; Y_i\\big)\n$$\n\nWhere:\n\n- $W$: baseline covariates (age, sex, baseline CD4, baseline viral load, comorbidities)\n- $L_t$: time-varying covariates at time $t$ (CD4 count, viral load, symptoms, mental health score)\n- $A_t$: adherence indicator at time $t$ (1 = high adherence, 0 = low)\n- $C_t$: censoring at time $t$ (1 = lost to follow-up, 0 = still observed)\n- $Y$: virologic suppression at 12 months\n\n**The key causal feature:** $L_t$ is affected by past treatment $\\bar{A}_{t-1}$ and also affects future treatment $A_t$ and the outcome $Y$. This is time-varying confounding affected by prior treatment.\n:::\n\n:::{.callout-warning}\n## Step 3: State the Assumptions\n\nThese assumptions are critical. If they fail, even the best estimator will give biased answers.\n\n**Sequential exchangeability:**\n$$\nY(\\bar{a}) \\perp\\!\\!\\!\\perp A_t \\mid \\bar{L}_t, \\bar{A}_{t-1}, W \\quad \\text{for all } t\n$$\nAt each time point, treatment is as-good-as-random given the observed history. This is the longitudinal analogue of \"no unmeasured confounding.\"\n\n**Sequential positivity:**\n$$\nP(A_t = a \\mid \\bar{L}_t, \\bar{A}_{t-1}, W) > 0 \\quad \\text{for all } t, a\n$$\nEvery adherence level must be possible at every time point for every covariate history. Violations occur when certain health states make adherence nearly deterministic.\n\n**Consistency:**\nThe observed outcome matches the potential outcome under the treatment actually received.\n\n**Independent censoring (conditional):**\n$$\nY(\\bar{a}) \\perp\\!\\!\\!\\perp C_t \\mid \\bar{L}_t, \\bar{A}_{t-1}, W \\quad \\text{for all } t\n$$\nCensoring is independent of potential outcomes given the measured history.\n\n**Why does this matter?** If unmeasured factors (e.g., substance use severity, housing instability) drive both adherence and outcomes, sequential exchangeability fails. If some patient subgroups never achieve high adherence, positivity fails. Both scenarios lead to biased estimates, regardless of the statistical method used.\n:::\n\n:::{.callout-tip}\n## Step 4: Map to a Statistical Estimand\n\nUnder the above assumptions, the G-computation formula for the longitudinal setting is:\n\n$$\nE[Y(\\bar{a})] = \\sum_{\\bar{l}} E[Y \\mid \\bar{A} = \\bar{a}, \\bar{L} = \\bar{l}, W] \\prod_t P(L_t = l_t \\mid \\bar{A}_{t-1}, \\bar{L}_{t-1}, W)\n$$\n\nThis is a high-dimensional integral that iterates over all possible covariate histories --- practically estimated via sequential regression or TMLE.\n\n**The good news:** This formula tells us exactly what we need to estimate. The assumptions above bridge the gap from the causal question (what *would* happen) to a statistical quantity we can estimate from observed data (what *did* happen, reweighted or modeled appropriately).\n:::\n\n:::{.callout-important}\n## Step 5: Choose an Estimation Strategy\n\n| Method | Approach | Limitation |\n|--------|----------|------------|\n| GEE with time-varying covariates | Conditions on $L_t$ directly | Biased under time-varying confounding |\n| Marginal structural model (MSM) via IPTW | Reweights by cumulative propensity | Products of weights become extreme |\n| Longitudinal G-computation | Sequential outcome regression | Fully parametric; error compounds |\n| Longitudinal TMLE (LTMLE) | Sequential targeting | Best properties but complex |\n| LMTP | Modified treatment policies with TMLE | Handles stochastic interventions naturally |\n\nWe implement the last two approaches.\n\n**Why LTMLE and LMTP?** These methods combine the strengths of outcome modeling and propensity weighting. They handle the feedback loop between treatment and confounders, avoid extreme weights, provide doubly robust estimation, and support flexible machine learning for nuisance parameter estimation.\n:::\n\n---\n\n# 4. Data Simulation\n\nWe simulate a realistic longitudinal HIV cohort with 6 monthly time points.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'stringr' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.6.0\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(2026)\nn  <- 3000\nT_max <- 6  # 6 monthly intervals (representing a 12-month follow-up, bi-monthly)\n\n# --- Baseline covariates ---\nW_age     <- rnorm(n, mean = 38, sd = 10)\nW_male    <- rbinom(n, 1, 0.65)\nW_cd4_bl  <- rnorm(n, mean = 350, sd = 150)  # baseline CD4\nW_cd4_bl  <- pmax(50, W_cd4_bl)               # floor at 50\nW_vl_bl   <- rnorm(n, mean = 4.5, sd = 0.8)  # baseline log10 viral load\nW_depress <- rbinom(n, 1, 0.30)               # baseline depression\n\n# --- Containers for longitudinal data ---\nL_cd4  <- matrix(NA, n, T_max)   # time-varying CD4\nL_sympt <- matrix(NA, n, T_max)  # time-varying symptom burden\nA_adh  <- matrix(NA, n, T_max)   # adherence at each time\nC_cens <- matrix(0, n, T_max)    # censoring indicator\nalive  <- rep(TRUE, n)           # tracking who is still observed\n\n# --- Simulate longitudinal process ---\nfor (t in 1:T_max) {\n\n  # Previous adherence (for t=1, use a starting value)\n  if (t == 1) {\n    A_prev <- rbinom(n, 1, 0.7)  # initial adherence tendency\n  } else {\n    A_prev <- A_adh[, t - 1]\n    A_prev[is.na(A_prev)] <- 0\n  }\n\n  # Previous CD4 (for t=1, use baseline)\n  cd4_prev <- if (t == 1) W_cd4_bl else L_cd4[, t - 1]\n  cd4_prev[is.na(cd4_prev)] <- W_cd4_bl[is.na(cd4_prev)]\n\n  # --- Time-varying CD4 ---\n  # CD4 increases with adherence, depends on history\n  L_cd4[, t] <- pmax(50,\n    cd4_prev +\n    rnorm(n, mean = 30 * A_prev - 10 * (1 - A_prev), sd = 40) +\n    -0.5 * (W_age - 38) +\n    20 * W_male\n  )\n\n  # --- Time-varying symptom burden ---\n  # Higher with low CD4, depression; reduced by adherence\n  L_sympt[, t] <- rbinom(n, 1,\n    plogis(-1.5 + -0.003 * L_cd4[, t] + 0.8 * W_depress + -0.5 * A_prev +\n           0.01 * (W_age - 38))\n  )\n\n  # --- Adherence at time t ---\n  # Affected by: symptoms (reduce adherence), CD4 response (feedback),\n  # prior adherence (habit), depression, age\n  lp_adh <- 0.5 +\n    -0.8 * L_sympt[, t] +\n    0.002 * (L_cd4[, t] - 350) +\n    0.6 * A_prev +\n    -0.7 * W_depress +\n    0.01 * (W_age - 38) +\n    0.2 * W_male\n\n  A_adh[, t] <- rbinom(n, 1, plogis(lp_adh))\n\n  # --- Censoring ---\n  # More likely with low CD4, symptoms, low adherence\n  lp_cens <- -4.0 +\n    -0.002 * L_cd4[, t] +\n    0.5 * L_sympt[, t] +\n    -0.3 * A_adh[, t] +\n    0.5 * W_depress\n\n  C_cens[, t] <- rbinom(n, 1, plogis(lp_cens)) * alive\n  alive <- alive & (C_cens[, t] == 0)\n\n  # Set future values to NA for censored individuals\n  if (t < T_max) {\n    A_adh[!alive, (t+1):T_max] <- NA\n    L_cd4[!alive, (t+1):T_max] <- NA\n    L_sympt[!alive, (t+1):T_max] <- NA\n    C_cens[!alive, (t+1):T_max] <- NA\n  }\n}\n\n# --- Outcome: virologic suppression at end of follow-up ---\n# Depends on cumulative adherence, final CD4, baseline VL\ncum_adh <- rowMeans(A_adh, na.rm = TRUE)\nfinal_cd4 <- apply(L_cd4, 1, function(x) {\n  valid <- x[!is.na(x)]\n  if (length(valid) > 0) tail(valid, 1) else NA\n})\n\nlp_outcome <- -1.0 +\n  2.5 * cum_adh +\n  0.003 * (final_cd4 - 350) +\n  -0.6 * W_vl_bl +\n  -0.3 * W_depress +\n  0.02 * W_male +\n  0.8 * cum_adh * (final_cd4 > 400)  # interaction: adherence more effective with good immune response\n\nY_supp <- rbinom(n, 1, plogis(lp_outcome))\n# Censored individuals: outcome is missing\nY_supp[!alive] <- NA\n\ncat(\"Sample size:\", n, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample size: 3000 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Censored by end:\", sum(!alive), \"(\", round(100*mean(!alive), 1), \"%)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCensored by end: 152 ( 5.1 %)\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Among uncensored, suppression rate:\", round(mean(Y_supp[alive]), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAmong uncensored, suppression rate: 0.344 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Mean cumulative adherence:\", round(mean(cum_adh, na.rm=TRUE), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean cumulative adherence: 0.709 \n```\n\n\n:::\n:::\n\n\n\n\n### True effect of perfect adherence\n\nWe can compute the counterfactual suppression rate under always-high adherence from the structural model. Since the true DGP is complex (with feedback), we approximate using simulation:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate counterfactual under always-high adherence (A_t = 1 for all t)\nset.seed(2026)\ncf_cd4 <- matrix(NA, n, T_max)\ncf_sympt <- matrix(NA, n, T_max)\n\nfor (t in 1:T_max) {\n  cd4_prev_cf <- if (t == 1) W_cd4_bl else cf_cd4[, t - 1]\n  A_prev_cf <- 1  # always high adherence\n\n  cf_cd4[, t] <- pmax(50,\n    cd4_prev_cf +\n    rnorm(n, mean = 30 * 1 - 10 * 0, sd = 40) +\n    -0.5 * (W_age - 38) + 20 * W_male\n  )\n\n  cf_sympt[, t] <- rbinom(n, 1,\n    plogis(-1.5 + -0.003 * cf_cd4[, t] + 0.8 * W_depress + -0.5 * 1 +\n           0.01 * (W_age - 38))\n  )\n}\n\ncf_cum_adh <- 1.0  # perfect adherence\ncf_final_cd4 <- cf_cd4[, T_max]\n\nlp_cf <- -1.0 +\n  2.5 * cf_cum_adh +\n  0.003 * (cf_final_cd4 - 350) +\n  -0.6 * W_vl_bl +\n  -0.3 * W_depress +\n  0.02 * W_male +\n  0.8 * cf_cum_adh * (cf_final_cd4 > 400)\n\ntrue_suppression_perfect <- mean(plogis(lp_cf))\ntrue_suppression_observed <- mean(plogis(lp_outcome), na.rm = TRUE)\n\ncat(\"True suppression under always-high adherence:\", round(true_suppression_perfect, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue suppression under always-high adherence: 0.542 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True suppression under observed adherence:   \", round(true_suppression_observed, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue suppression under observed adherence:    0.336 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True effect of perfect adherence:             \", round(true_suppression_perfect - true_suppression_observed, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue effect of perfect adherence:              0.206 \n```\n\n\n:::\n:::\n\n\n\n\n### Prepare data in long format\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Build a wide-format dataframe for analysis\ndat_wide <- tibble(\n  id = 1:n,\n  age = W_age,\n  male = W_male,\n  cd4_bl = W_cd4_bl,\n  vl_bl = W_vl_bl,\n  depress = W_depress\n)\n\nfor (t in 1:T_max) {\n  dat_wide[[paste0(\"cd4_\", t)]]   <- L_cd4[, t]\n  dat_wide[[paste0(\"sympt_\", t)]] <- L_sympt[, t]\n  dat_wide[[paste0(\"A_\", t)]]     <- A_adh[, t]\n  dat_wide[[paste0(\"C_\", t)]]     <- C_cens[, t]\n}\ndat_wide$Y <- Y_supp\n\n# For long format (useful for some analyses)\ndat_long <- dat_wide %>%\n  pivot_longer(\n    cols = matches(\"^(cd4|sympt|A|C)_\\\\d+$\"),\n    names_to = c(\".value\", \"time\"),\n    names_pattern = \"(\\\\w+)_(\\\\d+)\"\n  ) %>%\n  mutate(time = as.integer(time)) %>%\n  arrange(id, time)\n\nglimpse(dat_long)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 18,000\nColumns: 12\n$ id      <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4,…\n$ age     <dbl> 43.20589, 43.20589, 43.20589, 43.20589, 43.20589, 43.20589, 27…\n$ male    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ cd4_bl  <dbl> 105.6647, 105.6647, 105.6647, 105.6647, 105.6647, 105.6647, 17…\n$ vl_bl   <dbl> 3.480404, 3.480404, 3.480404, 3.480404, 3.480404, 3.480404, 4.…\n$ depress <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Y       <int> 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ time    <int> 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3,…\n$ cd4     <dbl> 129.7946, 142.5491, 252.5217, 315.5199, 421.1039, 450.3287, 23…\n$ sympt   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ A       <int> 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,…\n$ C       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n# 5. The Naive (Biased) Approach\n\nFirst, let us see what happens if we ignore the time-varying confounding structure.\n\n## 5A. Cross-sectional regression on cumulative adherence\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Only among uncensored\ndat_complete <- dat_wide %>% filter(!is.na(Y))\ndat_complete$cum_adh <- cum_adh[alive]\n\nnaive_gee <- glm(Y ~ cum_adh + cd4_bl + vl_bl + depress + age + male,\n                 family = binomial, data = dat_complete)\n\n# Predicted suppression at 100% vs 50% adherence\nnd_perfect <- dat_complete %>% mutate(cum_adh = 1.0)\nnd_half    <- dat_complete %>% mutate(cum_adh = 0.5)\n\nnaive_perfect <- mean(predict(naive_gee, newdata = nd_perfect, type = \"response\"))\nnaive_half    <- mean(predict(naive_gee, newdata = nd_half, type = \"response\"))\n\ncat(\"Naive model:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNaive model:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Predicted suppression at 100% adherence:\", round(naive_perfect, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Predicted suppression at 100% adherence: 0.568 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Predicted suppression at 50% adherence: \", round(naive_half, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Predicted suppression at 50% adherence:  0.159 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Difference:\", round(naive_perfect - naive_half, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Difference: 0.409 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nThis estimate is biased because it conditions on post-treatment variables.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nThis estimate is biased because it conditions on post-treatment variables.\n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-caution}\n## Why Is This Wrong?\n\nThe model treats cumulative adherence as a fixed baseline variable, but adherence is:\n\n1. **Affected by time-varying CD4 and symptoms** (confounders) --- patients who feel worse adhere less, and feeling worse also worsens the outcome directly\n2. **Itself affects future CD4 and symptoms** (it is a treatment) --- taking medication changes the very health indicators that influence future adherence\n3. **A collider** when conditioned on --- because both health status and unmeasured factors drive adherence decisions\n\nThe estimate conflates the causal effect of adherence with the reverse causal effect of health on adherence. You cannot untangle \"does adherence improve outcomes?\" from \"do better outcomes sustain adherence?\" using standard regression.\n\n**The fundamental issue:** Standard regression assumes covariates are fixed. But in a longitudinal feedback loop, today's treatment changes tomorrow's confounders, which change the day after's treatment. No single regression equation can properly account for this temporal cascade.\n:::\n\n---\n\n# 6. IPTW for Marginal Structural Models\n\nThe first correct approach is IPTW, which avoids conditioning on time-varying confounders by reweighting.\n\n### Estimate treatment weights at each time point\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Work in long format; only uncensored observations\ndat_long_uc <- dat_long %>%\n  group_by(id) %>%\n  filter(cumall(!is.na(A))) %>%  # keep complete treatment histories\n  ungroup()\n\n# Propensity model at each time point\n# P(A_t = 1 | history)\ndat_long_uc <- dat_long_uc %>%\n  group_by(id) %>%\n  mutate(\n    A_lag = lag(as.double(A), default = 0.7),  # lagged adherence\n    cd4_lag = lag(cd4, default = first(cd4))\n  ) %>%\n  ungroup()\n\n# Time-specific propensity models\ng_models <- list()\ndat_long_uc$ps_t <- NA_real_\ndat_long_uc$ps_num_t <- NA_real_\n\nfor (t_val in 1:T_max) {\n  idx <- dat_long_uc$time == t_val\n  dat_t <- dat_long_uc[idx, ]\n\n  g_t <- glm(A ~ cd4 + sympt + A_lag + depress + age + male + vl_bl,\n             family = binomial, data = dat_t)\n  g_models[[t_val]] <- g_t\n\n  ps_pred <- predict(g_t, type = \"response\")\n  # Stabilized numerator: P(A_t | baseline, past A)\n  g_num_t <- glm(A ~ A_lag + age + male,\n                 family = binomial, data = dat_t)\n  ps_num <- predict(g_num_t, type = \"response\")\n\n  dat_long_uc$ps_t[idx] <- ifelse(\n    dat_t$A == 1, ps_pred, 1 - ps_pred\n  )\n  dat_long_uc$ps_num_t[idx] <- ifelse(\n    dat_t$A == 1, ps_num, 1 - ps_num\n  )\n}\n\n# Any remaining NA (shouldn't happen, but safety net)\ndat_long_uc$ps_num_t[is.na(dat_long_uc$ps_num_t)] <- 1\n```\n:::\n\n\n\n\n### Compute cumulative weights\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cumulative product of time-specific propensity scores\nweight_df <- dat_long_uc %>%\n  group_by(id) %>%\n  summarise(\n    cum_ps_denom = prod(ps_t, na.rm = TRUE),\n    cum_adh = mean(A, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Stabilized weights (using marginal model in numerator)\nweight_df$sw <- 1 / weight_df$cum_ps_denom\n\n# Merge outcome\nweight_df <- weight_df %>%\n  left_join(dat_wide %>% select(id, Y), by = \"id\") %>%\n  filter(!is.na(Y))\n\n# Truncate extreme weights\nw99 <- quantile(weight_df$sw, 0.99)\nweight_df$sw_trunc <- pmin(weight_df$sw, w99)\n\ncat(\"Weight summary (truncated):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWeight summary (truncated):\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(weight_df$sw_trunc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.473  13.231  28.079  60.936  73.400 565.277 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot weight distribution\nggplot(weight_df, aes(x = sw_trunc)) +\n  geom_histogram(bins = 50, fill = \"#228833\", alpha = 0.7) +\n  labs(\n    x = \"Stabilized cumulative IPTW weight (truncated at 99th pctile)\",\n    y = \"Count\",\n    title = \"Distribution of Longitudinal IPTW Weights\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](03-07-longitudinal-td-confounding_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n:::{.callout-warning}\n## Beware: Extreme Cumulative Weights\n\nCumulative weights from longitudinal IPTW are **products** of time-specific weights. Even with modest individual weights (e.g., each between 0.5 and 2.0), the product over 6 or 12 time points can become extreme.\n\n**Why this happens:** If a patient's observed treatment history is unlikely under the model (e.g., they adhered despite many risk factors for non-adherence), their cumulative weight $w = \\prod_{t=1}^{T} 1/g_t$ can explode. A few such patients can dominate the entire weighted analysis.\n\n**Practical consequences:**\n\n- A single patient with weight 500 can outweigh hundreds of others combined\n- Truncating weights introduces bias but reduces variance --- a delicate trade-off\n- Confidence intervals become unreliable when weights are extreme\n- Weight instability gets worse with more time points\n\nThis is a well-known limitation of longitudinal IPTW and a key reason to prefer TMLE-based methods, which incorporate treatment information through numerically stable logistic fluctuation models rather than multiplicative weights.\n:::\n\n### Weighted outcome model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MSM: weighted regression of outcome on cumulative adherence\nmsm_fit <- glm(Y ~ cum_adh, family = binomial,\n               weights = sw_trunc, data = weight_df)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predicted suppression under perfect vs half adherence\nmsm_perfect <- predict(msm_fit,\n                       newdata = data.frame(cum_adh = 1.0),\n                       type = \"response\")\nmsm_half <- predict(msm_fit,\n                    newdata = data.frame(cum_adh = 0.5),\n                    type = \"response\")\n\ncat(\"IPTW-MSM estimate:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIPTW-MSM estimate:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Suppression at 100% adherence:\", round(msm_perfect, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Suppression at 100% adherence: 1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Suppression at 50% adherence: \", round(msm_half, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Suppression at 50% adherence:  0 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Difference:\", round(msm_perfect - msm_half, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Difference: 1 \n```\n\n\n:::\n:::\n\n\n\n\n---\n\n# 7. Longitudinal TMLE: Conceptual Overview\n\nLongitudinal TMLE extends the point-treatment TMLE from Chapter 2.4 to the time-varying setting. The core idea is the same --- fit initial models, then target them --- but now we work backwards through time.\n\n### The algorithm (simplified)\n\n:::{.callout-note}\n## Step 1: Outcome Model --- Start at the End\n**Start at the final time point** $T$ and fit the outcome model:\n\n$$\\hat{Q}_T(\\bar{A}_T, \\bar{L}_T, W) = E[Y \\mid \\bar{A}_T, \\bar{L}_T, W]$$\n\nThis is the same initial outcome regression as in point-treatment TMLE --- estimate the expected outcome given the full observed history. The difference is that \"history\" now includes treatment and covariates at every time point.\n:::\n\n:::{.callout-tip}\n## Step 2: Propensity Scores --- One at Every Time Point\nEstimate the treatment mechanism at each time $t$:\n\n$$g_t = P(A_t \\mid \\bar{A}_{t-1}, \\bar{L}_t, W)$$\n\nUnlike point-treatment TMLE which has a single propensity score, here you need $T$ separate propensity models --- one for each treatment decision. Each conditions on the full history available at that time point.\n:::\n\n:::{.callout-important}\n## Step 3: Clever Covariates --- Cumulative Inverse Probabilities\nFor each time $t$ from $T$ down to 1, compute the clever covariate using the cumulative treatment probability:\n\n$$H_t = \\frac{I(\\bar{A}_t = \\bar{a}_t)}{\\prod_{s=1}^{t} \\hat{g}_s}$$\n\nThe clever covariate at time $t$ involves the product of all propensity scores up to that time. This is what links the treatment model to the outcome model at each step of the backwards iteration.\n:::\n\n:::{.callout-warning}\n## Step 4: Targeting --- Update Backwards Through Time\nFor each time $t$ from $T$ down to 1, run a targeting (fluctuation) step to update $\\hat{Q}_t$:\n\n1. Regress the outcome (or pseudo-outcome from the next step) on the clever covariate $H_t$ with the logit of $\\hat{Q}_t$ as offset\n2. Use the estimated fluctuation parameter $\\hat{\\epsilon}_t$ to update predictions\n\n**Then average** the targeted predictions under the intervention of interest.\n\nEach targeting step corrects the outcome model at that time point using information from the treatment model, achieving sequential double robustness.\n:::\n\n:::{.callout-caution}\n## How Does This Handle Time-Dependent Confounding?\n\nThe backwards iteration is the key. At each step:\n\n- **Conditioning on $L_t$** in the outcome model at time $t$ correctly adjusts for confounding of $A_t$\n- **Integrating out $L_t$** in the backwards regression from time $t$ to $t-1$ preserves the mediated effect of $A_{t-1}$ through $L_t$\n\nThis is exactly the dilemma that standard regression cannot solve: you need to adjust for $L_t$ as a confounder of $A_t$ but not block it as a mediator of $A_{t-1}$. The sequential backwards structure does both, by conditioning on $L_t$ only at the time step where it is a confounder and then marginalizing over it at the step where it is a mediator.\n\n**In plain language:** L-TMLE \"peels off\" one layer of time-varying confounding at each step, working from the outcome back to baseline. By the time it reaches the first treatment decision, all the intermediate confounders have been properly handled.\n:::\n\n### Why LTMLE is better than longitudinal IPTW\n\n- Does not require computing cumulative weight products (avoids extreme weights)\n- Doubly robust at each time step\n- Can incorporate Super Learner for each nuisance model\n- Provides efficient influence curve-based inference\n\n### Packages\n\n- **ltmle:** The `ltmle` R package implements longitudinal TMLE for binary and continuous treatments with censoring\n- **lmtp:** The `lmtp` package implements a more general framework for modified treatment policies (static, dynamic, and stochastic interventions) using TMLE\n\n---\n\n# 8. Implementing Longitudinal TMLE\n\nWe demonstrate a simplified by-hand longitudinal TMLE for the static \"always treat\" regime, then show the package interface.\n\n### By-hand sequential regression approach\n\n:::{.callout-note}\n## Outcome Model: Initial Sequential Regression\n\nFirst, fit the outcome model at the final time step --- $Q_T = E[Y \\mid A_T, L_T, \\ldots, W]$ --- using all available information. Then predict under the counterfactual where all treatment decisions are set to \"high adherence.\"\n\nThis is the starting point: a G-computation estimate that would be correct if the outcome model is perfectly specified, but may be biased otherwise. The targeting steps below will correct this.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Work with complete cases for the by-hand demonstration\ndat_analysis <- dat_wide %>% filter(!is.na(Y))\n\n# --- Step 1: Fit outcome model at the final time step ---\n# Q_T = E[Y | A_T, L_T, ..., W]\n# We use all available information\n\n# Construct features for the final outcome model\ndat_analysis <- dat_analysis %>%\n  mutate(\n    cum_adh = rowMeans(across(starts_with(\"A_\")), na.rm = TRUE),\n    last_cd4 = cd4_6,\n    last_sympt = sympt_6\n  )\n\n# Initial outcome regression\nQ_mod <- glm(Y ~ A_1 + A_2 + A_3 + A_4 + A_5 + A_6 +\n               cd4_bl + vl_bl + depress + age + male +\n               cd4_6 + sympt_6,\n             family = binomial, data = dat_analysis)\n\n# Predict under always-high adherence (all A_t = 1)\ndat_cf <- dat_analysis %>%\n  mutate(across(starts_with(\"A_\"), ~1))\n\nQ_star <- predict(Q_mod, newdata = dat_cf, type = \"response\")\n\ncat(\"Sequential G-computation estimate (no targeting):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSequential G-computation estimate (no targeting):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Suppression under always-high adherence:\", round(mean(Q_star), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Suppression under always-high adherence: 0.511 \n```\n\n\n:::\n:::\n\n\n\n\n### Iterative targeting (simplified 2-step)\n\n:::{.callout-tip}\n## Propensity Scores: Modeling Treatment at Each Time Point\n\nWe estimate $P(A_t = 1 \\mid \\text{history}_t)$ at each time point. In this simplified demonstration, we model the cumulative probability of the full \"always treat\" regime as the product of time-specific propensities.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit <- function(p) log(p / (1 - p))\n\n# --- Propensity scores for cumulative treatment ---\n# Simplified: model P(all A_t = 1 | W) as product of marginals\n# This is an approximation for demonstration\ng_cum <- rep(1, nrow(dat_analysis))\n\nfor (t_val in 1:T_max) {\n  A_col <- paste0(\"A_\", t_val)\n  A_lag_col <- if(t_val > 1) paste0(\"A_\", t_val - 1) else NULL\n  cd4_col <- paste0(\"cd4_\", t_val)\n  sympt_col <- paste0(\"sympt_\", t_val)\n\n  formula_vars <- c(cd4_col, sympt_col, \"depress\", \"age\", \"male\", \"vl_bl\")\n  if (!is.null(A_lag_col)) formula_vars <- c(formula_vars, A_lag_col)\n\n  g_t_mod <- glm(\n    as.formula(paste(A_col, \"~\", paste(formula_vars, collapse = \" + \"))),\n    family = binomial, data = dat_analysis\n  )\n\n  g_t_pred <- predict(g_t_mod, type = \"response\")\n  g_cum <- g_cum * g_t_pred  # cumulative probability of always-treated\n}\n\n# Bound away from zero\ng_cum <- pmax(g_cum, 0.001)\n```\n:::\n\n\n\n\n:::{.callout-important}\n## Clever Covariate: Bridging Outcome and Treatment Models\n\nThe clever covariate for the \"always treat\" intervention is:\n\n$$H = \\frac{I(\\text{all } A_t = 1)}{g_{\\text{cum}}}$$\n\nFor patients who followed the regime at every time point, $H$ equals the inverse of their cumulative probability of doing so. For patients who deviated at any point, $H = 0$. This is the same logic as point-treatment TMLE's $H = I(A=1)/g(W)$, extended to the full treatment history.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Clever covariate for the \"always treat\" intervention\n# H = I(all A_t = 1) / g_cum\nall_treated <- rowSums(dat_analysis %>% select(starts_with(\"A_\"))) == T_max\nH_clever <- as.numeric(all_treated) / g_cum\n```\n:::\n\n\n\n\n:::{.callout-warning}\n## Targeting Step: Updating the Initial Estimates\n\nRun a logistic regression of $Y$ on $H$ with the logit of $\\hat{Q}$ as an offset. The coefficient $\\hat{\\epsilon}$ is the fluctuation parameter that corrects the initial outcome model to be unbiased for our specific estimand.\n\n$$\\text{logit}(Y) = \\text{logit}(\\hat{Q}) + \\epsilon \\cdot H$$\n\nThen update the counterfactual predictions:\n\n$$\\hat{Q}^* = \\text{expit}(\\text{logit}(\\hat{Q}_{\\text{cf}}) + \\hat{\\epsilon} \\cdot H_{\\text{cf}})$$\n\nwhere $H_{\\text{cf}} = 1/g_{\\text{cum}}$ because under the intervention, everyone is treated.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Targeting step\nQ_init <- predict(Q_mod, type = \"response\")\nfluc_mod <- glm(dat_analysis$Y ~ -1 + offset(logit(Q_init)) + H_clever,\n                family = binomial)\nepsilon <- coef(fluc_mod)\n\n# Update counterfactual predictions\nH_cf <- 1 / g_cum  # under the intervention, everyone is treated\nQ_targeted <- plogis(logit(Q_star) + epsilon * H_cf)\n\ncat(\"Targeted estimate (simplified LTMLE):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTargeted estimate (simplified LTMLE):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Suppression under always-high adherence:\", round(mean(Q_targeted), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Suppression under always-high adherence: 0.491 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Epsilon:\", round(epsilon, 5), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Epsilon: -0.00528 \n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-caution}\n## What Just Happened?\n\nThe targeting step nudged the initial G-computation predictions toward the truth by incorporating information from the treatment model. Here is the intuition:\n\n- If the outcome model was **already correct**, the fluctuation parameter $\\epsilon$ will be close to zero --- no correction needed\n- If the outcome model was **biased**, the clever covariate $H$ steers the correction in the right direction, using the treatment mechanism as a guide\n\nThe result is **double robustness**: the final estimate is consistent if either the outcome model $\\hat{Q}$ or the treatment model $\\hat{g}$ is correctly specified. Neither needs to be perfect on its own --- they compensate for each other's errors.\n:::\n\n### Influence-curve based inference\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# EIC for the always-treat estimand\npsi_hat <- mean(Q_targeted)\neic_long <- H_cf * (dat_analysis$Y - plogis(logit(Q_init) + epsilon * H_clever)) +\n            Q_targeted - psi_hat\n\nse_long <- sqrt(var(eic_long) / nrow(dat_analysis))\nci_long <- psi_hat + c(-1.96, 1.96) * se_long\n\ncat(\"Simplified LTMLE for always-high adherence:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimplified LTMLE for always-high adherence:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Estimate:\", round(psi_hat, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Estimate: 0.491 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  SE:\", round(se_long, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  SE: 0.523 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  95% CI: [\", round(ci_long[1], 3), \",\", round(ci_long[2], 3), \"]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  95% CI: [ -0.535 , 1.517 ]\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n# 9. Using the LMTP Package\n\nThe `lmtp` package provides a clean interface for estimating effects of longitudinal modified treatment policies. It handles:\n\n- Static interventions (always treat)\n- Dynamic interventions (treat based on covariate values)\n- Stochastic interventions (shift treatment probabilities)\n- Censoring adjustment\n- Super Learner integration\n\n### LMTP interface (conceptual)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmtp)\n\n# Define the shift function for \"always high adherence\"\nalways_adhere <- function(data, trt) {\n  rep(1, nrow(data))\n}\n\n# Define variable names\ntrt_vars <- paste0(\"A_\", 1:6)\ncens_vars <- paste0(\"C_\", 1:6)\nbaseline_vars <- c(\"age\", \"male\", \"cd4_bl\", \"vl_bl\", \"depress\")\ntv_vars <- list(\n  paste0(\"cd4_\", 1:6),\n  paste0(\"sympt_\", 1:6)\n)\n\n# Run LMTP-TMLE\nresult_lmtp <- lmtp_tmle(\n  data = dat_wide,\n  trt = trt_vars,\n  outcome = \"Y\",\n  baseline = baseline_vars,\n  time_vary = tv_vars,\n  cens = cens_vars,\n  shift = always_adhere,\n  outcome_type = \"binomial\",\n  learners_outcome = c(\"SL.glm\", \"SL.glmnet\"),\n  learners_trt = c(\"SL.glm\", \"SL.glmnet\"),\n  folds = 5\n)\n\n# View results\nresult_lmtp\n\n# Confidence interval\nconfint(result_lmtp)\n```\n:::\n\n\n\n\n### Stochastic intervention: shift adherence probability\n\nA more realistic intervention does not force 100% adherence but increases the probability of high adherence:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shift function: increase P(A=1) by 20 percentage points\n# but cap at 1\nshift_adherence <- function(data, trt) {\n  pmin(data[[trt]] + 0.2, 1)\n}\n\nresult_shift <- lmtp_tmle(\n  data = dat_wide,\n  trt = trt_vars,\n  outcome = \"Y\",\n  baseline = baseline_vars,\n  time_vary = tv_vars,\n  cens = cens_vars,\n  shift = shift_adherence,\n  outcome_type = \"binomial\",\n  learners_outcome = c(\"SL.glm\", \"SL.glmnet\"),\n  learners_trt = c(\"SL.glm\", \"SL.glmnet\"),\n  folds = 5\n)\n\nresult_shift\n```\n:::\n\n\n\n\n:::{.callout-tip}\n## Why Stochastic Interventions?\n\nStochastic interventions mitigate positivity violations by only shifting treatment probabilities rather than forcing them to 0 or 1. This makes the estimand more realistic --- no intervention can achieve 100% adherence.\n\n**Three reasons to prefer stochastic interventions:**\n\n1. **Realistic estimands:** Asking \"what if we increased the probability of adherence by 20%?\" is more actionable than \"what if everyone adhered perfectly?\" No real-world program achieves perfect adherence.\n\n2. **Positivity preservation:** If some patients have near-zero probability of adhering (e.g., due to severe side effects), a static \"always adhere\" intervention violates positivity for those patients. A shift intervention only moves probabilities incrementally, staying within the support of the observed data.\n\n3. **Policy relevance:** Decision-makers want to know the expected benefit of feasible interventions (SMS reminders, pill organizers, peer support) that shift adherence by realistic amounts --- not the theoretical maximum under impossible conditions.\n\nThe `lmtp` package makes stochastic interventions as easy to implement as static ones --- you just change the shift function.\n:::\n\n---\n\n# 10. Comparison: GEE vs. IPTW-MSM vs. LTMLE\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomparison <- tibble(\n  Method = c(\"Naive regression (biased)\",\n             \"IPTW-MSM (truncated weights)\",\n             \"Simplified LTMLE\"),\n  Approach = c(\"Conditions on cumulative adherence\",\n               \"Reweights by cumulative propensity\",\n               \"Sequential targeting with EIC inference\"),\n  Issue = c(\"Confounded by time-varying health\",\n            \"Extreme cumulative weights\",\n            \"More complex but doubly robust\")\n)\n\ncomparison\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  Method                       Approach                                Issue    \n  <chr>                        <chr>                                   <chr>    \n1 Naive regression (biased)    Conditions on cumulative adherence      Confound…\n2 IPTW-MSM (truncated weights) Reweights by cumulative propensity      Extreme …\n3 Simplified LTMLE             Sequential targeting with EIC inference More com…\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n# 11. Diagnostics for Longitudinal Settings\n\n## 11.1 Time-specific propensity score overlap\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check overlap at each time point\noverlap_checks <- list()\nfor (t_val in 1:T_max) {\n  A_col <- paste0(\"A_\", t_val)\n  cd4_col <- paste0(\"cd4_\", t_val)\n  sympt_col <- paste0(\"sympt_\", t_val)\n\n  formula_vars <- c(cd4_col, sympt_col, \"depress\", \"age\", \"male\")\n  if (t_val > 1) formula_vars <- c(formula_vars, paste0(\"A_\", t_val - 1))\n\n  g_check <- glm(\n    as.formula(paste(A_col, \"~\", paste(formula_vars, collapse = \" + \"))),\n    family = binomial, data = dat_analysis\n  )\n\n  ps_check <- predict(g_check, type = \"response\")\n  overlap_checks[[t_val]] <- tibble(\n    time = t_val,\n    ps = ps_check,\n    A = dat_analysis[[A_col]]\n  )\n}\n\noverlap_df <- bind_rows(overlap_checks)\n\nggplot(overlap_df, aes(x = ps, fill = factor(A, labels = c(\"Low adherence\", \"High adherence\")))) +\n  geom_density(alpha = 0.4) +\n  facet_wrap(~paste(\"Time\", time), ncol = 3) +\n  labs(\n    x = \"P(A_t = 1 | history)\",\n    fill = \"Adherence\",\n    title = \"Propensity Score Overlap by Time Point\"\n  ) +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"#EE6677\", \"#228833\"))\n```\n\n::: {.cell-output-display}\n![](03-07-longitudinal-td-confounding_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n## 11.2 Cumulative weight distribution\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"Cumulative IPTW weight summary:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCumulative IPTW weight summary:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Before truncation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBefore truncation:\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(weight_df$sw)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   1.473   13.231   28.079   64.916   73.400 2508.131 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nAfter truncation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAfter truncation:\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(weight_df$sw_trunc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.473  13.231  28.079  60.936  73.400 565.277 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nFraction > 10:\", round(mean(weight_df$sw > 10), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFraction > 10: 0.778 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Fraction > 50:\", round(mean(weight_df$sw > 50), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFraction > 50: 0.357 \n```\n\n\n:::\n:::\n\n\n\n\nLarge fractions of extreme weights indicate that the longitudinal treatment mechanism is poorly estimated or that positivity is violated. This motivates using LTMLE over IPTW.\n\n## 11.3 Adherence patterns over time\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize adherence trajectories\nadh_summary <- dat_long %>%\n  filter(!is.na(A)) %>%\n  group_by(time) %>%\n  summarise(\n    pct_adherent = mean(A),\n    n = n(),\n    .groups = \"drop\"\n  )\n\nggplot(adh_summary, aes(x = time, y = pct_adherent)) +\n  geom_line(color = \"#4477AA\", linewidth = 1) +\n  geom_point(size = 3, color = \"#4477AA\") +\n  ylim(0, 1) +\n  labs(\n    x = \"Time point\",\n    y = \"Proportion with high adherence\",\n    title = \"Adherence Over Time\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](03-07-longitudinal-td-confounding_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\n---\n\n# 12. Imperfect Adherence as an Intercurrent Event\n\nIn the ICH E9(R1) framework for clinical trials, adherence departures are **intercurrent events** that affect the interpretation of treatment effects.\n\n:::{.callout-note}\n## Estimand Strategies for Adherence\n\nThe ICH E9(R1) framework provides four strategies for handling intercurrent events like imperfect adherence. Each answers a different scientific question:\n\n| Strategy | Interpretation | Method |\n|----------|---------------|--------|\n| Treatment policy | Effect of being assigned treatment, regardless of adherence | ITT / standard comparison |\n| Hypothetical | Effect if adherence had been maintained | Longitudinal TMLE / LMTP |\n| While on treatment | Effect among the period of actual adherence | Censoring-based methods |\n| Composite | Adherence departure is part of the outcome | Modified outcome |\n\n**Longitudinal TMLE and LMTP directly estimate the hypothetical strategy** --- they estimate what would have happened under a counterfactual adherence pattern. This is the most relevant estimand for evaluating whether a treatment works when taken properly.\n\n**Why does this matter?** Choosing the wrong estimand strategy leads to answering the wrong question. If a drug works when taken but patients stop taking it due to side effects, the treatment policy estimand (ITT) will understate efficacy. The hypothetical estimand answers \"does the drug work?\" while the treatment policy estimand answers \"does prescribing the drug work?\" --- both are valid but serve different decisions.\n:::\n\n---\n\n# 13. Interpretation\n\n## What would we tell the public health agency?\n\nBased on our analysis:\n\n- Under the naturally observed adherence pattern, approximately 34% of patients achieve virologic suppression at 12 months\n- Under a hypothetical always-high-adherence intervention, the estimated suppression rate is approximately 49%\n- The estimated benefit of ensuring high adherence is approximately 14.7 percentage points of additional suppression\n\n## Implications for adherence support programs\n\nIf increasing adherence from observed levels to near-perfect could improve suppression by this magnitude, investments in adherence support (pill organizers, SMS reminders, peer support, reduced pill burden) may be cost-effective.\n\n## Fragile assumptions\n\n1. **Sequential exchangeability:** If unmeasured factors (e.g., substance use severity, housing instability) affect both adherence and outcomes, our estimates are biased\n2. **Positivity:** If some patient types never achieve high adherence, we cannot estimate the effect of forcing them to adhere --- stochastic interventions are more appropriate\n3. **Independent censoring:** If loss to follow-up is driven by unmeasured factors related to both adherence and outcome, even after conditioning on observed covariates, our censoring adjustment is biased\n\n---\n\n# Key Takeaways\n\n1. **Never condition on time-varying adherence in a regression model** when adherence is affected by time-varying confounders. This introduces collider bias through a feedback loop.\n\n2. **GEE models that adjust for time-varying covariates are biased** for causal questions about treatment effects under different adherence patterns.\n\n3. **Longitudinal IPTW (marginal structural models)** correctly handles time-varying confounding but produces unstable estimates when cumulative weights are extreme.\n\n4. **Longitudinal TMLE (LTMLE)** avoids the extreme weight problem by targeting the sequential outcome regressions directly. It is doubly robust at each time step and provides influence-curve inference.\n\n5. **LMTP extends this framework** to stochastic and modified treatment policies, which are more realistic than static \"always treat\" interventions and mitigate positivity violations.\n\n6. **Imperfect adherence is an intercurrent event.** The ICH E9(R1) framework helps clarify which estimand (treatment policy, hypothetical, etc.) matches the decision at hand. Longitudinal TMLE directly estimates the hypothetical strategy.\n\n7. **Diagnostics in longitudinal settings** require checking propensity score overlap at each time point and inspecting cumulative weight distributions. Extreme cumulative weights are a clear signal to prefer TMLE over IPTW.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26200)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3 forcats_1.0.0   stringr_1.6.0   dplyr_1.1.4    \n [5] purrr_1.0.2     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n [9] ggplot2_3.5.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6      jsonlite_2.0.0    compiler_4.4.2    tidyselect_1.2.1 \n [5] scales_1.3.0      yaml_2.3.10       fastmap_1.2.0     R6_2.6.1         \n [9] labeling_0.4.3    generics_0.1.3    knitr_1.49        htmlwidgets_1.6.4\n[13] munsell_0.5.1     pillar_1.9.0      tzdb_0.4.0        rlang_1.1.6      \n[17] utf8_1.2.4        stringi_1.8.7     xfun_0.49         timechange_0.3.0 \n[21] cli_3.6.5         withr_3.0.2       magrittr_2.0.3    digest_0.6.37    \n[25] grid_4.4.2        rstudioapi_0.17.1 hms_1.1.3         lifecycle_1.0.4  \n[29] vctrs_0.6.5       evaluate_1.0.5    glue_1.8.0        farver_2.1.2     \n[33] fansi_1.0.6       colorspace_2.1-1  rmarkdown_2.29    tools_4.4.2      \n[37] pkgconfig_2.0.3   htmltools_0.5.8.1\n```\n\n\n:::\n:::\n",
    "supporting": [
      "03-07-longitudinal-td-confounding_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}