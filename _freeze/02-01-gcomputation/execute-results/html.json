{
  "hash": "a4d955421c54f2ce19da35d2f9b4f811",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 2.1: Outcome Modeling and Standardization (G-Computation)\"\nformat: html\n---\n\n\n\n\n# Chapter 2.1: Outcome Modeling and Standardization  \n*G-computation as a foundation for causal estimation*\n\nOutcome modeling and standardization—often referred to as **g-computation**—is one of the oldest and most intuitive approaches to causal inference. In this chapter, we'll build intuition, walk carefully through why the method works, show where it fails, and provide fully reproducible examples in R (using tidyverse style).  \n\nThis chapter is intentionally thorough, designed for students new to causal inference but with working knowledge of regression.\n\n---\n\n:::{.callout-note}\n## Why Start With G-Computation?\n\n**In one sentence:** G-computation estimates causal effects by predicting what *would* happen to everyone under each treatment, then comparing those predictions -- no reliance on a single regression coefficient required.\n\nG-computation provides a way to estimate causal effects without relying on regression coefficients. Instead, it reconstructs the potential outcomes:\n\n- **Y(1)** = outcome if treated\n- **Y(0)** = outcome if untreated\n\nG-computation estimates the **Average Treatment Effect (ATE)**:\n\n$$\nATE = E[Y(1) - Y(0)]\n$$\n\nby using the identification formula:\n\n$$\nE[ Y(1) ] = E_W[ E[Y | A=1, W] ] \\\\\nE[ Y(0) ] = E_W[ E[Y | A=0, W] ]\n$$\n\nThen:\n\n$$\nATE = E[Y(1)] - E[Y(0)].\n$$\n\nThis is conceptually simple and is often the easiest method for students to grasp when beginning causal inference.\n:::\n\n:::{.callout-tip}\n## The \"Aha\" Moment: What Does G-Computation Actually Do?\n\nG-computation reconstructs what would have happened if *everyone* in the dataset had received treatment **A=1**, and independently what would have happened if *everyone* had received **A=0**.\n\nThink of it as running two parallel-universe versions of your study population:\n\n1. **Fit an outcome model:** $$ E[Y | A, W] $$\n2. **Predict outcomes** for *each* individual under both Treatment and Control\n3. **Average** those predicted outcomes (standardization)\n\nThis produces population-level risks that correspond to the causal estimand. That three-step recipe -- model, predict, average -- is the entire method. Everything else is details.\n:::\n\n# 3. Implementation Example: Simulated Osteoporosis Cohort\n\nWe simulate a small cohort similar to the denosumab vs zoledronic acid motivating example.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'stringr' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.6.0\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(2025)\nn <- 3000\n\n# baseline covariates\nage <- rnorm(n, 75, 6)\ncvd <- rbinom(n, 1, plogis(0.12 * (age - 70)))\n\n# treatment assignment\nA <- rbinom(n, 1, plogis(-1 + 0.07 * (age - 70) + 1.4 * cvd))\n\n# outcome\nY <- rbinom(n, 1, plogis(-2 + 0.6*A + 0.12*(age - 70) + 1.2*cvd))\n\ndat <- tibble(age, cvd, A, Y)\n```\n:::\n\n\n\n\nThe treatment is strongly confounded by cardiovascular history—perfect for demonstrating g-computation.\n\n---\n\n# 4. Step-by-Step G-Computation\n\n:::{.callout-note}\n## Step 1: Fit an Outcome Model\n\nWe fit a logistic regression model predicting the outcome from treatment and confounders:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- glm(Y ~ A + age + cvd, family = binomial, data = dat)\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Y ~ A + age + cvd, family = binomial, data = dat)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -9.445955   0.594115 -15.899  < 2e-16 ***\nA            0.498637   0.088087   5.661 1.51e-08 ***\nage          0.107847   0.007951  13.563  < 2e-16 ***\ncvd          1.271786   0.094031  13.525  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4127.2  on 2999  degrees of freedom\nResidual deviance: 3422.0  on 2996  degrees of freedom\nAIC: 3430\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n\n\n**Important for MPH students:** The coefficient on `A` from this logistic regression is a *conditional log-odds ratio* -- it tells you the association between treatment and outcome *holding confounders fixed*. That is **not** a causal effect on its own. We need the next two steps (prediction + averaging) to turn this model into a causal estimate.\n:::\n\n:::{.callout-tip}\n## Step 2: Predict Counterfactual Outcomes -- The \"What If\" Step\n\nThis is where g-computation earns its name. We ask two \"what if\" questions for *every single person* in the dataset, regardless of what treatment they actually received:\n\n- **What if this person had been treated?** (set `A = 1`)\n- **What if this person had been untreated?** (set `A = 0`)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create counterfactual datasets\ndat1 <- dat %>% mutate(A = 1)\ndat0 <- dat %>% mutate(A = 0)\n\n# predict potential outcomes\np1 <- predict(mod, newdata = dat1, type = \"response\")\np0 <- predict(mod, newdata = dat0, type = \"response\")\n```\n:::\n\n\n\n\nHere:\n- `p1[i]` = predicted outcome for person *i* if treated\n- `p0[i]` = predicted outcome for person *i* if untreated\n\nNotice we are *not* comparing treated vs. untreated groups. We are comparing two hypothetical worlds for the *same* people -- this is the counterfactual reasoning that separates causal inference from plain regression.\n:::\n\n:::{.callout-important}\n## Step 3: Standardize -- This Is the Step That Makes It Causal\n\nAveraging the counterfactual predictions over the *entire population* is what converts model-based predictions into a **causal** estimate. Without this step, you only have conditional predictions; *with* it, you have a standardized (marginal) risk difference -- the same quantity you would get from a perfectly executed randomized trial.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrisk1 <- mean(p1)\nrisk0 <- mean(p0)\nate  <- risk1 - risk0\n\nlist(risk1 = risk1, risk0 = risk0, ate = ate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$risk1\n[1] 0.4890903\n\n$risk0\n[1] 0.3890304\n\n$ate\n[1] 0.1000599\n```\n\n\n:::\n:::\n\n\n\n\nThis gives:\n- **Risk under treatment**\n- **Risk under control**\n- **Risk difference (ATE)**\n\nInterpretation example:\n\n> “Initiating denosumab rather than ZA is estimated to increase/decrease 3-year MI/stroke risk by X percentage points.”\n:::\n\n# 5. Why G-Computation Works\n\nIt directly implements the identification formula:\n\n$$\nE_W[ E[Y | A=a, W] ].\n$$\n\nThis contrasts with regression coefficients, which estimate:\n\n$$\n\text{conditional log-odds ratios given W}\n$$\n\n-- completely different from the marginal risk difference.\n\nStandardization always yields marginal (population-level) effects.\n\n:::{.callout-tip}\n## Key Distinction: Marginal Risk Difference vs. Conditional Log-Odds Ratio\n\nThis is one of the most commonly confused points in epidemiology:\n\n- **Conditional log-odds ratio** (what logistic regression gives you): \"Among people with the *same* age and CVD status, how do the odds of the outcome differ between treated and untreated?\" This is a measure of *association within strata* -- it does not tell you what would happen if you intervened on the whole population. It is also non-collapsible, meaning it changes depending on which covariates you condition on, even without confounding.\n\n- **Marginal risk difference** (what g-computation gives you): \"If we could treat *everyone* versus *no one*, how much would the overall population risk change?\" This is a *causal* contrast on the risk scale -- the kind of number a clinician or policy-maker can act on.\n\nG-computation bridges the gap: it uses the conditional model internally but **standardizes** over the population to produce the marginal (causal) effect.\n:::\n\n---\n\n# 6. Diagnostics: When Can G-Computation Fail?\n\n:::{.callout-warning}\n## Model Misspecification\n\nIf your model for \\(E[Y | A, W]\\) is wrong (e.g., omits interactions, assumes linearity), g-computation may be biased. Because the entire method rests on a single outcome model, getting that model wrong propagates bias directly into your causal estimate.\n\nCheck residuals, fit alternative models, or use machine learning (next chapter).\n:::\n\n---\n\n:::{.callout-caution}\n## Poor Positivity -- Beware of Predicting Into the Void\n\nIf some strata almost never receive a treatment, g-computation is forced to *extrapolate* -- predicting outcomes in regions of the data where you have little or no observed experience. This is a silent failure: R will happily return a number, but that number may be meaningless.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nps <- predict(glm(A ~ age + cvd, family = binomial, data = dat), type = \"response\")\nsummary(ps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1569  0.3715  0.6402  0.5647  0.7218  0.9025 \n```\n\n\n:::\n:::\n\n\n\n\nLook for:\n- Scores near 0 or 1 -- dangerous for extrapolation\n- G-computation may have to predict outcomes in unobserved regions\n:::\n\n---\n\n:::{.callout-warning}\n## Unmeasured Confounding\n\nNo modeling strategy fixes missing confounders. If an important common cause of treatment and outcome is absent from your dataset, g-computation -- like every other method in this handbook -- will produce a biased estimate.\n\nHowever, g-computation makes its assumptions very explicit (you can see exactly which variables W enter the model), which is an advantage for transparent reporting and sensitivity analysis.\n:::\n\n---\n\n:::{.callout-note}\n## Using Flexible Models (Teaser for TMLE + SuperLearner)\n\nOne of the biggest limitations of g-computation with logistic regression is that you have to get the functional form right. A powerful alternative: replace your parametric model with a data-adaptive learner.\n\nYou can replace logistic regression with:\n- random forests\n- gradient boosting\n- generalized additive models\n- SuperLearner ensembles\n\nThis reduces reliance on parametric assumptions.\n\nExample:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(SuperLearner)\n\nsl_mod <- SuperLearner(\n  Y = dat$Y,\n  X = dat %>% select(A, age, cvd),\n  family = binomial(),\n  SL.library = c(\"SL.glm\", \"SL.ranger\", \"SL.gam\")\n)\n\n# predict for counterfactuals\np1_sl <- predict(sl_mod, newdata = dat1)$pred\np0_sl <- predict(sl_mod, newdata = dat0)$pred\n\nmean(p1_sl - p0_sl)\n```\n:::\n\n\n\n\nIn later chapters, we will systematically integrate SuperLearner with **TMLE**.\n:::\n\n---\n\n:::{.callout-tip}\n## Summary\n\nIn this chapter you learned:\n\n- What g-computation is and why it is foundational\n- How to compute standardized risk differences\n- How g-computation connects to the identification formula\n- Where g-computation can fail (positivity, misspecification)\n- How flexible ML-based models can help\n\n**The one-liner to remember:** G-computation = fit a model, predict under both treatments for everyone, then average. That is standardization, and that is what gives you a causal estimate.\n\nNext: **IPTW**, another way to estimate causal effects by reweighting the data instead of modeling the outcome.\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}