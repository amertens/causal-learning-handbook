{
  "hash": "bc8837cb6705850b896e836fc8f99870",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 2.3: Doubly Robust Estimators and Targeted Learning (AIPW + TMLE)\"\nformat: html\n---\n\n\n\n\n# Chapter 2.3: Doubly Robust Estimation and Targeted Learning\n*Bridging outcome modeling and weighting for more robust causal effect estimation*\n\nIn previous chapters, you learned:\n\n- **G-computation** depends on correctly modeling the *outcome*\n- **IPTW** depends on correctly modeling the *treatment mechanism*\n\nBut what if you could use **both models**, and as long as **either one is correct**, your estimator is still consistent?\n\nThis is exactly what **doubly robust estimators** provide.\n\nWe explore:\n\n- Why doubly robust estimators matter\n- AIPW (Augmented IPTW)\n- TMLE (Targeted Maximum Likelihood Estimation)\n- How to implement both with tidyverse-friendly R code\n- Why TMLE is preferred in modern causal inference\n\n---\n\n# 1. Why Doubly Robust Estimators?\n\n:::{.callout-important}\n## Why Doubly Robust Estimators?\n\nA doubly robust estimator is consistent if **either**:\n\n- the treatment model (propensity score) is correctly specified, **or**\n- the outcome model is correctly specified\n\nThis is especially important in real-world epidemiologic research where:\n\n- all models are approximations --- parametric forms are always a choice, and rarely the truth\n- parametric models are easily misspecified (e.g., missing an interaction or non-linearity)\n- ML-based models can have high variance and may not converge to the truth without careful tuning\n\nBy combining both sources of information, doubly robust estimators provide **two chances to get it right**. In practice, they often outperform either G-computation or IPTW used alone, because small errors in one model are corrected by the other.\n:::\n\n---\n\n# 2. AIPW: Augmented Inverse Probability Weighting\n\n:::{.callout-note}\n## What Makes AIPW \"Doubly Robust\"?\n\nAIPW adds a correction term to IPTW using the outcome regression. The key insight is that it is constructed so that:\n\n- If the **treatment model** is right --- the IPTW component produces an unbiased estimate\n- If the **outcome model** is right --- the augmentation (G-computation) component corrects any bias in the weights\n- If **both** are right --- the estimator achieves the **semiparametric efficiency bound**, meaning it has the smallest possible variance among all regular asymptotically linear estimators\n\nIn other words, AIPW \"augments\" the weighted estimator with a bias-correction term derived from the outcome model. This makes the estimator robust to misspecification of either (but not both) nuisance models --- hence the name *doubly robust*.\n:::\n\n## 2.1 Formula\n\nLet:\n\n- $e(W) = P(A = 1 \\mid W)$ be the propensity score\n- $m(a, W) = E[Y \\mid A = a, W]$ be the outcome regression\n\nThen:\n\n$$\n\\hat{\\mu}_1 = \\frac{1}{n} \\sum_i \\left[ \\frac{A_i Y_i}{e(W_i)} - \\frac{A_i - e(W_i)}{e(W_i)} m(1, W_i) \\right]\n$$\n\n$$\n\\hat{\\mu}_0 = \\frac{1}{n} \\sum_i \\left[ \\frac{(1-A_i) Y_i}{1 - e(W_i)} + \\frac{A_i - e(W_i)}{1 - e(W_i)} m(0, W_i) \\right]\n$$\n\nATE = $\\hat{\\mu}_1 - \\hat{\\mu}_0$\n\n---\n\n# 3. Simulated Osteoporosis Dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'stringr' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.6.0\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(2026)\nn <- 4000\n\nage <- rnorm(n, 75, 6)\ncvd <- rbinom(n, 1, plogis(0.12 * (age - 70)))\n\n# Treatment model\nA <- rbinom(n, 1, plogis(-1 + 0.08*(age - 70) + 1.4*cvd))\n\n# Outcome model\nY <- rbinom(n, 1, plogis(-2 + 0.6*A + 0.10*(age - 70) + 1.0*cvd))\n\ndat <- tibble(age, cvd, A, Y)\n```\n:::\n\n\n\n\n---\n\n# 4. Fit Outcome and Propensity Score Models\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nQmod <- glm(Y ~ A + age + cvd, family = binomial, data = dat)\ngmod <- glm(A ~ age + cvd, family = binomial, data = dat)\n\ndat <- dat %>%\n  mutate(\n    ps = predict(gmod, type = \"response\"),\n    Q1 = predict(Qmod, newdata = mutate(dat, A=1), type = \"response\"),\n    Q0 = predict(Qmod, newdata = mutate(dat, A=0), type = \"response\")\n  )\n```\n:::\n\n\n\n\n---\n\n# 5. Compute AIPW Estimator\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(dat, {\n  mu1 <- mean( Q1 + A * (Y - Q1) / ps )\n  mu0 <- mean( Q0 + (1 - A) * (Y - Q0) / (1 - ps) )\n  ate <- mu1 - mu0\n  c(mu1 = mu1, mu0 = mu0, ate = ate)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      mu1       mu0       ate \n0.4555965 0.3221936 0.1334029 \n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-tip}\n## Interpreting the AIPW Estimate\n\nThe AIPW estimate of the ATE represents the average causal effect of treatment on the outcome in the population, combining information from both the outcome model and the propensity score model.\n\nNotice the structure of each component:\n\n- `Q1 + A * (Y - Q1) / ps` starts with the outcome model prediction $m(1, W)$ and adds a weighted residual correction. For treated individuals ($A = 1$), the residual $(Y - Q1)$ is upweighted by $1/\\text{ps}$; for untreated individuals ($A = 0$), the correction term is zero.\n- `Q0 + (1 - A) * (Y - Q0) / (1 - ps)` works symmetrically for the control potential outcome.\n\nThis structure is exactly what gives AIPW its doubly robust property: even if the outcome model $Q$ is wrong, the IPW correction fixes the bias (and vice versa).\n:::\n\n---\n\n# 6. TMLE: Targeted Maximum Likelihood Estimation\n\n:::{.callout-important}\n## Why TMLE Over AIPW?\n\nAIPW is a good doubly robust estimator, but **TMLE offers several practical advantages** that make it the preferred method in modern causal inference:\n\n- **Bounded predictions:** TMLE updates the outcome model on the probability scale (via logistic fluctuation), so predicted probabilities always stay in $[0, 1]$. AIPW can produce estimates outside the parameter space.\n- **Plug-in estimator:** TMLE produces an updated probability distribution, not just a point estimate. This means the estimate always respects the constraints of the statistical model.\n- **Natural integration with machine learning:** TMLE pairs seamlessly with SuperLearner (ensemble ML), allowing flexible, data-adaptive modeling of nuisance parameters while maintaining valid inference.\n- **Solves the efficient influence curve (EIC) equation:** The TMLE fluctuation step ensures that the EIC estimating equation is solved at the estimate, providing a foundation for valid standard errors and confidence intervals.\n- **Finite-sample stability:** Because TMLE works on the logistic scale and uses a targeted update rather than direct weighting, it tends to be more stable when propensity scores are near 0 or 1.\n:::\n\n---\n\n# 7. TMLE Step-by-Step\n\n:::{.callout-note}\n## Step 1: Fit the Initial Outcome Model (Q)\n\nThe first step is to fit a model for $E[Y \\mid A, W]$ and generate initial predictions. This is the same outcome model used in G-computation. We need predictions under both treatment values for each individual:\n\n- $Q_n^0(1, W_i)$ = predicted outcome if treated\n- $Q_n^0(0, W_i)$ = predicted outcome if untreated\n\nAt this stage, the predictions are \"initial\" --- they have not yet been targeted toward the causal parameter of interest.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit <- function(p) log(p/(1-p))\nexpit <- function(x) 1/(1+exp(-x))\n\n# Step 1: Initial Q predictions at observed A\nQinit <- predict(Qmod, type=\"response\")\n```\n:::\n\n\n\n\n:::{.callout-tip}\n## Step 2: Estimate the Propensity Score\n\nThe propensity score $e(W) = P(A = 1 \\mid W)$ is needed to construct the clever covariate in the next step. We already fit this model in Section 4.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 2: Propensity scores (already estimated)\nps <- dat$ps\n```\n:::\n\n\n\n\n:::{.callout-important}\n## Step 3: Construct the Clever Covariate\n\nThe **clever covariate** $H(A, W)$ is the key ingredient that makes TMLE \"targeted.\" It is derived from the efficient influence curve for the ATE:\n\n$$\nH(A, W) = \\frac{A}{e(W)} - \\frac{1 - A}{1 - e(W)}\n$$\n\nFor computing the counterfactual means, we also need the treatment-specific components:\n\n- $H_1(W) = \\frac{1}{e(W)}$ (for the treated potential outcome)\n- $H_0(W) = \\frac{-1}{1 - e(W)}$ (for the control potential outcome)\n\nThe clever covariate directs the fluctuation to reduce bias for the specific causal estimand we care about --- the ATE.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 3: Clever covariate at observed treatment\nH <- with(dat, A/ps - (1-A)/(1-ps))\n```\n:::\n\n\n\n\n:::{.callout-warning}\n## Step 4: Fit the Fluctuation Model and Update Predictions\n\nThis is the critical \"targeting\" step. We fit a logistic regression of $Y$ on $H$ with the initial logit-predictions as an offset:\n\n$$\n\\text{logit}(E[Y \\mid A, W]) = \\text{logit}(Q_n^0(A, W)) + \\epsilon \\cdot H(A, W)\n$$\n\nThe estimated $\\hat{\\epsilon}$ is then used to **update** (fluctuate) the initial predictions. Crucially, we must apply this fluctuation to the counterfactual predictions $Q_n^0(1, W)$ and $Q_n^0(0, W)$ using their respective clever covariates $H_1$ and $H_0$.\n\nThis step ensures the final estimates solve the efficient influence curve equation, which is what gives TMLE its optimal statistical properties.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 4: Fluctuation model\nepsilon <- glm(dat$Y ~ -1 + offset(logit(Qinit)) + H,\n               family = binomial)$coef\n```\n:::\n\n\n\n\n---\n\n# 8. Counterfactual Predictions for TMLE\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get initial (pre-fluctuation) predictions under each treatment\nQ1_init <- predict(Qmod, newdata = mutate(dat, A=1), type=\"response\")\nQ0_init <- predict(Qmod, newdata = mutate(dat, A=0), type=\"response\")\n\n# Clever covariates for counterfactual treatment values\nH1 <- 1 / ps\nH0 <- -1 / (1 - ps)\n\n# Apply the fluctuation to counterfactual predictions\nQ1_star <- plogis(logit(Q1_init) + epsilon * H1)\nQ0_star <- plogis(logit(Q0_init) + epsilon * H0)\n\n# TMLE ATE estimate\ntmle_ate <- mean(Q1_star) - mean(Q0_star)\ntmle_ate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1334025\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n# 8.1 TMLE Inference: Standard Errors and Confidence Intervals\n\nA major advantage of TMLE is that valid inference follows directly from the **efficient influence curve (EIC)**. The EIC for the ATE at the TMLE fit is:\n\n$$\nD_i = H_1(W_i)(Y_i - Q_1^*(W_i)) - H_0(W_i)(Y_i - Q_0^*(W_i)) + Q_1^*(W_i) - Q_0^*(W_i) - \\hat{\\psi}\n$$\n\nwhere $\\hat{\\psi}$ is the TMLE ATE estimate. The variance of the ATE is estimated by the sample variance of the EIC divided by $n$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Observed outcomes\nY_obs <- dat$Y\n\n# EIC (efficient influence curve) components\n# For treated: use Q1_star; for control: use Q0_star\n# But we need Q* at the OBSERVED A for the residual terms\nQstar_obs <- ifelse(dat$A == 1, Q1_star, Q0_star)\nH_obs <- ifelse(dat$A == 1, H1, H0)\n\n# Efficient influence curve for each observation\nEIC <- H_obs * (Y_obs - Qstar_obs) + Q1_star - Q0_star - tmle_ate\n\n# Standard error and 95% CI\ntmle_se <- sqrt(var(EIC) / n)\nci_lower <- tmle_ate - 1.96 * tmle_se\nci_upper <- tmle_ate + 1.96 * tmle_se\n\ntibble(\n  Estimate = tmle_ate,\n  SE = tmle_se,\n  CI_lower = ci_lower,\n  CI_upper = ci_upper\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  Estimate     SE CI_lower CI_upper\n     <dbl>  <dbl>    <dbl>    <dbl>\n1    0.133 0.0160    0.102    0.165\n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-tip}\n## Interpreting TMLE Inference\n\nThe EIC-based standard error is a key output of TMLE. Because the fluctuation step solves the EIC estimating equation, the resulting confidence interval has correct coverage under standard regularity conditions --- even when machine learning is used for the nuisance models (provided cross-fitting or other sample-splitting techniques are employed).\n\nIf the confidence interval excludes zero, we have evidence that the treatment has a non-null causal effect on the outcome at the 95% confidence level.\n:::\n\n---\n\n# 9. TMLE + SuperLearner (Recommended)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(SuperLearner)\n\nSL_lib <- c(\"SL.glm\", \"SL.glmnet\", \"SL.ranger\", \"SL.mean\")\n\nQ_SL <- SuperLearner(\n  Y = dat$Y,\n  X = dat %>% select(A, age, cvd),\n  family = binomial(),\n  SL.library = SL_lib\n)\n\ng_SL <- SuperLearner(\n  Y = dat$A,\n  X = dat %>% select(age, cvd),\n  family = binomial(),\n  SL.library = SL_lib\n)\n```\n:::\n\n\n\n\nYou can plug these flexible nuisance models directly into TMLE. SuperLearner builds an ensemble (weighted combination) of multiple algorithms, reducing the risk that any single model is badly misspecified. When combined with TMLE's fluctuation step, this provides a powerful, data-adaptive approach to causal inference that maintains valid statistical inference.\n\n---\n\n# 10. Comparison: AIPW vs TMLE\n\n:::{.callout-note}\n## AIPW vs TMLE: Head-to-Head\n\n| Property | AIPW | TMLE |\n|---|---|---|\n| Doubly robust | Yes | Yes |\n| Semiparametrically efficient | Yes (asymptotically) | Yes |\n| Handles ML well | Requires cross-fitting | Yes, with SuperLearner |\n| Predictions bounded in $[0,1]$ | No | Yes |\n| Solves EIC equation | Not guaranteed | Yes, by construction |\n| Implementation complexity | Simple | Moderate |\n\n**Bottom line for epidemiologists:** Both AIPW and TMLE are valid doubly robust estimators. TMLE is generally preferred because it respects the bounds of the parameter space (e.g., probabilities stay in $[0,1]$), integrates naturally with ensemble machine learning, and solves the efficient influence curve equation by construction --- giving it better finite-sample performance and more trustworthy inference.\n:::\n\n---\n\n# 11. Summary\n\n:::{.callout-tip}\n## Key Takeaways\n\nYou now understand:\n\n- **The motivation** for doubly robust estimators: they give you two chances to get the right answer by combining outcome and treatment models\n- **How AIPW works:** it augments the IPW estimator with a bias-correction term from the outcome model\n- **Why TMLE offers advantages:** bounded predictions, plug-in estimation, and natural integration with machine learning\n- **How TMLE is implemented step-by-step:** initial Q, propensity score, clever covariate, fluctuation, and EIC-based inference\n- **How to obtain valid inference** from TMLE using the efficient influence curve\n- **How these fit into the Causal Roadmap:** doubly robust estimators are the recommended default for point-treatment causal inference in modern epidemiology\n\nIn the next chapter, we move to **SuperLearner and machine learning integration**.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26200)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3 forcats_1.0.0   stringr_1.6.0   dplyr_1.1.4    \n [5] purrr_1.0.2     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n [9] ggplot2_3.5.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6      jsonlite_2.0.0    compiler_4.4.2    tidyselect_1.2.1 \n [5] scales_1.3.0      yaml_2.3.10       fastmap_1.2.0     R6_2.6.1         \n [9] generics_0.1.3    knitr_1.49        htmlwidgets_1.6.4 munsell_0.5.1    \n[13] pillar_1.9.0      tzdb_0.4.0        rlang_1.1.6       utf8_1.2.4       \n[17] stringi_1.8.7     xfun_0.49         timechange_0.3.0  cli_3.6.5        \n[21] withr_3.0.2       magrittr_2.0.3    digest_0.6.37     grid_4.4.2       \n[25] rstudioapi_0.17.1 hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5      \n[29] evaluate_1.0.5    glue_1.8.0        fansi_1.0.6       colorspace_2.1-1 \n[33] rmarkdown_2.29    tools_4.4.2       pkgconfig_2.0.3   htmltools_0.5.8.1\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}