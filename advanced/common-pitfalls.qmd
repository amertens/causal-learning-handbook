---
title: "Chapter X: Common Pitfalls and How to Avoid Them"
format: html
---

# Chapter X  
## Common Pitfalls and How to Avoid Them  
*A practical guide for avoiding frequent mistakes in causal inference and targeted learning*

Even with a solid understanding of the causal roadmap and modern estimators, it is remarkably easy to make analytic choices that lead to biased, unstable, or misleading results. In real-world evidence (RWE), the stakes are even higher: data are messy, treatment pathways are irregular, and clinical validity depends on careful execution.

This chapter catalogs **the most common pitfalls** analysts encounter in causal inference—especially in pharmacoepidemiologic and longitudinal RWE settings—and provides **concrete strategies** to avoid them.

---

## 1. Pitfall: Confusing Association With Causation  
### The regression coefficient problem

**Mistake:**  
Running multivariable regression and interpreting the treatment coefficient as a causal effect.

**Why it matters:**  
Regression coefficients estimate **conditional associations**, not causal contrasts. Unless the regression exactly corresponds to the identification formula and correctly specifies the functional forms, the estimate is biased.

**Avoid by:**  
- Defining a precise **causal question** and **estimand**  
- Using **G-computation**, **IPTW**, **AIPW**, or **TMLE** instead of regression coefficients  
- Using flexible nuisance estimation (SuperLearner)  

---

## 2. Pitfall: Adjusting for Post-Treatment Variables  
### The mediator/collider trap

**Mistake:** Including variables measured after treatment assignment in regression or propensity score models.

**Why it matters:**  
- These variables often lie on **causal pathways** → adjusting blocks part of the effect  
- They may be **colliders** → inducing bias  
- Breaks the hypothetical intervention definition

**Avoid by:**  
- Adjusting only for **baseline** confounders  
- Using **LMTP/LTMLE** when post-treatment confounding exists  
- Drawing DAGs to clarify temporal structure  

---

## 3. Pitfall: Violated Positivity / Lack of Overlap  
### Treatment not comparable across covariate strata

**Symptoms:**  
- Propensity scores near 0 or 1  
- Large IPTW weights  
- Extreme clever covariates in TMLE  

**Avoid by:**  
- Checking PS overlap  
- Truncating weights  
- Restricting to **regions of support**  
- Using **stochastic interventions** when static interventions are not feasible  

---

## 4. Pitfall: Misspecified Outcome or Propensity Models  
### Relying on simple models when relationships are nonlinear

**Avoid by:**  
- Using **SuperLearner**  
- Evaluating cross-validated risk  
- Inspecting calibration  

---

## 5. Pitfall: Blindly Trusting Machine Learning  
### Flexible models ≠ correct models

**Avoid by:**  
- Always using **cross-validation**  
- Checking calibration and residual diagnostics  
- Using TMLE to protect inference from ML instability  

---

## 6. Pitfall: Ignoring Censoring and Informative Dropout  

**Avoid by:**  
- Modeling censoring with SL  
- Using TMLE/LMTP with censoring nodes  
- Conducting sensitivity analyses  

---

## 7. Pitfall: Over-interpreting Heterogeneous Treatment Effects  
### HTE estimates are high variance and easy to misinterpret

**Avoid by:**  
- Prespecifying subgroups  
- Reporting uncertainty  
- Treating causal forest results as **exploratory**  
- Using TMLE-based variable-importance for confirmatory analyses  

---

## 8. Pitfall: Using the Wrong Estimand  
### Hazard ratios and odds ratios are not causal effects

**Avoid by:**  
- Targeting risk differences, risk ratios, survival curves, or RMST  
- Using LMTP/TMLE for causal survival analysis  

---

## 9. Pitfall: Not Performing Diagnostics  
### “The model ran” ≠ “The result is valid”

**Avoid by:**  
- Inspecting weight distributions  
- Checking clever covariate ranges  
- Examining influence curve stability  
- Running calibration checks for Q/g  

---

## 10. Pitfall: Neglecting Sensitivity Analyses  
### Assuming away unmeasured confounding

**Avoid by:**  
- E-values  
- Quantitative bias analysis  
- Negative control outcomes/exposures  
- Stochastic sensitivity analyses  

---

## 11. Pitfall: Poor Alignment Between RCT and RWD in Hybrid Designs  

**Avoid by:**  
- Harmonizing definitions  
- Checking covariate balance across datasets  
- Applying ES-CV-TMLE or A-TMLE  
- Conducting negative controls in RWD  

---

## 12. Pitfall: Reporting Without Context or Interpretation  

**Avoid by:**  
- Including absolute risk estimates  
- Transparently stating assumptions  
- Discussing limitations and robustness  
- Framing the estimand in scientific terms  

---

## 13. Summary

Common pitfalls arise from misalignment between analytic choices and scientific questions, misspecification, unchecked assumptions, and overinterpretation. Avoiding them requires:

- The causal roadmap  
- Diagnostics  
- Sensitivity analyses  
- Careful communication  

These principles yield causal evidence that is credible, reproducible, and actionable.

```{r}
sessionInfo()
```
