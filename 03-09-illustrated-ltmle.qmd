---
title: "An Illustrated Guide to Longitudinal TMLE"
subtitle: "Extending the KH Stats approach to time-varying treatments and confounders"
format: html
---

# An Illustrated Guide to Longitudinal TMLE
*How L-TMLE removes time-dependent confounding, step by step*

This chapter extends the pedagogical approach of Kat Hoffman's [Illustrated Guide to TMLE](https://www.khstats.com/blog/tmle/tutorial) to the **longitudinal setting**. If you have read the KH Stats guides and understand point-treatment TMLE, you are ready for this chapter.

We will build up from the familiar 4-step TMLE algorithm to its longitudinal counterpart, using a simple two-time-point example so you can see exactly what changes — and why.

---

# 1. Quick Recap: Point-Treatment TMLE in 4 Steps

In the [KH Stats illustrated guide](https://www.khstats.com/blog/tmle/tutorial-pt2), TMLE for a binary treatment and outcome proceeds in four steps:

:::{.callout-note}
## Step 1: Estimate the Expected Outcome
Fit a model for $E[Y \mid A, W]$ — the expected outcome given treatment and confounders. Use this to predict outcomes under treatment ($A = 1$) and under control ($A = 0$) for every observation.

$$\hat{Q}(A, W) = \hat{E}[Y \mid A, W]$$
$$\hat{Q}(1, W) \quad \text{and} \quad \hat{Q}(0, W)$$
:::

:::{.callout-tip}
## Step 2: Estimate the Propensity Score
Fit a model for $P(A = 1 \mid W)$ — the probability of treatment given confounders.

$$\hat{g}(W) = \hat{P}(A = 1 \mid W)$$
:::

:::{.callout-important}
## Step 3: Compute the Clever Covariate
Combine the propensity score into a covariate that "cleverly" bridges the outcome model and the treatment model:

$$H(A, W) = \frac{I(A=1)}{\hat{g}(W)} - \frac{I(A=0)}{1 - \hat{g}(W)}$$
:::

:::{.callout-warning}
## Step 4: Target, Then Estimate
Run a logistic regression of $Y$ on $H(A, W)$ with $\text{logit}(\hat{Q}(A, W))$ as an offset. The coefficient $\hat{\epsilon}$ is the **fluctuation parameter**. Update predictions:

$$\hat{Q}^*(A, W) = \text{expit}\big(\text{logit}(\hat{Q}(A, W)) + \hat{\epsilon} \cdot H(A, W)\big)$$

The TMLE estimate of the ATE is:

$$\hat{\psi} = \frac{1}{n}\sum_i \hat{Q}^*(1, W_i) - \frac{1}{n}\sum_i \hat{Q}^*(0, W_i)$$
:::

This works beautifully for a single treatment decision at one point in time. But what happens when treatment occurs **repeatedly over time**, and confounders at each time are affected by prior treatment?

---

# 2. The New Problem: Time-Varying Treatment and Confounding

## A simple scenario

Consider a patient followed over **two time periods**. At each period, they either take a medication ($A_t = 1$) or not ($A_t = 0$). We measure their health status ($L_t$) at each period. We observe a final outcome $Y$.

The data for one patient looks like:

```
Baseline → Treatment₁ → Health₁ → Treatment₂ → Health₂ → Outcome
   W          A₁          L₁          A₂          L₂         Y
```

## Why is this harder?

The critical complication is a **feedback loop**:

```
                    ┌──────────────────────────────┐
                    │                              │
                    ▼                              │
   W ──→ A₁ ──→ L₁ ──→ A₂ ──→ Y                  │
         │              ▲                          │
         │              │                          │
         └──────────────┘                          │
         (A₁ affects L₁, which affects A₂)        │
```

- $L_1$ is a **confounder** for the $A_2 \to Y$ relationship (it affects both $A_2$ and $Y$)
- $L_1$ is also **affected by prior treatment** $A_1$

This means $L_1$ is simultaneously:

- A confounder we need to adjust for
- A mediator on the pathway from $A_1$ to $Y$

:::{.callout-caution}
## The Standard Regression Trap
If you **condition on** $L_1$ in a regression (the standard approach), you block part of the causal effect of $A_1$ that operates through $L_1$. You also potentially open collider bias paths.

If you **don't condition on** $L_1$, the effect of $A_2$ is confounded.

**You cannot win with standard regression.** This is the fundamental problem that g-methods (including L-TMLE) solve.
:::

## A concrete example

Imagine studying whether continuous statin use prevents heart attack over 2 years:

- **$W$:** Baseline cholesterol, age, smoking
- **$A_1$:** Statin use in year 1
- **$L_1$:** Cholesterol level at end of year 1 (affected by whether they took statins)
- **$A_2$:** Statin use in year 2 (depends on their year-1 cholesterol)
- **$Y$:** Heart attack by end of year 2

A doctor is more likely to prescribe statins in year 2 if year-1 cholesterol is high ($L_1$ affects $A_2$). But high cholesterol also directly increases heart attack risk ($L_1$ affects $Y$). And year-1 statin use lowers year-1 cholesterol ($A_1$ affects $L_1$).

Conditioning on year-1 cholesterol in a regression blocks the indirect effect of year-1 statin use. Not conditioning on it leaves the year-2 effect confounded. **L-TMLE resolves this by working backwards through time.**

---

# 3. The Key Insight: Working Backwards

Point-treatment TMLE estimates $E[Y \mid A, W]$ in a single step. Longitudinal TMLE uses **iterated conditional expectations** — a sequence of regressions that build on each other, working from the final outcome backwards to baseline.

## The G-computation formula for longitudinal data

For the simple two-time-point case, the causal effect of "always treat" ($\bar{a} = (1, 1)$) is:

$$
E[Y(\bar{a})] = E_W\bigg[ E_{L_1}\Big[ E\big[Y \mid A_2 = 1, L_1, A_1 = 1, W\big] \;\Big|\; A_1 = 1, W \Big] \bigg]
$$

Reading from inside out:

1. **Inner expectation:** Model the outcome given the full history, then evaluate under the intervention $A_2 = 1$
2. **Middle expectation:** Average over $L_1$ (the intermediate confounder) under the intervention $A_1 = 1$
3. **Outer expectation:** Average over baseline covariates $W$

This is a **nested sequence** of conditional expectations. L-TMLE estimates each one, starting from the inside (the final outcome) and working outward (backwards in time).

## Why backwards?

:::{.callout-tip}
## The Backwards Intuition
Think of it like planning a road trip:

- **Forwards:** "From home, where might I end up?" (requires considering all possible routes — combinatorial explosion)
- **Backwards:** "To reach my destination, where must I be at each prior step?" (only one path to trace back)

L-TMLE starts at the **destination** (the final outcome $Y$) and asks: given where we are at each earlier time, what outcome would we expect? Each step peels off one layer of time-varying confounding.
:::

---

# 4. L-TMLE Step by Step (Two Time Points)

We now walk through the full L-TMLE algorithm for the "always treat" regime $\bar{a} = (1, 1)$ with two time points. Each step is color-coded to match the point-treatment TMLE steps, so you can see the parallels.

## Setup: Simulate data

```{r}
library(tidyverse)
set.seed(2026)
n <- 2000

# --- Baseline ---
W <- rnorm(n, mean = 0, sd = 1)   # baseline confounder

# --- Time 1 ---
# Treatment at time 1 (depends on W)
g1_true <- plogis(0.5 + 0.8 * W)
A1 <- rbinom(n, 1, g1_true)

# Time-varying confounder (affected by A1 and W)
L1 <- rnorm(n, mean = 0.5 * W + 0.7 * A1, sd = 1)

# --- Time 2 ---
# Treatment at time 2 (depends on L1 and A1)
g2_true <- plogis(-0.3 + 0.6 * L1 + 0.4 * A1)
A2 <- rbinom(n, 1, g2_true)

# --- Outcome ---
# Y depends on full history
Y <- rbinom(n, 1, plogis(-2 + 0.5 * A1 + 0.6 * A2 + 0.3 * L1 +
                           0.4 * W + 0.2 * A1 * A2))

dat <- tibble(W, A1, L1, A2, Y)

cat("Observed data:\n")
head(dat)
```

### True effect of "always treat"

```{r}
# Simulate the true counterfactual under (A1=1, A2=1)
set.seed(2026)
W_cf <- W  # same baseline

# Under A1 = 1
L1_cf <- rnorm(n, mean = 0.5 * W_cf + 0.7 * 1, sd = 1)

# Under A2 = 1
Y_cf_11 <- plogis(-2 + 0.5 * 1 + 0.6 * 1 + 0.3 * L1_cf +
                     0.4 * W_cf + 0.2 * 1 * 1)

# Under (A1=0, A2=0)
L1_cf_00 <- rnorm(n, mean = 0.5 * W_cf + 0.7 * 0, sd = 1)
Y_cf_00 <- plogis(-2 + 0.5 * 0 + 0.6 * 0 + 0.3 * L1_cf_00 +
                     0.4 * W_cf + 0.2 * 0 * 0)

true_EY_11 <- mean(Y_cf_11)
true_EY_00 <- mean(Y_cf_00)
true_ATE <- true_EY_11 - true_EY_00

cat("True E[Y(1,1)]:", round(true_EY_11, 4), "\n")
cat("True E[Y(0,0)]:", round(true_EY_00, 4), "\n")
cat("True ATE:       ", round(true_ATE, 4), "\n")
```

---

Now we estimate $E[Y(1,1)]$ — the expected outcome if everyone always received treatment — using L-TMLE.

## Phase A: Estimate the Treatment Mechanisms (the g's)

:::{.callout-tip}
## L-TMLE Step 1: Estimate Treatment at EACH Time Point

In point-treatment TMLE, we estimated one propensity score $g(W)$. In L-TMLE, we estimate a **treatment mechanism at every time point**, conditional on the history up to that point.

**Time 1:**
$$\hat{g}_1(W) = \hat{P}(A_1 = 1 \mid W)$$

**Time 2:**
$$\hat{g}_2(A_1, L_1, W) = \hat{P}(A_2 = 1 \mid A_1, L_1, W)$$
:::

```{r}
# Fit treatment models at each time point
g1_mod <- glm(A1 ~ W, family = binomial, data = dat)
g2_mod <- glm(A2 ~ L1 + A1 + W, family = binomial, data = dat)

g1_hat <- predict(g1_mod, type = "response")
g2_hat <- predict(g2_mod, type = "response")

# Bound away from 0 and 1
g1_hat <- pmax(0.01, pmin(0.99, g1_hat))
g2_hat <- pmax(0.01, pmin(0.99, g2_hat))
```

### The cumulative treatment probability

For the "always treat" regime, the probability that a patient follows the full regime is the **product** of time-specific probabilities:

$$\hat{g}_{\bar{1}}(W, L_1) = \hat{g}_1(W) \times \hat{g}_2(A_1=1, L_1, W)$$

```{r}
# Cumulative probability of following the "always treat" regime
# g2 needs to be evaluated at A1=1 (the intervention value)
g2_under_a1 <- predict(g2_mod,
                        newdata = dat %>% mutate(A1 = 1),
                        type = "response")
g2_under_a1 <- pmax(0.01, pmin(0.99, g2_under_a1))

g_cumulative <- g1_hat * g2_under_a1
cat("Cumulative g range:", round(range(g_cumulative), 3), "\n")
cat("Cumulative g mean: ", round(mean(g_cumulative), 3), "\n")
```

---

## Phase B: Sequential Outcome Regression — Working Backwards

This is where L-TMLE diverges from point-treatment TMLE. Instead of one outcome model, we build a **sequence** of outcome regressions, starting at the final outcome and moving backwards.

:::{.callout-note}
## L-TMLE Step 2a: Start at the End — Model the Final Outcome

Just like point-treatment TMLE, start by modeling $E[Y \mid A_2, L_1, A_1, W]$. This is the expected outcome given the complete observed history.

$$\hat{Q}_2(A_2, L_1, A_1, W) = \hat{E}[Y \mid A_2, L_1, A_1, W]$$

Then predict under the intervention $A_2 = 1$:

$$\hat{Q}_2(1, L_1, A_1, W)$$
:::

```{r}
# Step 2a: Outcome regression at the final time point
Q2_mod <- glm(Y ~ A2 + L1 + A1 + W + A1:A2, family = binomial, data = dat)

# Predict under intervention A2 = 1, keeping everything else observed
Q2_A2is1 <- predict(Q2_mod,
                     newdata = dat %>% mutate(A2 = 1),
                     type = "response")

cat("Initial Q2(A2=1) range:", round(range(Q2_A2is1), 4), "\n")
```

:::{.callout-note}
## L-TMLE Step 2b: Move Backwards — Create a "Pseudo-Outcome"

Here is the critical step that handles time-dependent confounding. We take the predictions $\hat{Q}_2(1, L_1, A_1, W)$ — which are functions of $L_1$, $A_1$, and $W$ — and treat them as a **new outcome** to be modeled at time 1.

We fit a regression of $\hat{Q}_2(1, L_1, A_1, W)$ on $(A_1, W)$, **averaging over $L_1$** in the process:

$$\hat{Q}_1(A_1, W) = \hat{E}\big[\hat{Q}_2(1, L_1, A_1, W) \;\big|\; A_1, W\big]$$

Then predict under the intervention $A_1 = 1$:

$$\hat{Q}_1(1, W)$$
:::

```{r}
# Step 2b: Backwards regression — model Q2 predictions as a function of (A1, W)
# This "integrates out" L1 from the estimation

Q1_mod <- glm(Q2_A2is1 ~ A1 + W, family = quasibinomial, data = dat)

# Predict under intervention A1 = 1
Q1_A1is1 <- predict(Q1_mod,
                     newdata = dat %>% mutate(A1 = 1),
                     type = "response")

cat("Initial Q1(A1=1) range:", round(range(Q1_A1is1), 4), "\n")
cat("Initial G-comp estimate E[Y(1,1)]:", round(mean(Q1_A1is1), 4), "\n")
cat("True E[Y(1,1)]:                   ", round(true_EY_11, 4), "\n")
```

:::{.callout-caution}
## What Just Happened?

This two-step backwards regression is the **longitudinal G-computation** formula in action:

1. **Step 2a** estimated $E[Y \mid A_2=1, L_1, A_1, W]$ — what we expect the outcome to be if $A_2 = 1$, for each specific value of $L_1$

2. **Step 2b** estimated $E_{L_1}[E[Y \mid A_2=1, L_1, A_1=1, W] \mid A_1=1, W]$ — the expected outcome under both $A_1=1$ and $A_2=1$, **averaging over $L_1$ as it would be under $A_1 = 1$**

By regressing the time-2 predictions on $(A_1, W)$ **without conditioning on $L_1$**, we allowed $L_1$ to vary as it naturally would under the intervention $A_1 = 1$. This is how the backwards regression avoids the standard regression trap — it never conditions on the time-varying confounder while simultaneously modeling it as affected by treatment.

**This is longitudinal G-computation. But like point-treatment G-computation, it depends on both models being correctly specified. The targeting steps below make it doubly robust.**
:::

---

## Phase C: Targeting — Making It Doubly Robust

Now we have initial estimates $\hat{Q}_2$ and $\hat{Q}_1$ from the sequential regressions. These estimates are optimized to predict well, but they are not yet "targeted" at our specific estimand $E[Y(1,1)]$.

The targeting steps update these initial estimates using information from the treatment mechanism, just like in point-treatment TMLE — but now we do it **at each time step, working backwards**.

:::{.callout-important}
## L-TMLE Step 3a: Target $\hat{Q}_2$ at Time 2

Construct the clever covariate at time 2. For the "always treat" regime, among observations with $A_1 = 1$ and $A_2 = 1$ (those who followed the regime), this is:

$$H_2 = \frac{I(A_1 = 1, A_2 = 1)}{\hat{g}_1(W) \times \hat{g}_2(1, L_1, W)}$$

This is the inverse of the **cumulative** probability of following the full treatment regime through time 2.

Fit the fluctuation model:

$$\text{logit}(Y) = \text{logit}(\hat{Q}_2(A_2, L_1, A_1, W)) + \epsilon_2 \cdot H_2$$

Update: $\hat{Q}_2^*(A_2, L_1, A_1, W) = \text{expit}(\text{logit}(\hat{Q}_2) + \hat{\epsilon}_2 \cdot H_2)$
:::

```{r}
logit <- function(p) log(p / (1 - p))

# Clever covariate at time 2
# Only patients who followed regime at BOTH times contribute
H2 <- (dat$A1 == 1 & dat$A2 == 1) / (g1_hat * g2_hat)

# Prediction from Q2 at OBSERVED (A1, A2)
QA_init_t2 <- predict(Q2_mod, type = "response")

# Fluctuation model at time 2
fluc2 <- glm(dat$Y ~ -1 + offset(logit(QA_init_t2)) + H2,
             family = binomial)
eps2 <- coef(fluc2)

# Update Q2 predictions under A2=1
Q2_star <- plogis(logit(Q2_A2is1) + eps2 / (g1_hat * g2_under_a1))
# Note: for the targeted predictions under intervention, H2 = 1/g_cumulative

cat("Epsilon at time 2:", round(eps2, 5), "\n")
cat("Updated Q2*(A2=1) range:", round(range(Q2_star), 4), "\n")
```

:::{.callout-important}
## L-TMLE Step 3b: Target $\hat{Q}_1$ at Time 1

Now we repeat the targeting at time 1, but using the **updated** $\hat{Q}_2^*$ as the outcome.

Re-fit the backwards regression using $\hat{Q}_2^*$ (the targeted predictions from step 3a):

$$\hat{Q}_1^{\text{updated}}(A_1, W) = \hat{E}\big[\hat{Q}_2^*(1, L_1, A_1, W) \;\big|\; A_1, W\big]$$

Then target with the clever covariate at time 1:

$$H_1 = \frac{I(A_1 = 1)}{\hat{g}_1(W)}$$

Fit: $\text{logit}(\hat{Q}_2^*) = \text{logit}(\hat{Q}_1^{\text{updated}}) + \epsilon_1 \cdot H_1$

Update: $\hat{Q}_1^*(1, W) = \text{expit}(\text{logit}(\hat{Q}_1^{\text{updated}}) + \hat{\epsilon}_1 \cdot H_1)$
:::

```{r}
# Re-fit backwards regression using targeted Q2*
Q1_mod_updated <- glm(Q2_star ~ A1 + W, family = quasibinomial, data = dat)

Q1_updated <- predict(Q1_mod_updated, type = "response")
Q1_under_a1 <- predict(Q1_mod_updated,
                        newdata = dat %>% mutate(A1 = 1),
                        type = "response")

# Clever covariate at time 1
H1 <- (dat$A1 == 1) / g1_hat

# Fluctuation model at time 1
fluc1 <- glm(Q2_star ~ -1 + offset(logit(Q1_updated)) + H1,
             family = quasibinomial)
eps1 <- coef(fluc1)

# Final targeted predictions under A1 = 1
Q1_star <- plogis(logit(Q1_under_a1) + eps1 / g1_hat)

cat("Epsilon at time 1:", round(eps1, 5), "\n")
```

---

## Phase D: The Final L-TMLE Estimate

:::{.callout-warning}
## L-TMLE Step 4: Plug In and Estimate

The L-TMLE estimate of $E[Y(1,1)]$ is the sample mean of the final targeted predictions:

$$\hat{\psi}_{\text{LTMLE}} = \frac{1}{n} \sum_{i=1}^{n} \hat{Q}_1^*(1, W_i)$$
:::

```{r}
ltmle_estimate <- mean(Q1_star)

cat("========================================\n")
cat("L-TMLE estimate E[Y(1,1)]:", round(ltmle_estimate, 4), "\n")
cat("True E[Y(1,1)]:           ", round(true_EY_11, 4), "\n")
cat("Naive G-comp (no targeting):", round(mean(Q1_A1is1), 4), "\n")
cat("========================================\n")
```

---

# 5. How Does Targeting Remove Time-Dependent Confounding?

This is the most important section. Let us build intuition for **why** the backwards targeting procedure works.

## The problem, restated

In our statin example:

- Year-1 statins ($A_1$) lower cholesterol ($L_1$)
- Lower cholesterol ($L_1$) makes year-2 statins ($A_2$) less likely
- Lower cholesterol ($L_1$) also directly lowers heart attack risk ($Y$)

A naive regression that conditions on $L_1$ blocks the indirect effect $A_1 \to L_1 \to Y$ and introduces collider bias. But not adjusting for $L_1$ confounds the $A_2 \to Y$ effect.

## How L-TMLE handles this

:::{.callout-tip}
## The Backwards Regression Solves the "Condition or Not?" Dilemma

**At time 2:** We fit $E[Y \mid A_2, L_1, A_1, W]$ and DO condition on $L_1$. This correctly adjusts for the confounding of $A_2$ by $L_1$.

**At time 1:** We regress the time-2 predictions on $(A_1, W)$ and do NOT condition on $L_1$. Instead, we let $L_1$ vary freely. This preserves the indirect effect $A_1 \to L_1 \to Y$.

By working backwards, we condition on $L_1$ only where it is needed (as a confounder of $A_2$) and integrate it out where conditioning would be harmful (as a mediator of $A_1$).
:::

## What does targeting add?

The sequential regression (G-computation) above handles the time-ordering correctly, but it relies on **all** outcome models being correctly specified. The targeting steps add robustness:

:::{.callout-important}
## Why Targeting Matters

**At time 2:** The fluctuation step using $H_2 = \frac{I(A_1=1, A_2=1)}{g_1 \cdot g_2}$ corrects the outcome model using information from the treatment models. Even if $\hat{Q}_2$ is somewhat misspecified, the clever covariate steers the predictions toward the correct value for our target estimand.

**At time 1:** The fluctuation step using $H_1 = \frac{I(A_1=1)}{g_1}$ further corrects $\hat{Q}_1$ using the time-1 treatment model.

Together, these targeting steps achieve **sequential double robustness**: the final estimate is consistent if, at each time point, either the outcome model OR the treatment model is correctly specified.
:::

## The double robustness property

For point-treatment TMLE, double robustness means: consistent if $\hat{Q}$ OR $\hat{g}$ is correct.

For L-TMLE, it extends to: consistent if **at each time step**, either the $\hat{Q}_t$ or $\hat{g}_t$ is correct. Specifically:

| If correct at time 2 | If correct at time 1 | L-TMLE is... |
|---|---|---|
| $Q_2$ only | $Q_1$ only | Consistent |
| $g_2$ only | $g_1$ only | Consistent |
| $Q_2$ only | $g_1$ only | Consistent |
| $g_2$ only | $Q_1$ only | Consistent |
| Neither | Anything | Not guaranteed |

This is a much stronger property than simply requiring one global model to be correct.

---

# 6. Visualizing the Difference

## Compare estimators

```{r}
# --- Naive: condition on everything ---
naive_mod <- glm(Y ~ A1 + A2 + L1 + W, family = binomial, data = dat)
naive_pred <- predict(naive_mod,
                       newdata = dat %>% mutate(A1 = 1, A2 = 1),
                       type = "response")
naive_est <- mean(naive_pred)

# --- Standard IPTW with cumulative weights ---
# Weight = 1 / P(observed treatment sequence)
iptw_w <- ifelse(dat$A1 == 1 & dat$A2 == 1,
                 1 / (g1_hat * g2_hat),
                 0)
# Horvitz-Thompson
iptw_est <- sum(iptw_w * dat$Y) / sum(iptw_w)

# --- Compare ---
comparison <- tibble(
  Method = c("True E[Y(1,1)]",
             "Naive regression (conditions on L1)",
             "Sequential G-computation (no targeting)",
             "Longitudinal IPTW",
             "L-TMLE"),
  Estimate = round(c(true_EY_11, naive_est, mean(Q1_A1is1), iptw_est, ltmle_estimate), 4)
)
comparison
```

```{r}
ggplot(comparison, aes(x = Estimate,
                       y = factor(Method, levels = rev(comparison$Method)))) +
  geom_point(size = 3, color = c("red", "#4477AA", "#228833", "#EE6677", "#AA3377")) +
  geom_vline(xintercept = true_EY_11, linetype = "dashed", color = "red", linewidth = 0.5) +
  annotate("text", x = true_EY_11, y = 5.4, label = "Truth", color = "red", size = 3) +
  labs(x = "Estimated E[Y(1,1)]", y = "", title = "How Well Does Each Estimator Recover the Truth?") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 10))
```

---

# 7. The Full Picture: Point-Treatment vs. Longitudinal TMLE

```
POINT-TREATMENT TMLE                    LONGITUDINAL TMLE (2 time points)

┌─────────────────────┐               ┌─────────────────────────────────┐
│ Step 1: Fit Q(A,W)  │               │ Step 2a: Fit Q₂(A₂,L₁,A₁,W)   │
│ = E[Y | A, W]       │               │ = E[Y | A₂, L₁, A₁, W]        │
└──────────┬──────────┘               └──────────────┬──────────────────┘
           │                                         │
           │                          ┌──────────────▼──────────────────┐
           │                          │ Step 3a: Target Q₂ using H₂    │
           │                          │ (clever cov with cumulative g)  │
           │                          └──────────────┬──────────────────┘
           │                                         │
           │                          ┌──────────────▼──────────────────┐
           │                          │ Step 2b: Fit Q₁(A₁,W)          │
           │                          │ = E[Q₂* | A₁, W]               │
           │                          │ (uses TARGETED Q₂ as outcome)   │
           │                          └──────────────┬──────────────────┘
           │                                         │
┌──────────▼──────────┐               ┌──────────────▼──────────────────┐
│ Step 2: Fit g(W)    │               │ Step 1: Fit g₁(W) and          │
│ = P(A=1|W)          │               │         g₂(L₁,A₁,W)           │
└──────────┬──────────┘               └──────────────┬──────────────────┘
           │                                         │
┌──────────▼──────────┐               ┌──────────────▼──────────────────┐
│ Step 3: Clever cov  │               │ Step 3b: Target Q₁ using H₁    │
│ H = I(A=1)/g        │               │ (clever cov with g₁ only)       │
└──────────┬──────────┘               └──────────────┬──────────────────┘
           │                                         │
┌──────────▼──────────┐               ┌──────────────▼──────────────────┐
│ Step 4: Target Q    │               │ Step 4: Average Q₁*            │
│ and estimate ATE    │               │ = E[Y(1,1)] via L-TMLE          │
└─────────────────────┘               └─────────────────────────────────┘
```

The parallel structure is clear: L-TMLE repeats the "model → target" cycle at each time point, working backwards from the outcome.

---

# 8. Scaling Up: More Than Two Time Points

With $T$ time points, L-TMLE follows the same pattern:

1. **Estimate treatment mechanisms** $\hat{g}_t$ for $t = 1, \ldots, T$
2. **Start at time $T$:** Fit $\hat{Q}_T = E[Y \mid \bar{A}_T, \bar{L}_T, W]$
3. **Target $\hat{Q}_T$** using $H_T = I(\bar{A}_T = \bar{a}_T) / \prod_{s=1}^{T} \hat{g}_s$
4. **Move to time $T-1$:** Fit $\hat{Q}_{T-1} = E[\hat{Q}_T^* \mid \bar{A}_{T-1}, \bar{L}_{T-1}, W]$
5. **Target $\hat{Q}_{T-1}$** using $H_{T-1} = I(\bar{A}_{T-1} = \bar{a}_{T-1}) / \prod_{s=1}^{T-1} \hat{g}_s$
6. **Repeat** until $\hat{Q}_1^*$ is obtained
7. **Estimate:** $\hat{\psi} = \frac{1}{n}\sum_i \hat{Q}_1^*(1, W_i)$

Each backwards step:

- Conditions on the time-varying confounder $L_t$ (to deconfound $A_{t+1}$)
- Then integrates $L_t$ out in the next regression (to preserve $A_t$'s mediated effect)
- Targets the estimate using the cumulative treatment probability through time $t$

:::{.callout-caution}
## Practical Warning: Cumulative Products
In longitudinal IPTW, the weights are products of $T$ terms: $w = \prod_{t=1}^{T} 1/g_t$. With many time points, these products can become extremely large or small, making IPTW unstable.

L-TMLE avoids this by incorporating the treatment information through **targeting steps** rather than **multiplicative weights**. The clever covariates still involve the cumulative product, but they enter through a logistic fluctuation model that is far more numerically stable than multiplying raw weights.

This is a major practical advantage of L-TMLE over longitudinal IPTW.
:::

---

# 9. Using the `ltmle` Package

The `ltmle` R package automates the entire procedure above. Here is how you would use it for our two-time-point example:

```{r, eval=FALSE}
library(ltmle)

# Data must be in temporal order: W, A1, L1, A2, Y
dat_ltmle <- data.frame(
  W  = dat$W,
  A1 = dat$A1,
  L1 = dat$L1,
  A2 = dat$A2,
  Y  = dat$Y
)

# Specify the "always treat" intervention
result <- ltmle(
  data = dat_ltmle,
  Anodes = c("A1", "A2"),           # treatment nodes
  Lnodes = c("L1"),                  # time-varying covariate nodes
  Ynodes = "Y",                      # outcome
  abar = c(1, 1),                    # intervention: always treat
  SL.library = c("SL.glm", "SL.mean")  # Super Learner library
)

summary(result)
```

The package handles all the backwards regressions, clever covariates, fluctuation steps, and influence-curve inference automatically.

For more flexible interventions (stochastic, dynamic), the `lmtp` package extends this framework:

```{r, eval=FALSE}
library(lmtp)

result_lmtp <- lmtp_tmle(
  data = dat,
  trt = c("A1", "A2"),
  outcome = "Y",
  baseline = "W",
  time_vary = list("L1"),
  shift = function(data, trt) rep(1, nrow(data)),  # always treat
  outcome_type = "binomial",
  learners_outcome = c("SL.glm", "SL.mean"),
  learners_trt = c("SL.glm", "SL.mean"),
  folds = 5
)

result_lmtp
```

---

# 10. Summary: What Makes L-TMLE Special

:::{.callout-note}
## The Three Key Ideas

**1. Backwards sequential regression** handles time-varying confounding correctly by conditioning on $L_t$ where it is a confounder and integrating it out where it is a mediator. No standard regression can do both.

**2. Targeting at each time step** makes the estimator doubly robust at every stage. Even if some outcome models are misspecified, the treatment mechanism models can compensate (and vice versa).

**3. Working on the logit scale** ensures all predictions stay bounded between 0 and 1, and avoids the extreme-weight instability of longitudinal IPTW.
:::

| Feature | Standard regression | Longitudinal IPTW | Longitudinal G-comp | L-TMLE |
|---------|--------------------|--------------------|---------------------|--------|
| Handles time-varying confounding | No | Yes | Yes | Yes |
| Avoids extreme weights | Yes | No | Yes | Yes |
| Doubly robust | No | No | No | Yes |
| Works with machine learning | Partially | Partially | Partially | Yes |
| Valid inference | Standard SE | Sandwich SE | Bootstrap | Influence curve |
| Respects bounds | Depends | N/A | Not guaranteed | Yes |

---

# Key Takeaways

1. **Time-varying confounders affected by prior treatment** create a problem that no standard regression can solve. You need g-methods.

2. **L-TMLE extends point-treatment TMLE** by iterating the "model → target" cycle backwards through time. If you understand the KH Stats 4-step TMLE, L-TMLE is the same idea repeated at each time point.

3. **The backwards regression** handles the "condition or not?" dilemma: condition on $L_t$ to deconfound $A_{t+1}$, then integrate it out to preserve $A_t$'s mediated effect.

4. **Targeting at each step** makes L-TMLE doubly robust at every time point, not just globally.

5. **L-TMLE avoids the extreme weight problem** of longitudinal IPTW by incorporating treatment information through logistic fluctuation models rather than multiplicative weights.

6. **Packages `ltmle` and `lmtp`** automate the entire procedure, including Super Learner integration and influence-curve inference.

---

*This guide was inspired by [Kat Hoffman's Illustrated Guide to TMLE](https://www.khstats.com/blog/tmle/tutorial) and her [visual guides for causal inference](https://github.com/kathoffman/causal-inference-visual-guides). The backwards sequential regression framework for L-TMLE is described in [Petersen, Schwab, Gruber, Blaser, Schomaker, and van der Laan (2014)](https://pmc.ncbi.nlm.nih.gov/articles/PMC4405134/) and [Schnitzer et al.](https://putnamds.com/LTMLE_Schnitzer.pdf)*

```{r}
sessionInfo()
```
